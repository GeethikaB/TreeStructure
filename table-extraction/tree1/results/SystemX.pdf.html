<html><div id=1><figure bbox=46.94263008,113.91804064,371.54743008,979.53084064></figure><paragraph><word top=78.3581994 left=139.597 bottom=99.9807418 right=472.4051128>SystemX: Knowledge Base Construction</word> <word top=98.2831994 left=193.516 bottom=119.9057418 right=418.4868472>from Richly Formatted Data</word> </paragraph><paragraph><word top=146.7779216 left=53.798 bottom=162.3316368 right=118.236528>ABSTRACT</word> <word top=166.3042304 left=53.377 bottom=177.1087424 right=295.537585984>We focus on knowledge base construction (KBC) from richly for-</word> <word top=177.2632304 left=53.798 bottom=188.0677424 right=294.045335552>matted data. In contrast to KBC from text or tabular data, KBC from</word> <word top=188.2222304 left=53.798 bottom=199.0267424 right=294.361670144>richly formatted data aims to extract entity relations conveyed jointly</word> <word top=199.1812304 left=53.574 bottom=209.9857424 right=294.043379482>via textual, structural, tabular, and visual expressions. We introduce</word> <word top=209.0373632 left=53.798 bottom=220.9985408 right=295.529862349>SystemX, a machine-learning based KBC system for richly format-</word> <word top=221.0992304 left=53.798 bottom=231.9037424 right=229.686736256>ted data. SystemX introduces a new data model</word> <word top=221.0992304 left=232.393871744 bottom=231.9037424 right=294.200701568>that accounts for</word> <word top=232.0572304 left=53.798 bottom=242.8617424 right=295.529705139>three challenging characteristics of richly formatted data: (1) preva-</word> <word top=243.0162304 left=53.798 bottom=253.8207424 right=295.61822144>lent document-level relations, (2) multimodality, and (3) data variety.</word> <word top=252.8723632 left=53.798 bottom=264.8335408 right=294.046987648>SystemX uses a new deep learning model to automatically capture</word> <word top=264.9342304 left=53.798 bottom=275.7387424 right=294.044788602>the representation (i.e., features) needed to learn to extract relations</word> <word top=275.8932304 left=53.798 bottom=286.6977424 right=295.530549856>from richly formatted data. Finally, SystemX introduces a new pro-</word> <word top=286.8522304 left=53.798 bottom=297.6567424 right=294.041139277>gramming model that enables users to convert multimodal domain</word> <word top=297.8112304 left=53.798 bottom=308.6157424 right=294.047128832>expertise, based on both textual and visual context, to meaningful</word> <word top=308.7702304 left=53.798 bottom=319.5747424 right=294.044671744>signals of supervision for training the KBC system. SystemX-based</word> <word top=319.7292304 left=53.798 bottom=330.5337424 right=295.162907648>KBC systems have been put in production in a range of use cases,</word> <word top=330.6882304 left=53.798 bottom=341.4927424 right=295.533858048>including a major online retailer. We compare SystemX against state-</word> <word top=341.6462304 left=53.798 bottom=352.4507424 right=294.044788602>of-the-art KBC approaches in four different domains. We show that</word> <word top=351.5023632 left=53.798 bottom=363.4635408 right=294.049222054>SystemX achieves an average improvement of 41 F1 points on the</word> <word top=363.5642304 left=53.798 bottom=374.3687424 right=294.045335552>quality of the output knowledge base–and in some cases produces up</word> <word top=374.5232304 left=53.798 bottom=385.3277424 right=294.041787354>to 1.87× the number of correct entries–compared to expert-curated</word> <word top=385.4822304 left=53.798 bottom=396.2867424 right=294.046447386>public knowledge bases. We also conduct a user study to assess the</word> <word top=396.4412304 left=53.798 bottom=407.2457424 right=294.194752064>usability of SystemX’s new programming model. We show that after</word> <word top=407.4002304 left=53.798 bottom=418.2047424 right=294.042338944>using SystemX for only 30 minutes, domain experts were able to</word> <word top=418.3592304 left=53.798 bottom=429.1637424 right=294.197880915>design KBC systems that achieve on average 23.3 F1 points higher</word> <word top=429.3182304 left=53.798 bottom=440.1227424 right=280.0292384>quality than existing machine-learning based KBC approaches.</word> </paragraph><paragraph><word top=457.5419216 left=53.798 bottom=473.0956368 right=59.7756>1</word> <word top=457.5419216 left=71.7308 bottom=473.0956368 right=167.013744>INTRODUCTION</word> <word top=477.0682304 left=53.798 bottom=487.8727424 right=295.537882496>Knowledge base construction is the process of populating a data-</word> <word top=488.0272304 left=53.798 bottom=498.8317424 right=225.161505536>base with information from data such as text,</word> <word top=488.0272304 left=228.362510336 bottom=498.8317424 right=251.98592576>tables,</word> <word top=488.0272304 left=255.18693056 bottom=498.8317424 right=294.19346048>images, or</word> <word top=498.9852304 left=53.574 bottom=509.7897424 right=294.361579085>video. Extensive efforts have been made to build large, high-quality</word> <word top=509.9442304 left=53.798 bottom=520.7487424 right=295.531835328>knowledge bases (KBs), such as Freebase [5], YAGO [36], IBM Wat-</word> <word top=520.9032304 left=53.798 bottom=531.7077424 right=295.613959424>son [6, 10], PharmGKB [16], and Google Knowledge Graph [35].</word> <word top=531.8622304 left=53.52 bottom=542.6667424 right=294.043500672>Traditionally, KBC solutions have focused on relation extraction</word> <word top=542.8212304 left=53.798 bottom=553.6257424 right=294.362801696>from unstructured text [22, 26, 34, 42]. These KBC systems already</word> <word top=553.7802304 left=53.798 bottom=564.5847424 right=295.537882496>support a broad range of downstream applications such as infor-</word> <word top=564.7392304 left=53.798 bottom=575.5437424 right=294.047128832>mation retrieval, question answering, medical diagnosis, and data</word> <word top=575.6982304 left=53.574 bottom=586.5027424 right=294.042626304>visualization. However, troves of information remain untapped in</word> <word top=586.5496336 left=53.798 bottom=597.2555152 right=294.042424512>richly formatted data, where relations and attributes are expressed</word> <word top=597.6162304 left=53.574 bottom=608.4207424 right=294.042626304>via combinations of textual, structural, tabular, and visual cues. In</word> <word top=608.5742304 left=53.798 bottom=619.3787424 right=294.048500691>these scenarios, the semantics of the data are significantly affected</word> <word top=619.5332304 left=53.798 bottom=630.3377424 right=294.356415834>by the organization and layout of the document. Examples of richly</word> <word top=630.4922304 left=53.798 bottom=641.2967424 right=295.533309632>formatted data include webpages, business reports, product specifi-</word> <word top=641.4512304 left=53.798 bottom=652.2557424 right=294.047084>cations, and scientific literature. We use an example to demonstrate</word> <word top=652.4102304 left=53.798 bottom=663.2147424 right=171.840656>KBC from richly formatted data:</word> </paragraph><paragraph><word top=682.9972262 left=53.798 bottom=691.3239434 right=182.5413218>SIGMOD’18, June 2018, Houston, Texas USA</word> <word top=691.8479118 left=53.798 bottom=700.2513408 right=214.7254088>2018. ACM ISBN 978-x-xxxx-xxxx-x/YY/MM. . . $15.00</word> <word top=699.8179118 left=53.798 bottom=708.2213408 right=170.1070364>https://doi.org/10.1145/nnnnnnn.nnnnnnn</word> </paragraph><figure_caption><word top=284.2206912 left=317.955 bottom=295.8859776 right=559.689144>Figure 1: A KBC task to populate Relation HasCollectorCur-</word> <word top=295.1796912 left=317.955 bottom=306.8449776 right=558.2007216>rent(Transistor Part, Current)from transistor datasheets. Part</word> <word top=306.1386912 left=317.955 bottom=317.8039776 right=558.2007216>and Current mentions are boxed in blue and green respectively.</word> </figure_caption><paragraph><word top=328.7116336 left=327.918 bottom=339.4175152 right=559.681527424>Example 1.1 (HasCollectorCurrent). We highlight the ELEC-</word> <word top=341.5518041 left=317.937 bottom=350.1953896 right=558.20039321>TRONICS domain. We are given a collection of transistor datasheets</word> <word top=350.7372304 left=317.659 bottom=361.5417424 right=559.688497216>(as the one shown in Figure 1),and we want to build a KB of transis-</word> <word top=361.8602304 left=317.955 bottom=372.6647424 right=558.204031078>tors’ maximum collector current.1 The output knowledge base can</word> <word top=372.8192304 left=317.955 bottom=383.6237424 right=559.687601779>power a tool which verifies that transistors do not exceed their maxi-</word> <word top=383.7782304 left=317.955 bottom=394.5827424 right=558.19937664>mum ratings in a circuit. Figure 1 shows how relevant information</word> <word top=394.7372304 left=317.955 bottom=405.5417424 right=558.354880915>is located in both the document header and table cell and how their</word> <word top=405.6962304 left=317.955 bottom=416.5007424 right=558.9090672>relationship is expressed using semantics from multiple modalities.</word> </paragraph><paragraph><word top=423.1032304 left=327.918 bottom=433.9077424 right=558.353762688>The heterogeneity of signals in richly formatted poses a major</word> <word top=434.0622304 left=317.955 bottom=444.8667424 right=558.2007216>challenge for existing KBC systems. The above example illustrates</word> <word top=445.0212304 left=317.955 bottom=455.8257424 right=558.204128832>that KBC systems which focus on text data—and adjacent textual</word> <word top=455.9802304 left=317.955 bottom=466.7847424 right=559.686974131>contexts such as sentences or paragraphs—can miss important infor-</word> <word top=466.9392304 left=317.955 bottom=477.7437424 right=559.768317312>mation due to this breadth of signals in richly formatted datasheets.</word> <word top=477.8982304 left=317.534 bottom=488.7027424 right=559.771410099>We review the major challenges of KBC from richly formatted data.</word> </paragraph><paragraph><word top=492.1094374 left=327.918 bottom=504.0047818 right=558.199213568>Challenges. KBC on richly formatted data poses a number of</word> <word top=504.1732304 left=317.955 bottom=514.9777424 right=559.689260563>challenges beyond those present with unstructured data: (1) accom-</word> <word top=515.1322304 left=317.955 bottom=525.9367424 right=559.690798733>modating prevalent document-level relations, (2) capturing the mul-</word> <word top=525.9836336 left=317.955 bottom=536.6895152 right=558.20045824>timodality of information in the input data, and (3) addressing the</word> <word top=537.0492304 left=317.955 bottom=547.8537424 right=407.3576>tremendous data variety.</word> <word top=552.8446912 left=317.955 bottom=564.5099776 right=558.198149056>Prevalent Document-Level Relations We define the context of a</word> <word top=564.7272304 left=317.955 bottom=575.5317424 right=558.197565427>relation as the scope information that needs to be considered when</word> <word top=575.6862304 left=317.955 bottom=586.4907424 right=558.198632429>extracting the relation. Context can range from a single sentence to</word> <word top=586.6452304 left=317.955 bottom=597.4497424 right=558.203016998>a whole document. KBC systems typically limit context scope to a</word> <word top=597.6042304 left=317.955 bottom=608.4087424 right=558.202335552>few sentences or a single table, assuming that relations are expressed</word> <word top=608.5632304 left=317.955 bottom=619.3677424 right=558.202335552>relatively locally. However, for richly formatted data, many relations</word> <word top=619.5222304 left=317.955 bottom=630.3267424 right=456.631673664>rely on information from throughout</word> <word top=619.5222304 left=460.180216128 bottom=630.3267424 right=535.34895456>the entire document</word> <word top=619.5222304 left=538.906642752 bottom=630.3267424 right=558.204128832>to be</word> <word top=630.4812304 left=317.955 bottom=641.2857424 right=385.5437232>properly extracted.</word> </paragraph><paragraph><word top=647.7816336 left=327.918 bottom=658.4875152 right=476.90539705>Example 1.2 (Document-Level Relations).</word> <word top=647.8892304 left=481.384 bottom=658.6937424 right=558.356240128>In Figure 1, transistor</word> <word top=658.8472304 left=317.955 bottom=669.6517424 right=558.204128832>parts are located in the document header (boxed in blue), and the</word> <word top=669.8062304 left=317.955 bottom=680.6107424 right=559.322005786>collector current value is in a table cell (boxed in green). Moreover,</word> <word top=690.7327745 left=317.544 bottom=697.335572 right=558.322738744>1Transistors are semiconductor devices often used as switches or amplifiers. Their</word> <word top=699.9799118 left=317.955 bottom=708.3833408 right=511.9451946>electrical specifications are published by manufacturers in datasheets.</word> </paragraph></div><div id=2><header><word top=57.26044 left=53.798 bottom=68.3608432 right=236.1656096>SIGMOD’18, June 2018, Houston, Texas USA</word> </header><paragraph><word top=84.7812304 left=53.798 bottom=95.5857424 right=294.042663565>the interpretation of some numerical values depends on their units</word> <word top=95.7402304 left=53.798 bottom=106.5447424 right=227.8627232>reported in another table column (e.g., 200 mA).</word> <word top=111.2542304 left=53.798 bottom=122.0587424 right=294.047128832>Limiting the context scope to a single sentence or a single table</word> <word top=122.2132304 left=53.798 bottom=133.0177424 right=293.819583846>misses many potential relations, up to 97% in the ELECTRONICS</word> <word top=133.1722304 left=53.798 bottom=143.9767424 right=294.045335552>application, for example. On the other hand, considering all possible</word> <word top=144.1312304 left=53.798 bottom=154.9357424 right=295.529301651>entity pairs throughout the document as candidates renders the ex-</word> <word top=155.0892304 left=53.798 bottom=165.8937424 right=294.047128832>traction problem unnecessarily computationally intractable due to</word> <word top=166.0482304 left=53.798 bottom=176.8527424 right=205.823312>the combinatorial explosion of candidates.</word> <word top=181.8436912 left=53.798 bottom=193.5089776 right=294.048607168>Multimodality Classical KBC systems usually model input data as</word> <word top=193.7262304 left=53.798 bottom=204.5307424 right=294.049239987>unstructured text [22, 25, 34]. With richly formatted data semantics</word> <word top=204.6852304 left=53.798 bottom=215.4897424 right=295.161293696>are expressed through multiple modalities–textual, structural, tabular,</word> <word top=215.6442304 left=53.798 bottom=226.4487424 right=92.9004704>and visual:</word> </paragraph><paragraph><word top=231.0506336 left=63.761 bottom=241.7565152 right=167.932482771>Example 1.3 (Multimodality).</word> <word top=231.1582304 left=172.417 bottom=241.9627424 right=294.048618995>In Figure 1, important information</word> <word top=242.1172304 left=53.502 bottom=252.9217424 right=294.045316416>(e.g., the transistor names in the header) is expressed in larger, bold</word> <word top=253.0762304 left=53.798 bottom=263.8807424 right=207.135275648>fonts (displayed in yellow). Furthermore,</word> <word top=253.0762304 left=210.089345792 bottom=263.8807424 right=294.047128832>the meaning of a table</word> <word top=264.0352304 left=53.798 bottom=274.8397424 right=222.015375104>entry depends on other entries with which it</word> <word top=264.0352304 left=225.362711552 bottom=274.8397424 right=294.047128832>is visually aligned</word> <word top=274.9942304 left=53.502 bottom=285.7987424 right=294.043630733>(shown by the red arrow). For instance, the semantics of a numeric</word> <word top=285.9532304 left=53.574 bottom=296.7577424 right=189.0652704>value are specified by an aligned unit.</word> <word top=301.4672304 left=53.798 bottom=312.2717424 right=294.042663565>Semantics from different modalities can vary significantly but can</word> <word top=312.4262304 left=53.798 bottom=323.2307424 right=183.9094304>convey complementary information.</word> <word top=328.2216912 left=53.798 bottom=339.8869776 right=211.472661184>Data Variety With richly formatted data,</word> <word top=329.1452304 left=214.609645888 bottom=339.9497424 right=294.049439296>the same information</word> <word top=340.1042304 left=53.798 bottom=350.9087424 right=294.047128832>can be presented in many different formats and styles, in addition</word> <word top=351.0632304 left=53.798 bottom=361.8677424 right=189.24623168>to linguistic variations. For example,</word> <word top=351.0632304 left=191.85276416 bottom=361.8677424 right=294.047128832>tabular data can be oriented</word> <word top=362.0222304 left=53.574 bottom=372.8267424 right=293.8197216>vertically or horizontally and numerical values have many formats:</word> </paragraph><paragraph><word top=377.4286336 left=63.761 bottom=388.1345152 right=166.915666112>Example 1.4 (Data Variety).</word> <word top=377.5362304 left=171.401 bottom=388.3407424 right=294.04521248>In Figure 1, numeric intervals are</word> <word top=388.4952304 left=53.798 bottom=399.2997424 right=294.047128832>expressed as “-65 . . . 150”, but other datasheets show intervals as</word> <word top=399.4532304 left=52.606 bottom=410.2577424 right=294.044608>“-65 ∼ 150”, or “-65 to 150”. Similarly, some attribute names are</word> <word top=410.4122304 left=53.798 bottom=421.2167424 right=274.4776816>synonymous with symbols, like “Collector current” and “Ic”.</word> <word top=425.9262304 left=53.798 bottom=436.7307424 right=294.047128832>Data variety requires KBC systems to be generalizable and robust</word> <word top=436.8852304 left=53.798 bottom=447.6897424 right=154.3651424>against changing input data.</word> </paragraph><paragraph><word top=450.4894374 left=63.761 bottom=462.3847818 right=294.043793536>Our Approach. We introduce SystemX, a machine-learning</word> <word top=462.5522304 left=53.798 bottom=473.3567424 right=294.042155136>based system for KBC from richly formatted data. SystemX takes as</word> <word top=473.5112304 left=53.798 bottom=484.3157424 right=295.165005786>input richly formatted documents which may be of diverse formats,</word> <word top=484.4702304 left=53.798 bottom=495.2747424 right=294.045538432>including PDF, HTML, and XML. SystemX parses the documents</word> <word top=495.4292304 left=53.798 bottom=506.2337424 right=295.537882496>and analyzes the corresponding multimodal, document-level con-</word> <word top=506.3882304 left=53.798 bottom=517.1927424 right=294.04922697>texts to extract entity relations. The final output is a knowledge base</word> <word top=517.3472304 left=53.475 bottom=528.1517424 right=295.532213056>with entity relations classified to be correct. SystemX’s machine-</word> <word top=528.3062304 left=53.798 bottom=539.1107424 right=295.513135232>learning based approach comes with a series of technical challenges.</word> <word top=544.1016912 left=53.502 bottom=555.7669776 right=289.8446336>Technical Challenges The challenges in designing SystemX are:</word> <word top=555.9842304 left=53.502 bottom=566.7887424 right=294.045522643>(1) Reasoning about entity-relation candidates that are manifested</word> <word top=566.9432304 left=53.798 bottom=577.7477424 right=170.159097344>in heterogeneous formats (e.g.,</word> <word top=566.9432304 left=173.478996608 bottom=577.7477424 right=294.047128832>text and tables) and span across</word> <word top=577.9022304 left=53.798 bottom=588.7067424 right=125.720004992>an entire document</word> <word top=577.9022304 left=128.921009792 bottom=588.7067424 right=294.047812096>requires SystemX’s machine learning model</word> <word top=588.8612304 left=53.798 bottom=599.6657424 right=294.047128832>to analyze heterogeneous, document-level contexts. While deep</word> <word top=599.8192304 left=53.798 bottom=610.6237424 right=294.04635303>learning models such as recurrent neural networks [2] are effective</word> <word top=610.7782304 left=53.475 bottom=621.5827424 right=294.043148653>with sentence- or paragraph-level context [21], they fall short with</word> <word top=621.7372304 left=53.798 bottom=632.5417424 right=294.047128832>document-level contexts, such as contexts that span both textual</word> <word top=632.6962304 left=53.798 bottom=643.5007424 right=144.057189632>and visual features (e.g.,</word> <word top=632.6962304 left=146.636284928 bottom=643.5007424 right=295.537882496>information conveyed via fonts or align-</word> <word top=643.6552304 left=53.798 bottom=654.4597424 right=294.0454848>ment) [20]. Developing such models is an open challenge and active</word> <word top=654.6142304 left=53.798 bottom=665.4187424 right=129.967568>area of research [20].</word> <word top=665.5732304 left=53.502 bottom=676.3777424 right=295.534545792>(2) The heterogeneity of contexts in richly formatted data magni-</word> <word top=676.5322304 left=53.798 bottom=687.3367424 right=294.046456352>fies the need for large amounts of training data. Manual annotation</word> <word top=687.4912304 left=53.798 bottom=698.2957424 right=295.529400282>is prohibitively expensive, especially when domain expertise is re-</word> <word top=698.4502304 left=53.798 bottom=709.2547424 right=294.047128832>quired. At the same time, human-curated KBs, which can be used</word> </paragraph><paragraph><word top=84.7812304 left=317.955 bottom=95.5857424 right=459.283934784>to generate training data, may exhibit</word> <word top=84.7812304 left=462.530668224 bottom=95.5857424 right=558.204128832>low coverage or not exist</word> <word top=95.7402304 left=317.955 bottom=106.5447424 right=558.204128832>altogether. Alternatively, weak supervision sources can be used to</word> <word top=106.6992304 left=317.955 bottom=117.5037424 right=558.35046048>programmatically create large training sets, but it is often unclear</word> <word top=117.6582304 left=317.955 bottom=128.4627424 right=559.76804832>how to consistently apply these sources to richly formatted data.</word> <word top=128.6162304 left=317.534 bottom=139.4207424 right=558.203115008>Whereas patterns in unstructured data can be identified based on text</word> <word top=139.5752304 left=317.955 bottom=150.3797424 right=558.20622697>alone, expressing patterns consistently across different modalities in</word> <word top=150.5342304 left=317.955 bottom=161.3387424 right=447.698808>richly formatted data is challenging.</word> <word top=161.4932304 left=317.659 bottom=172.2977424 right=518.828433088>(3) Considering candidates across an entire document</word> <word top=161.4932304 left=521.965417792 bottom=172.2977424 right=558.200792128>leads to a</word> <word top=172.4522304 left=317.955 bottom=183.2567424 right=558.204128832>combinatorial explosion of possible candidates, and thus random</word> <word top=183.4112304 left=317.731 bottom=194.2157424 right=559.77089824>variables, which need to be considered during learning and inference.</word> <word top=194.3702304 left=317.677 bottom=205.1747424 right=421.106037952>This leads to a fundamental</word> <word top=194.3702304 left=423.968650816 bottom=205.1747424 right=558.200500672>tension between building a practical</word> <word top=205.3292304 left=317.955 bottom=216.1337424 right=558.202335552>KBC system and learning accurate models that exhibit high recall. In</word> <word top=216.2882304 left=317.955 bottom=227.0927424 right=558.204084>addition, the combinatorial explosion of possible candidates results</word> <word top=227.2472304 left=317.955 bottom=238.0517424 right=558.205500691>in a large class imbalance, where the number of “True” candidates</word> <word top=238.2052304 left=317.955 bottom=249.0097424 right=559.319862816>is much smaller than the number of “False” candidates. Therefore,</word> <word top=249.1642304 left=317.955 bottom=259.9687424 right=559.692838157>techniques to prune candidates to balance running time and end-to-</word> <word top=260.1232304 left=317.955 bottom=270.9277424 right=405.60156>end quality are required.</word> <word top=275.9186912 left=317.659 bottom=287.5839776 right=552.362568>Technical Contributions Our main contributions are as follows:</word> <word top=287.8012304 left=317.659 bottom=298.6057424 right=559.325716672>(1) To account for the breadth of signals in richly formatted data,</word> <word top=298.7602304 left=317.632 bottom=309.5647424 right=558.20364144>we design a new data model that preserves structural and semantic</word> <word top=309.7192304 left=317.955 bottom=320.5237424 right=558.197906176>information across different data modalities. The role of SystemX’s</word> <word top=320.6782304 left=317.955 bottom=331.4827424 right=359.330273472>data model</word> <word top=320.6782304 left=362.604444096 bottom=331.4827424 right=402.891375936>is twofold:</word> <word top=320.6782304 left=406.16554656 bottom=331.4827424 right=416.829465408>(1)</word> <word top=320.6782304 left=420.103636032 bottom=331.4827424 right=558.204128832>to allow users to specify multimodal</word> <word top=331.6372304 left=317.955 bottom=342.4417424 right=558.205690368>domain knowledge that SystemX leverages to automate the KBC</word> <word top=342.5962304 left=317.955 bottom=353.4007424 right=558.201346304>process over richly formatted data, and (2) to provide SystemX’s</word> <word top=353.5552304 left=317.955 bottom=364.3597424 right=558.205742784>machine learning model with the necessary representation to reason</word> <word top=364.5142304 left=317.955 bottom=375.3187424 right=482.174616>about document-wide context (see Section 3).</word> <word top=375.4722304 left=317.659 bottom=386.2767424 right=558.201527424>(2) We empirically show that existing deep learning models [45]</word> <word top=386.4312304 left=317.955 bottom=397.2357424 right=559.68735072>tailored for text information extraction (such as long short-term mem-</word> <word top=397.3902304 left=317.955 bottom=408.1947424 right=558.20094089>ory networks (LSTM) [17]) struggle to capture the multimodality of</word> <word top=408.3492304 left=317.955 bottom=419.1537424 right=558.423626304>richly formatted data. We introduce a multimodal LSTM network</word> <word top=419.3082304 left=317.955 bottom=430.1127424 right=558.202335552>that combines textual context with universal features that correspond</word> <word top=430.2672304 left=317.955 bottom=441.0717424 right=558.204128832>to structural and visual properties of the input documents. These</word> <word top=441.2262304 left=317.955 bottom=452.0307424 right=558.204728192>features are inherently captured by SystemX’s data model and are</word> <word top=452.1852304 left=317.955 bottom=462.9897424 right=558.202335552>generated automatically (see Section 4.2). We also introduce a series</word> <word top=463.1442304 left=317.955 bottom=473.9487424 right=558.202052326>of data layout optimizations to ensure the scalability of SystemX to</word> <word top=474.1032304 left=317.955 bottom=484.9077424 right=519.6721008>millions of document-wide candidates (see Section 4.4).</word> <word top=485.0622304 left=317.659 bottom=495.8667424 right=559.68696128>(3) SystemX introduces a programming model where no develop-</word> <word top=496.0202304 left=317.955 bottom=506.8247424 right=558.204128832>ment cycles are spent on feature engineering. Users only need to</word> <word top=506.9792304 left=317.955 bottom=517.7837424 right=558.20260352>specify candidates, the potential entries in the target KB, and provide</word> <word top=517.9382304 left=317.955 bottom=528.7427424 right=559.6863224>lightweight supervision rules which capture a user’s domain knowl-</word> <word top=528.8972304 left=317.955 bottom=539.7017424 right=558.204128832>edge and programmatically label subsets of candidates, which are</word> <word top=539.8562304 left=317.955 bottom=550.6607424 right=559.77093705>used for training SystemX’s deep learning model (see Section 4.3).</word> <word top=550.8152304 left=317.534 bottom=561.6197424 right=559.766891264>We conduct a user study to evaluate SystemX’s programming model.</word> <word top=561.7742304 left=317.534 bottom=572.5787424 right=559.694585984>We find that when working with richly formatted data users uti-</word> <word top=572.7332304 left=317.955 bottom=583.5377424 right=558.204128832>lize the semantics from multiple modalities of the data, including</word> <word top=583.6922304 left=317.955 bottom=594.4967424 right=558.511228032>both structural and textual information in the document. Our study</word> <word top=594.6512304 left=317.955 bottom=605.4557424 right=558.204955968>demonstrates that given 30 minutes, SystemX’s programming model</word> <word top=605.6092304 left=317.955 bottom=616.4137424 right=558.351715776>allows users on average to attain F1 scores that are 23.3 points higher</word> <word top=616.5682304 left=317.955 bottom=627.3727424 right=558.204084>than traditional forms of supervision that rely on manually labeling</word> <word top=627.5272304 left=317.955 bottom=638.3317424 right=413.5726896>candidates (see Section 6).</word> </paragraph><paragraph><word top=641.9374374 left=327.918 bottom=653.8327818 right=558.204983552>Summary of Results. SystemX-based systems have been put in</word> <word top=654.0012304 left=317.955 bottom=664.8057424 right=559.688041133>production in a range of academic and industrial uses cases, includ-</word> <word top=664.9602304 left=317.955 bottom=675.7647424 right=559.68788704>ing a major online retailer. SystemX comes with several advance-</word> <word top=675.9192304 left=317.955 bottom=686.7237424 right=558.204128832>ments over prior KBC systems (see Appendix D): (1) In contrast</word> <word top=686.8782304 left=317.955 bottom=697.6827424 right=559.692165677>to prior systems that focus on adjacent textual data, system can ex-</word> <word top=697.8372304 left=317.955 bottom=708.6417424 right=559.319907648>tract document-level entity relations expressed in diverse formats,</word> </paragraph></div><div id=3><paragraph><word top=54.19144 left=53.798 bottom=65.2918432 right=214.7628128>SystemX: Knowledge Base Construction</word> <word top=65.15044 left=53.798 bottom=76.2508432 right=163.367408>from Richly Formatted Data</word> </paragraph><paragraph><word top=84.7812304 left=53.798 bottom=95.5857424 right=294.043953702>ranging from textual to tabular formats; (2) SystemX reasons about</word> <word top=95.7402304 left=53.798 bottom=106.5447424 right=294.045335552>multimodal context, i.e., both textual and visual characteristics of the</word> <word top=106.6992304 left=53.798 bottom=117.5037424 right=295.53035072>input documents, to extract more accurate entity relations; (3) In con-</word> <word top=117.6582304 left=53.798 bottom=128.4627424 right=294.041964186>trast to prior KBC systems that rely heavily on feature engineering</word> <word top=128.6162304 left=53.798 bottom=139.4207424 right=294.045892333>to achieve high quality [33], SystemX obviates the need for feature</word> <word top=139.5752304 left=53.798 bottom=150.3797424 right=294.04608873>engineering by extending a bidirectional LSTM—the de-facto deep</word> <word top=150.5342304 left=53.798 bottom=161.3387424 right=294.04419687>learning standard in natural language processing [23]—to obtain a</word> <word top=161.4932304 left=53.798 bottom=172.2977424 right=294.358083584>representation needed to automate relation extraction from richly</word> <word top=172.4522304 left=53.798 bottom=183.2567424 right=295.53134656>formatted data. We evaluate SystemX in four real-world richly for-</word> <word top=183.4112304 left=53.798 bottom=194.2157424 right=294.28052>matted information extraction applications and show that SystemX</word> <word top=194.3702304 left=53.798 bottom=205.1747424 right=295.537882496>enables users to build high-quality KBs, achieving an average im-</word> <word top=205.3292304 left=53.798 bottom=216.1337424 right=275.6536352>provement of 41 F1 points over state-of-the-art KBC systems.</word> <word top=219.3089216 left=53.798 bottom=234.8626368 right=59.7756>2</word> <word top=219.3089216 left=71.7308 bottom=234.8626368 right=158.0234336>BACKGROUND</word> <word top=238.7272304 left=53.377 bottom=249.5317424 right=278.1915472>We review concepts and terminology used in the next sections.</word> <word top=252.8542628 left=53.798 bottom=267.0470019 right=67.434375>2.1</word> <word top=252.8542628 left=78.343475 bottom=267.0470019 right=218.8417739>Knowledge Base Construction</word> <word top=270.8312304 left=53.52 bottom=281.6357424 right=294.044460077>The input to a KBC system is a collection of documents. The output</word> <word top=281.7892304 left=53.798 bottom=292.5937424 right=61.416391424>of</word> <word top=281.7892304 left=64.827747968 bottom=292.5937424 right=294.047128832>the system is a relational database containing facts extracted</word> <word top=292.7482304 left=53.798 bottom=303.5527424 right=294.044788602>from the input and stored in an appropriate schema. To describe the</word> <word top=303.7072304 left=53.798 bottom=314.5117424 right=142.00854656>KBC process, we adopt</word> <word top=303.7072304 left=144.907742336 bottom=314.5117424 right=294.047128832>the standard terminology from the KBC</word> <word top=314.8302304 left=53.798 bottom=325.6347424 right=294.047539898>community2. There are four types of objects that play integral roles</word> <word top=325.7892304 left=53.798 bottom=336.5937424 right=295.165257446>in KBC systems: (1) entities, (2) relations, (3) mentions of entities,</word> <word top=336.7482304 left=53.798 bottom=347.5527424 right=146.6086>and (4) relation mentions.</word> </paragraph><paragraph><word top=347.7072304 left=63.761 bottom=358.5117424 right=295.53657984>An entity e in a knowledge base corresponds to a distinct real-</word> <word top=358.6662304 left=53.475 bottom=369.4707424 right=294.044229312>world person, place, or object. These entities can be grouped into</word> <word top=369.6252304 left=53.798 bottom=380.4297424 right=295.537017344>different entity types T1, T2, . . . , Tn. Furthermore, entities partici-</word> <word top=380.5842304 left=53.798 bottom=391.3887424 right=294.044735552>pate in relationships. A relationship between n entities is represented</word> <word top=391.5432304 left=53.798 bottom=402.3477424 right=294.0456224>as an n-ary relation R(e1, e2, . . . , en) and is described by a schema</word> <word top=404.4300064 left=53.798 bottom=413.0646496 right=294.042749696>SR(T1, T2, . . . , Tn) where ei ∈ Ti. A mention m is a span of text</word> <word top=413.4602304 left=53.798 bottom=424.2647424 right=295.531543251>that refers to an entity. KBC systems typically assume that all men-</word> <word top=424.4192304 left=53.798 bottom=435.2237424 right=294.047128832>tions of entities in a document have a corresponding span of text</word> <word top=435.3782304 left=53.798 bottom=446.1827424 right=294.042517798>that refers to them. A relation mention candidate is an n-ary tuple</word> <word top=448.2650064 left=53.798 bottom=456.8996496 right=159.369592>c = (m1, m2, . . . , mn) that</word> <word top=446.3372304 left=162.598033984 bottom=457.1417424 right=294.049582528>represents an instance of a relation</word> <word top=459.2240064 left=53.798 bottom=467.8586496 right=294.044053056>R(e1, e2, . . . , en) in a document. A candidate classified as true is</word> <word top=468.2552304 left=53.798 bottom=479.0597424 right=205.4016>called a relation mention, denoted by rR.</word> </paragraph><paragraph><word top=485.2956336 left=63.761 bottom=496.0015152 right=294.046858688>Example 2.1 (KBC). Consider the HasCollectorCurrent task in</word> <word top=496.3612304 left=53.798 bottom=507.1657424 right=294.044756928>Figure 1. SystemX takes a corpus of transistor datasheets as input</word> <word top=507.3202304 left=53.798 bottom=518.1247424 right=294.360325184>and constructs a KB containing the (transistor part, current) binary</word> <word top=517.3556912 left=53.798 bottom=529.0209776 right=294.371415814>relation as output. Parts like SMBT3904 and currents like 200mA</word> <word top=529.2382304 left=53.798 bottom=540.0427424 right=165.109341824>are entities. The spans of text</word> <word top=529.2382304 left=168.045120512 bottom=540.0427424 right=295.63717184>that read “SMBT3904” and “200”</word> <word top=540.1972304 left=53.502 bottom=551.0017424 right=294.04289632>(boxed in blue and green, respectively) are mentions of those two</word> <word top=551.1562304 left=53.798 bottom=561.9607424 right=294.044455443>entities, and together they form a candidate. If the evidence in the</word> <word top=562.1152304 left=53.798 bottom=572.9197424 right=140.856184832>document suggests that</word> <word top=562.1152304 left=143.993169536 bottom=572.9197424 right=260.848136192>these two mentions are related,</word> <word top=562.1152304 left=263.985120896 bottom=572.9197424 right=294.047128832>then the</word> <word top=573.0742304 left=53.798 bottom=583.8787424 right=294.645019597>output KB will include the relation mention (SMBT3904, 200mA)</word> <word top=584.0332304 left=53.798 bottom=594.8377424 right=183.2907488>of the HasCollectorCurrent relation.</word> </paragraph><section_header><word top=601.1802304 left=53.52 bottom=611.9847424 right=198.2466624>The KBC problem is defined as follows:</word> </section_header><paragraph><word top=618.2206336 left=63.761 bottom=628.9265152 right=294.041880768>Definition 2.2 (Knowledge Base Construction). Given a set of</word> <word top=629.2872304 left=53.798 bottom=640.0917424 right=293.54516906>documents D and a KB schema SR(T1, T2, . . . , Tn), where each Ti</word> <word top=640.2462304 left=53.798 bottom=651.0507424 right=295.165432>corresponds to an entity type, extract a set of relations rR from D,</word> <word top=651.2052304 left=53.475 bottom=662.0097424 right=218.3312304>which populate the schema’s relational tables.</word> </paragraph><paragraph><word top=668.3522304 left=63.761 bottom=679.1567424 right=295.50352>Like other machine-learning based KBC systems [7, 34], SystemX</word> <word top=679.3112304 left=53.798 bottom=690.1157424 right=294.048500691>converts KBC to a statistical learning and inference problem: each</word> <word top=698.9357745 left=53.798 bottom=705.538572 right=154.3692608>2http://www.itl.nist.gov/iad/mig/tests/ace/</word> </paragraph><section_header><word top=65.15044 left=375.8352224 bottom=76.2508432 right=558.202832>SIGMOD’18, June 2018, Houston, Texas USA</word> </section_header><paragraph><word top=84.7812304 left=317.955 bottom=95.5857424 right=558.204128832>candidate is assigned a Boolean random variable that can take the</word> <word top=95.7402304 left=317.731 bottom=106.5447424 right=374.105267392>value “True” if</word> <word top=95.7402304 left=377.169086272 bottom=106.5447424 right=558.35510368>the corresponding relation mention is correct, or</word> <word top=106.6992304 left=316.763 bottom=117.5037424 right=558.201073472>“False” otherwise. In machine-learning based KBC systems, each</word> <word top=117.6582304 left=317.955 bottom=128.4627424 right=558.206146272>candidate is associated with certain features that provide evidence</word> <word top=128.6162304 left=317.955 bottom=139.4207424 right=380.1459504>on the value that</word> <word top=128.6162304 left=382.999417536 bottom=139.4207424 right=559.76804832>the corresponding random variable should take.</word> <word top=139.5752304 left=317.955 bottom=150.3797424 right=559.687727309>Machine-learning based KBC systems use machine learning to max-</word> <word top=150.5342304 left=317.955 bottom=161.3387424 right=558.357678432>imize the probability of correctly classifying candidates, given their</word> <word top=161.4932304 left=317.955 bottom=172.2977424 right=446.5600752>features and ground truth examples.</word> <word top=175.4222628 left=317.955 bottom=189.6150019 right=331.591375>2.2</word> <word top=175.4222628 left=342.500475 bottom=189.6150019 right=472.4605833>Recurrent Neural Networks</word> <word top=193.3332304 left=317.677 bottom=204.1377424 right=558.203830592>The machine learning model we use in SystemX is based on a</word> <word top=204.2922304 left=317.955 bottom=215.0967424 right=559.690309632>recurrent neural network (RNN). RNNs have obtained state-of-the-</word> <word top=215.2512304 left=317.955 bottom=226.0557424 right=559.3226672>art results in many NLP tasks, including information extraction [14,</word> <word top=226.2102304 left=317.283 bottom=237.0147424 right=548.277708992>15, 41]. RNNs take sequential data as input. For each element</word> <word top=226.2102304 left=551.085447488 bottom=237.0147424 right=558.200823872>in</word> <word top=237.1692304 left=317.955 bottom=247.9737424 right=558.203447386>the input sequence, the information from previous inputs can affect</word> <word top=248.1282304 left=317.955 bottom=258.9327424 right=389.483738688>the network output</word> <word top=248.1282304 left=393.032281152 bottom=258.9327424 right=403.6962>for</word> <word top=248.1282304 left=407.244742464 bottom=258.9327424 right=558.204128832>the current element. For sequential data</word> <word top=261.0150064 left=317.955 bottom=269.6496496 right=559.41055273>{x1, ..., xT }, the structure of an RNN is mathematically described as:</word> </paragraph><section_header><word top=276.3940064 left=367.337 bottom=285.0286496 right=508.81634304>ht = f(xt, ht−1), y = g({h1, ..., hT })</word> </section_header><paragraph><word top=289.8462304 left=317.632 bottom=300.6507424 right=558.202389696>where ht is the hidden state for element t, and y is the representation</word> <word top=300.8052304 left=317.955 bottom=311.6097424 right=559.7706336>generated by considering the sequence of hidden states {h1, ..., hT }.</word> <word top=311.7642304 left=317.955 bottom=322.5687424 right=558.20295328>Functions f and g correspond to nonlinear transformations. In typical</word> <word top=322.7232304 left=317.955 bottom=333.5277424 right=557.70043372>RNNs, f = tanh(Whxt + Uhht−1 + bh) where Wh, Uh, and bh</word> <word top=333.6822304 left=317.955 bottom=344.4867424 right=558.4283472>are parameter metrics and a vector, and g is chosen based on the task</word> <word top=344.6402304 left=317.955 bottom=355.4447424 right=346.8447408>in hand.</word> </paragraph><paragraph><word top=360.4366912 left=317.955 bottom=372.1019776 right=558.19943296>Long Short-term Memory (LSTM) LSTM [17] networks are a</word> <word top=372.3182304 left=317.955 bottom=383.1227424 right=558.20014775>special type of RNN which introduce new structures referred to as</word> <word top=383.1696336 left=317.955 bottom=393.8755152 right=559.686841728>gates, which control the flow of information which can capture long-</word> <word top=394.2362304 left=317.955 bottom=405.0407424 right=557.70334913>term dependencies. There are three types of gates: “input” gates it</word> <word top=405.1952304 left=317.955 bottom=415.9997424 right=558.204128832>control which values are updated in a memory cell; “forget” gates</word> <word top=418.0820064 left=317.955 bottom=426.7166496 right=557.70334913>ft control which values remain in memory; and “output” gates ot</word> <word top=427.1132304 left=317.955 bottom=437.9177424 right=558.203321856>control which values in memory are used to compute the output of</word> <word top=438.0722304 left=317.955 bottom=448.8767424 right=504.6264816>the cell. The final structure of an LSTM is given by:</word> </paragraph><paragraph><word top=455.3790064 left=344.058 bottom=464.0136496 right=459.13934304>it = σ(Wixt + Uiht−1 + bi)</word> <word top=469.6220064 left=343.52 bottom=478.2566496 right=460.62334304>ft = σ(Wfxt + Ufht−1 + bf)</word> <word top=483.8640064 left=342.238 bottom=492.4986496 right=464.15734304>ot = σ(Woxt + Uoht−1 + bo)</word> <word top=498.1060064 left=342.919 bottom=506.7406496 right=534.44434304>ct = ft ◦ ct−1 + it ◦ tanh(Wcxt + Ucht−1 + bc)</word> <word top=512.3480064 left=341.709 bottom=520.9826496 right=412.44234304>ht = ot ◦ tanh(ct)</word> </paragraph><paragraph><word top=525.8182304 left=317.632 bottom=536.6227424 right=558.201223258>where ct is the cell state vector, W, U, b are parameter metrics and</word> <word top=536.7772304 left=317.955 bottom=547.5817424 right=557.7243376>a vector, σ is the sigmoid function, and ◦ is the Hadamard product.</word> </paragraph><paragraph><word top=547.7362304 left=327.918 bottom=558.5407424 right=559.773125875>Bidirectional LSTMs consist of forward and backward LSTMs.</word> <word top=558.6952304 left=317.677 bottom=569.4997424 right=559.691146093>The forward LSTM fF reads the sequence from x1 to xT and calcu-</word> <word top=569.9202304 left=317.955 bottom=580.7247424 right=475.15461931>lates a sequence of forward hidden states (hF</word> <word top=574.7062997 left=470.607 bottom=583.4699032 right=499.74761931>1 , ..., hF</word> <word top=576.3719302 left=495.2 bottom=583.3755403 right=558.199926272>T ). The backward</word> <word top=582.4162304 left=317.955 bottom=593.2207424 right=559.690683328>LSTM fB reads the sequence from xT to x1 and calculates a se-</word> <word top=593.6412304 left=317.955 bottom=604.4457424 right=461.52752359>quence of backward hidden states (hB</word> <word top=598.4272997 left=455.697 bottom=607.1909032 right=487.85652359>1 , ..., hB</word> <word top=600.0929302 left=482.026 bottom=607.0965403 right=558.19725664>T ). The final hidden</word> <word top=604.6002304 left=317.955 bottom=615.4047424 right=559.694882496>state for the sequential is the concatenation the forward and back-</word> <word top=615.8252304 left=317.632 bottom=626.6297424 right=440.04861931>ward hidden states, e.g., hi = [hF</word> <word top=622.2769302 left=435.501 bottom=629.2805403 right=455.68852359>i , hB</word> <word top=622.2769302 left=449.858 bottom=629.2805403 right=460.9226>i ].</word> </paragraph><paragraph><word top=631.6206912 left=317.632 bottom=643.2859776 right=558.203540787>Attention Previous work explored using pooling strategies to train</word> <word top=643.5032304 left=317.955 bottom=654.3077424 right=558.351459712>the model, such as max pooling [39], which makes it difficult for</word> <word top=654.4622304 left=317.955 bottom=665.2667424 right=558.351715776>RNNs to capture all the information contained in a sequence. In order</word> <word top=665.4212304 left=317.955 bottom=676.2257424 right=558.19937664>to benefit by giving more attention to all parts of the sequence, the</word> <word top=676.2726336 left=317.955 bottom=686.9785152 right=558.204712896>attention mechanism is proposed to capture the information of whole</word> <word top=687.3392304 left=317.955 bottom=698.1437424 right=558.199242944>sequence [44]. SystemX uses a bidirectional LSTM with attention to</word> <word top=698.1906336 left=317.955 bottom=708.8965152 right=558.203904672>automatically represent textual features of relation candidates from</word> </paragraph></div><div id=4><figure bbox=61.8190025583,49.216824,679.803048,590.5116585></figure><figure_caption><word top=217.6026912 left=53.798 bottom=229.2679776 right=558.2008656>Figure 2: An overview of SystemX KBC over richly formatted data. Given a set of richly formatted documents and a series of</word> <word top=228.5616912 left=53.798 bottom=240.2269776 right=422.64684>lightweight inputs from the user, SystemX extracts facts and stores them in a relational database.</word> </figure_caption><section_header><word top=65.15044 left=53.798 bottom=76.2508432 right=236.1656096>SIGMOD’18, June 2018, Houston, Texas USA</word> </section_header><figure_caption><word top=385.6116912 left=112.242 bottom=397.2769776 right=235.601584>Figure 3: SystemX’s data model.</word> </figure_caption><paragraph><word top=411.2236336 left=53.798 bottom=421.9295152 right=294.048027392>the documents. We extend this LSTM with additional features to</word> <word top=422.2892304 left=53.798 bottom=433.0937424 right=142.699856>capture other modalities.</word> <word top=436.8469216 left=53.798 bottom=452.4006368 right=59.7756>3</word> <word top=436.8469216 left=71.7308 bottom=452.4006368 right=225.9453568>THE SystemX FRAMEWORK</word> <word top=456.4572304 left=53.475 bottom=467.2617424 right=294.04050048>An overview of SystemX is shown in Figure 2. SystemX takes as</word> <word top=467.4162304 left=53.798 bottom=478.2207424 right=294.043004288>input a collection of richly formatted documents and a collection of</word> <word top=478.3752304 left=53.798 bottom=489.1797424 right=294.043614003>user inputs. It follows a machine-learning based approach to extract</word> <word top=489.3342304 left=53.798 bottom=500.1387424 right=294.047128832>relations from the input documents. The entity relations extracted</word> <word top=500.2932304 left=53.798 bottom=511.0977424 right=235.0177904>by SystemX are stored in a target knowledge base.</word> </paragraph><paragraph><word top=511.2522304 left=63.761 bottom=522.0567424 right=294.048452096>We introduce SystemX’s data model for representing different</word> <word top=522.2112304 left=53.798 bottom=533.0157424 right=294.041249933>properties of richly formatted data. We then review SystemX’s data</word> <word top=533.1692304 left=53.798 bottom=543.9737424 right=294.047128832>processing pipeline and describe the new programming paradigm</word> <word top=544.1282304 left=53.798 bottom=554.9327424 right=271.2780192>introduced by SystemX for KBC from richly formatted data.</word> <word top=558.8322628 left=53.798 bottom=573.0250019 right=67.434375>3.1</word> <word top=558.417717 left=78.344 bottom=573.1122747 right=183.4740542>SystemX’s Data Model</word> <word top=577.0012304 left=53.52 bottom=587.8057424 right=294.048637632>The design of SystemX was strongly guided by interactions with</word> <word top=587.9602304 left=53.798 bottom=598.7647424 right=294.045335552>collaborators (see the user study in Section 6). We find that to support</word> <word top=598.9192304 left=53.798 bottom=609.7237424 right=272.7647184>KBC over richly formatted data SystemX’s data model must:</word> </paragraph><section_header><word top=611.8702304 left=69.738 bottom=622.6747424 right=294.044082662>• Generically capture documents of diverse types ranging from</word> </section_header><section_header><word top=622.8292304 left=78.207 bottom=633.6337424 right=239.8532592>PDFs to HTML to XML in a unified manner.</word> </section_header><section_header><word top=635.7812304 left=69.738 bottom=646.5857424 right=294.04483584>• Preserve attributes from all modalities of the document since</word> </section_header><section_header><word top=646.7392304 left=78.207 bottom=657.5437424 right=235.4866224>they contain valuable semantic information.</word> </section_header><section_header><word top=659.6912304 left=69.738 bottom=670.4957424 right=275.5395312>• Serve as an abstraction for system and user interaction.</word> </section_header><paragraph><word top=671.5393632 left=63.761 bottom=683.5005408 right=147.15068992>SystemX’s data model</word> <word top=672.6422304 left=150.26023744 bottom=683.4467424 right=277.212087808>is a directed acyclic graph (DAG)</word> <word top=672.6422304 left=280.330781056 bottom=683.4467424 right=294.049373056>that</word> <word top=683.6012304 left=53.798 bottom=694.4057424 right=294.045335552>contains a hierarchy of contexts, whose structure reflects the intuitive</word> <word top=694.5602304 left=53.798 bottom=705.3647424 right=294.047128832>hierarchy of document components. In this graph, each node is a</word> <word top=705.4116336 left=53.798 bottom=716.1175152 right=80.027947904>context</word> <word top=705.5192304 left=83.11 bottom=716.3237424 right=294.047070592>(represented as boxes in Figure 3). The root of the DAG</word> </paragraph><paragraph><word top=257.6792304 left=317.955 bottom=268.4837424 right=558.204626688>is a Document, which contains Section contexts. Each section is</word> <word top=268.6382304 left=317.955 bottom=279.4427424 right=558.202713094>divided into: Texts, Tables, and Figures. Texts can contain multiple</word> <word top=279.4896336 left=317.677 bottom=290.1955152 right=558.202270464>Paragraphs; Tables and Figures can contain Captions; Tables can</word> <word top=290.5562304 left=317.955 bottom=301.3607424 right=559.770976>also contain Rows and Columns, which are in turn made up of Cells.</word> <word top=301.5142304 left=317.955 bottom=312.3187424 right=558.199914944>Each context ultimately breaks down into Paragraphs that are parsed</word> <word top=312.4732304 left=317.955 bottom=323.2777424 right=559.692765632>into Sentences. In Figure 3, a downward edge indicates a parent-</word> <word top=323.4322304 left=317.955 bottom=334.2367424 right=415.5811632>contains-child relationship.</word> </paragraph><paragraph><word top=334.3912304 left=327.918 bottom=345.1957424 right=559.686384922>In addition, for each context, we store the textual contents, point-</word> <word top=345.3502304 left=317.955 bottom=356.1547424 right=558.197565427>ers to the parent contexts, and a wide range of attributes from each</word> <word top=356.3092304 left=317.955 bottom=367.1137424 right=558.204128832>modality found in the original document. For example, standard</word> <word top=367.2682304 left=317.955 bottom=378.0727424 right=558.198569664>NLP pre-processing tools are used to generate linguistic attributes</word> <word top=378.2272304 left=317.955 bottom=389.0317424 right=559.326157229>such as lemmas, part of speech tags, named entity recognition tags,</word> <word top=389.1862304 left=317.955 bottom=399.9907424 right=402.443235264>dependency paths, etc.</word> <word top=389.1862304 left=405.644240064 bottom=399.9907424 right=558.35040096>for each Sentence. Structural and tabular</word> <word top=400.1452304 left=317.955 bottom=410.9497424 right=559.322148826>attributes of a Sentence, such as tags, and row/column information,</word> <word top=411.1032304 left=317.955 bottom=421.9077424 right=558.202335552>and parent attributes, can be easily captured while traversing its path</word> <word top=422.0622304 left=317.955 bottom=432.8667424 right=558.518670144>in the data model. Visual attributes for the document are recorded by</word> <word top=433.0212304 left=317.955 bottom=443.8257424 right=558.200676768>storing bounding box and page information, if applicable, for each</word> <word top=443.9802304 left=317.632 bottom=454.7847424 right=387.7576>word in a Sentence.</word> </paragraph><paragraph><word top=461.9266336 left=327.918 bottom=472.6325152 right=429.508746624>Example 3.1 (Data Model).</word> <word top=462.0342304 left=433.988 bottom=472.8387424 right=477.759454208>In Figure 1,</word> <word top=462.0342304 left=480.466589696 bottom=472.8387424 right=559.686885632>the data model repre-</word> <word top=472.9932304 left=317.955 bottom=483.7977424 right=558.354638643>senting the PDF contains one Section with three children: a Text for</word> <word top=483.9512304 left=317.955 bottom=494.7557424 right=558.354725952>the document header, a Text for the table description, and a Table for</word> <word top=494.9102304 left=317.955 bottom=505.7147424 right=558.201928576>the table itself (with ten Rows and four Columns where each Cell</word> <word top=505.8692304 left=317.955 bottom=516.6737424 right=558.203755072>links to both a Row and Column). The Text and Cell contexts contain</word> <word top=516.7206336 left=317.677 bottom=527.4265152 right=414.4686>Paragraphs and Sentences.</word> </paragraph><paragraph><word top=534.8822304 left=327.918 bottom=545.6867424 right=558.200795904>To construct the DAG for each document, we extract all the words</word> <word top=545.8402304 left=317.955 bottom=556.6447424 right=558.205742784>in their original order. For structural and tabular information we use</word> <word top=556.9642304 left=317.955 bottom=567.7687424 right=558.950215104>tools such as Poppler3 to convert an input file into HTML format;</word> <word top=567.9222304 left=317.955 bottom=578.7267424 right=558.198139277>for visual information such as coordinates and bounding boxes we</word> <word top=578.8812304 left=317.955 bottom=589.6857424 right=385.011477696>use a PDF printer</word> <word top=578.8812304 left=388.230773952 bottom=589.6857424 right=541.604632512>to convert an input file into PDF format.</word> <word top=578.8812304 left=544.833074496 bottom=589.6857424 right=558.204128832>If a</word> <word top=589.8402304 left=317.955 bottom=600.6447424 right=558.202335552>conversion occurred, we associate the multimodal information in the</word> <word top=600.7992304 left=317.955 bottom=611.6037424 right=558.202335552>converted file with all extracted words. We align the word sequences</word> <word top=611.7582304 left=317.955 bottom=622.5627424 right=558.35046048>of the converted file with their originals by checking if both their</word> <word top=622.7172304 left=317.955 bottom=633.5217424 right=558.204128832>characters and number of repeated occurrences before the current</word> <word top=633.6762304 left=317.632 bottom=644.4807424 right=558.517444173>word are the same. SystemX can recover from conversion errors by</word> <word top=644.6352304 left=317.955 bottom=655.4397424 right=544.6973232>using the inherent redundancy in signals from other modalities.</word> </paragraph><section_header><word top=706.4137745 left=317.955 bottom=713.016572 right=396.1341712>3https://poppler.freedesktop.org/</word> </section_header><paragraph><word top=659.1864374 left=327.918 bottom=671.0817818 right=559.319526976>Takeaways. SystemX consolidates multiple document formats,</word> <word top=671.2492304 left=317.955 bottom=682.0537424 right=558.202335552>multiple common types of contexts, and multiple modality semantics</word> <word top=682.2082304 left=317.955 bottom=693.0127424 right=558.206428096>into a single model. SystemX’s data model serves as the formal</word> </paragraph></div><div id=5><paragraph><word top=54.19144 left=53.798 bottom=65.2918432 right=214.7628128>SystemX: Knowledge Base Construction</word> <word top=65.15044 left=53.798 bottom=76.2508432 right=163.367408>from Richly Formatted Data</word> </paragraph><paragraph><word top=92.6712304 left=53.798 bottom=103.4757424 right=294.045335552>representation of the intermediate data utilized in all future stages of</word> <word top=103.6302304 left=53.798 bottom=114.4347424 right=134.092112>the extraction process.</word> <word top=117.5952628 left=53.798 bottom=131.7880019 right=67.434375>3.2</word> <word top=117.5952628 left=78.343475 bottom=131.7880019 right=244.105858>User Inputs and SystemX’s Pipeline</word> <word top=135.5182304 left=53.52 bottom=146.3227424 right=294.358930528>The SystemX processing pipeline follows three phases. We briefly</word> <word top=146.4772304 left=53.798 bottom=157.2817424 right=294.362853709>describe each phase in turn and focus on the user inputs required by</word> <word top=157.4362304 left=53.798 bottom=168.2407424 right=266.5345488>each phase. SystemX’s internals are described in Section 4.</word> </paragraph><paragraph><word top=173.2316912 left=53.502 bottom=184.8969776 right=294.048806272>(1) KBC Initialization The first phase in SystemX’s pipeline is to</word> <word top=185.1142304 left=53.798 bottom=195.9187424 right=295.611478707>initialize the target KB where the extracted relations will be stored.</word> <word top=196.0732304 left=53.798 bottom=206.8777424 right=294.045309491>During this phase, SystemX requires that the user specifies a target</word> <word top=206.9246336 left=53.798 bottom=217.6305152 right=294.043137382>schema that corresponds to the relations to be extracted. The target</word> <word top=217.9912304 left=53.798 bottom=228.7957424 right=294.043944448>schema SR(T1, . . . , Tn) defines a relation R to be extracted from the</word> <word top=228.9492304 left=53.798 bottom=239.7537424 right=292.7884256>input documents. An example of such a schema is provided below.</word> </paragraph><paragraph><word top=245.6836336 left=63.761 bottom=256.3895152 right=294.195700992>Example 3.2 (Relation Schema). An example SQL schema for</word> <word top=256.7502304 left=53.798 bottom=267.5547424 right=146.6988704>the relation in Figure 1 is:</word> <word top=272.0297088 left=53.516 bottom=280.5836544 right=99.76289792>CREATE TABLE</word> <word top=271.8085376 left=104.279 bottom=280.5836544 right=178.8824368>H a s C o l l e c t o r C u r r e n t (</word> <word top=278.7825376 left=62.414 bottom=287.5576544 right=113.7735392>T r a n s i s t o r P a r t</word> <word top=279.0037088 left=119.323 bottom=287.5576544 right=143.76779744>varchar</word> <word top=278.7825376 left=146.279 bottom=287.5576544 right=148.04298976>,</word> <word top=285.7565376 left=62.245 bottom=294.5316544 right=87.2702224>Current</word> <word top=285.9777088 left=92.741 bottom=294.5316544 right=125.17659488>varchar ) ;</word> </paragraph><paragraph><word top=306.8763632 left=63.761 bottom=318.8375408 right=294.355317952>SystemX uses the user-specified schema to initialize an empty</word> <word top=318.9382304 left=53.798 bottom=329.7427424 right=295.537882496>relational database where the output KB will be stored. Further-</word> <word top=329.8962304 left=53.798 bottom=340.7007424 right=294.042959616>more, SystemX iterates over its input corpus and transforms each</word> <word top=340.8552304 left=53.798 bottom=351.6597424 right=294.041744659>document into SystemX’s data model to capture the variability and</word> <word top=351.8142304 left=53.798 bottom=362.6187424 right=217.927952>multimodality of richly formatted documents.</word> </paragraph><paragraph><word top=367.6096912 left=53.502 bottom=379.2749776 right=294.043009856>(2) Candidate Generation In this phase, SystemX extracts relation</word> <word top=379.4922304 left=53.798 bottom=390.2967424 right=294.047128832>candidates from the input documents. Here, users are required to</word> <word top=390.4512304 left=53.798 bottom=401.2557424 right=295.6137344>provide two types of input functions: (1) matchers and (2) throttlers.</word> <word top=401.4102304 left=53.377 bottom=412.2147424 right=204.102184>We review each of these functions in turn.</word> </paragraph><paragraph><word top=417.3222544 left=53.404 bottom=428.8709776 right=294.045443558>Matchers To generate candidates for Relation R, SystemX requires</word> <word top=429.0882304 left=53.798 bottom=439.8927424 right=294.0463328>that users define matchers for all distinct mention types in schema</word> <word top=441.9750064 left=53.798 bottom=450.6096496 right=294.048838144>SR. Matchers are how users specify what a mention looks like. In</word> <word top=449.9033632 left=53.798 bottom=461.8645408 right=294.047070848>SystemX, matchers are Python functions which accept as input a</word> <word top=461.9652304 left=53.798 bottom=472.7697424 right=294.04237664>span of text—which has a reference to its data model—and output</word> <word top=472.9242304 left=53.475 bottom=483.7287424 right=294.046363315>whether or not the match conditions are met. Matchers range from</word> <word top=483.8822304 left=53.798 bottom=494.6867424 right=294.047128832>simple regular expressions to complicated functions that take into</word> <word top=494.8412304 left=53.798 bottom=505.6457424 right=268.229456>account signals across multiple modalities of the input data.</word> </paragraph><paragraph><word top=511.5756336 left=63.761 bottom=522.2815152 right=294.043404045>Example 3.3 (Matchers). From the HasCollectorCurrent relation</word> <word top=522.6422304 left=53.798 bottom=533.4467424 right=294.367229312>in Figure 1, users define matchers for each type of the schema. A</word> <word top=533.6012304 left=53.798 bottom=544.4057424 right=295.61104832>dictionary of valid transistor parts can be used as the first matcher.</word> <word top=544.5602304 left=53.798 bottom=555.3647424 right=294.045335552>For maximum current, users can exploit the pattern that these values</word> <word top=555.5182304 left=53.798 bottom=566.3227424 right=294.047128832>are commonly expressed as a numerical value in the interval 100</word> </paragraph><section_header><word top=623.0208816 left=244.653 bottom=630.995 right=247.82830112>0</word> </section_header><paragraph><word top=650.5552544 left=53.251 bottom=662.1039776 right=294.044081728>Throttlers Users can optionally provide throttlers, which act as</word> <word top=662.3212304 left=53.798 bottom=673.1257424 right=123.90000512>hard filtering rules</word> <word top=662.3212304 left=127.457693312 bottom=673.1257424 right=294.047128832>to reduce the number of candidates that are</word> <word top=673.2802304 left=53.798 bottom=684.0847424 right=294.047128832>materialized. Throttlers are also Python functions, but rather than</word> <word top=684.2382304 left=53.798 bottom=695.0427424 right=173.671056896>accepting spans of text as input,</word> <word top=684.2382304 left=176.597689856 bottom=695.0427424 right=294.047128832>they operate on candidates, and</word> <word top=695.1972304 left=53.798 bottom=706.0017424 right=295.61104832>output whether or not a candidate meets the specified condition.</word> <word top=706.1562304 left=53.52 bottom=716.9607424 right=291.4316>Throttlers limit the number of candidates considered by SystemX.</word> </paragraph><section_header><word top=65.15044 left=375.8352224 bottom=76.2508432 right=558.202832>SIGMOD’18, June 2018, Houston, Texas USA</word> </section_header><paragraph><word top=92.5636336 left=327.918 bottom=103.2695152 right=559.690647616>Example 3.4 (Throttler). Continuing the example shown in Fig-</word> <word top=103.6302304 left=317.955 bottom=114.4347424 right=558.204128832>ure 1, the user provides a throttler which only keeps currents that</word> <word top=114.5892304 left=317.955 bottom=125.3937424 right=438.149592>have “Value” as a column header.</word> </paragraph><paragraph><word top=185.2122304 left=327.918 bottom=196.0167424 right=559.690368512>Given the input matchers and throttlers, SystemX extracts rela-</word> <word top=196.1712304 left=317.955 bottom=206.9757424 right=558.200452608>tion candidates by traversing its data model representation of each</word> <word top=207.1302304 left=317.955 bottom=217.9347424 right=498.592273728>document. By applying matchers to each leaf of</word> <word top=207.1302304 left=501.64694688 bottom=217.9347424 right=559.319907648>the data model,</word> <word top=216.9863632 left=317.955 bottom=228.9475408 right=558.206103226>SystemX can generate sets of mentions for each component of the</word> <word top=229.0482304 left=317.955 bottom=239.8527424 right=557.0081904>schema. The cross-product of these mentions produces candidates:</word> </paragraph><section_header><word top=245.4218064 left=346.267 bottom=258.5844816 right=470.4759072>Candidate(idcandidate, mention1,</word> <word top=245.4218064 left=473.5513824 bottom=258.5844816 right=530.60887008>. . . , mentionn)</word> </section_header><paragraph><word top=266.9062304 left=317.632 bottom=277.7107424 right=558.204457216>where mentions are spans of text and contain pointers to their context</word> <word top=277.8652304 left=317.955 bottom=288.6697424 right=558.204128832>in the data model of their respective document. The output of this</word> <word top=288.8242304 left=317.955 bottom=299.6287424 right=425.7116>phase is a set of candidates C.</word> </paragraph><paragraph><word top=304.6196912 left=317.659 bottom=316.2849776 right=560.13352>(3) Training a multimodal LSTM for KBC In this phase, SystemX</word> <word top=316.5022304 left=317.955 bottom=327.3067424 right=389.858713536>trains a multimodal</word> <word top=316.5022304 left=392.529266112 bottom=327.3067424 right=558.204128832>long short-term memory network (LSTM) to</word> <word top=327.3536336 left=317.955 bottom=338.0595152 right=559.800617664>classify the candidates generated during Phase 1 as “True” or “False”</word> <word top=338.4192304 left=317.955 bottom=349.2237424 right=558.203996992>mentions of target relations. SystemX’s multimodal LSTM combines</word> <word top=349.3782304 left=317.955 bottom=360.1827424 right=558.204236429>both visual and textual features. Recent work has also proposed the</word> <word top=360.3372304 left=317.955 bottom=371.1417424 right=559.770685312>use of LSTMs for KBC but have focused only on textual data [45].</word> <word top=371.2962304 left=317.955 bottom=382.1007424 right=558.200614003>In Section 5.3.3 we experimentally demonstrate that state-of-the-art</word> <word top=382.2552304 left=317.955 bottom=393.0597424 right=558.513541363>LSTMs struggle to capture the multimodal characteristics of richly</word> <word top=393.2142304 left=317.955 bottom=404.0187424 right=559.77522144>formatted data which results in poor-quality KBs (see Section 5.3.3).</word> <word top=403.0703632 left=327.918 bottom=415.0315408 right=558.2049312>SystemX uses a bidirectional LSTM (reviewed in Section 2.2) to</word> <word top=415.1322304 left=317.955 bottom=425.9367424 right=559.68735072>capture textual features and extends it with additional structural, tab-</word> <word top=426.0912304 left=317.955 bottom=436.8957424 right=558.20237129>ular and visual features captured by SystemX’s data model. The full</word> <word top=437.0502304 left=317.955 bottom=447.8547424 right=558.203869056>LSTM used by SystemX is described in Section 4.2. The training</word> <word top=448.0092304 left=317.955 bottom=458.8137424 right=559.692812858>phase in SystemX is split in two sub-phases: (1) a multimodal fea-</word> <word top=458.9672304 left=317.955 bottom=469.7717424 right=558.203590848>turization phase, and (2) a phase where supervision data is collected</word> <word top=469.9262304 left=317.955 bottom=480.7307424 right=452.1999408>by the user. We describe each in turn:</word> </paragraph><paragraph><word top=485.8382544 left=317.561 bottom=497.3869776 right=558.205621184>Multimodal Featurization Here, SystemX traverses its internal data</word> <word top=497.6042304 left=317.955 bottom=508.4087424 right=558.202335552>model instance for each input document and automatically generates</word> <word top=508.5632304 left=317.955 bottom=519.3677424 right=558.204084>features that correspond to structural, tabular, and visual modalities</word> <word top=519.5222304 left=317.955 bottom=530.3267424 right=558.202335552>as described Section 4.2. These features provide an extended feature</word> <word top=530.4812304 left=317.955 bottom=541.2857424 right=558.204128832>library (shown as feature_lib, below), which augments the textual</word> <word top=541.4402304 left=317.955 bottom=552.2447424 right=558.20622697>features learned by the LSTM. All generated features are stored in a</word> <word top=552.3992304 left=317.955 bottom=563.2037424 right=347.8400112>relation:</word> </paragraph><section_header><word top=568.7728064 left=330.644 bottom=581.9354816 right=469.8539072>Features(idcandidate, LSTMtextual,</word> <word top=568.7728064 left=472.9293824 bottom=581.9354816 right=546.23187008>feature_libothers)</word> </section_header><paragraph><word top=590.2572304 left=317.632 bottom=601.0617424 right=558.202341146>A detailed list of the features extracted during this phase is provided</word> <word top=601.2162304 left=317.955 bottom=612.0207424 right=558.199996032>in Appendix B. No user input is required during this step. SystemX’s</word> <word top=612.0676336 left=317.704 bottom=622.7735152 right=491.3204032>LSTM obviates the need for feature engineering.</word> </paragraph><paragraph><word top=628.0872544 left=317.704 bottom=639.6359776 right=558.199258477>Supervision To train its multimodal LSTM, SystemX requires that</word> <word top=639.8532304 left=317.955 bottom=650.6577424 right=559.685889197>users provide some form of supervision. Collecting sufficient train-</word> <word top=650.8112304 left=317.955 bottom=661.6157424 right=558.202335552>ing data for multi-context deep learning models is a well-established</word> <word top=661.7702304 left=317.955 bottom=672.5747424 right=558.204128832>challenge. Taking into account a context of more than a handful</word> <word top=672.7292304 left=317.955 bottom=683.5337424 right=558.204128832>of words for text-based deep learning models requires very large</word> <word top=683.6882304 left=317.955 bottom=694.4927424 right=487.5365232>training corpora, as stated by LeCun et al. [20].</word> </paragraph><paragraph><word top=694.6472304 left=327.918 bottom=705.4517424 right=558.202615066>To soften the burden of traditional supervision, SystemX uses a</word> <word top=705.6062304 left=317.955 bottom=716.4107424 right=558.20056608>supervision paradigm referred to as data programming [31]. Data</word> </paragraph></div><div id=6><header><word top=65.15044 left=53.798 bottom=76.2508432 right=236.1656096>SIGMOD’18, June 2018, Houston, Texas USA</word> </header><paragraph><word top=92.6712304 left=53.798 bottom=103.4757424 right=294.042555968>programming is a human-in-the-loop paradigm for training machine</word> <word top=103.6302304 left=53.798 bottom=114.4347424 right=294.358083584>learning systems. In data programming users only need to specify</word> <word top=114.5892304 left=53.798 bottom=125.3937424 right=294.044137984>lightweight functions, referred to as labeling functions (LFs), that</word> <word top=125.5472304 left=53.798 bottom=136.3517424 right=294.370457216>programmatically assign labels to the input data. A detailed overview</word> <word top=136.5062304 left=53.798 bottom=147.3107424 right=229.4856416>of data programming is provided in Appendix A.</word> </paragraph><paragraph><word top=147.4652304 left=63.761 bottom=158.2697424 right=294.04765408>As a result, SystemX requires that users specify a set of labeling</word> <word top=158.4242304 left=53.798 bottom=169.2287424 right=294.047128832>functions that noisily label the candidates from Phase 2. Labeling</word> <word top=169.3832304 left=53.798 bottom=180.1877424 right=269.165387648>functions in SystemX correspond to Python functions that</word> <word top=169.3832304 left=272.000563328 bottom=180.1877424 right=294.041767808>take a</word> <word top=180.3422304 left=53.798 bottom=191.1467424 right=294.042224576>candidate as input and assign +1 to label a candidate as “True”, 0 to</word> <word top=191.3012304 left=53.798 bottom=202.1057424 right=294.0454336>abstain, or −1 to label a candidate as “False”. An example labeling</word> <word top=202.2602304 left=53.798 bottom=213.0647424 right=143.7578912>function is shown below:</word> </paragraph><paragraph><word top=218.7946336 left=63.761 bottom=229.5005152 right=294.048420499>Example 3.5 (Labeling Functions). Looking at the datasheet in</word> <word top=229.8612304 left=53.798 bottom=240.6657424 right=294.047128832>Figure 1, users can express patterns such as having the Part and</word> <word top=240.8202304 left=53.798 bottom=251.6247424 right=170.698695296>Current y-aligned on the visual</word> <word top=240.8202304 left=173.762514176 bottom=251.6247424 right=219.491154176>rendering of</word> <word top=240.8202304 left=222.554973056 bottom=251.6247424 right=295.162907648>the page. Similarly,</word> <word top=251.7792304 left=53.798 bottom=262.5837424 right=294.045335552>users can write a rule that labels a candidate as true if it has the word</word> <word top=262.7382304 left=52.606 bottom=273.5427424 right=266.0242528>“current” in the same row as the candidate current mention.</word> </paragraph><paragraph><word top=361.8882304 left=63.761 bottom=372.6927424 right=294.044637146>As shown in Example 3.5, SystemX’s internal data model allows</word> <word top=372.8472304 left=53.798 bottom=383.6517424 right=294.047236429>users to specify labeling functions that capture supervision patterns</word> <word top=383.8062304 left=53.798 bottom=394.6107424 right=294.361849472>across any modality of the data (see Section 4.3). In our user-study</word> <word top=394.7652304 left=53.475 bottom=405.5697424 right=294.047941402>we find that it is common for users to write labeling functions that</word> <word top=405.7242304 left=53.798 bottom=416.5287424 right=295.537882496>span multiple modalities and consider both textual and visual pat-</word> <word top=416.6832304 left=53.798 bottom=427.4877424 right=189.7644896>terns of the input data (see Section 6).</word> </paragraph><paragraph><word top=427.6422304 left=63.761 bottom=438.4467424 right=294.043795904>The user-specified labeling functions together with the candidates</word> <word top=438.6012304 left=53.798 bottom=449.4057424 right=208.965321728>generated by SystemX are passed as input</word> <word top=438.6012304 left=211.56270848 bottom=449.4057424 right=294.04607328>to Snorkel [32], a data</word> <word top=449.5602304 left=53.798 bottom=460.3647424 right=294.359760301>programming engine, which converts the noisy labels generated by</word> <word top=460.5192304 left=53.798 bottom=471.3237424 right=294.047128832>the input labeling functions to denoised labeled data used to train</word> <word top=470.3743632 left=53.798 bottom=482.3355408 right=255.363152>SystemX’s multimodal LSTM model (see Appendix A).</word> </paragraph><paragraph><word top=487.3902544 left=53.502 bottom=498.9389776 right=294.046627008>Classification SystemX uses its trained LSTM to assign a marginal</word> <word top=499.1552304 left=53.798 bottom=509.9597424 right=294.04607808>probability to each candidate. The last layer of SystemX’s LSTM</word> <word top=510.1142304 left=53.798 bottom=520.9187424 right=294.04314775>is a softmax classifier (described in Section 4.2) that computes the</word> <word top=521.0732304 left=53.798 bottom=531.8777424 right=295.165768>probability of a candidate being a “True” entity relation. In SystemX,</word> <word top=532.0322304 left=53.798 bottom=542.8367424 right=294.048691968>users can specify a threshold over the output marginal probabilities to</word> <word top=542.9912304 left=53.798 bottom=553.7957424 right=294.044196819>determine which candidates will be satisfied as “True” (those whose</word> <word top=553.9502304 left=53.798 bottom=564.7547424 right=294.643215104>marginal probability of being true exceeds the specified threshold)</word> <word top=564.9092304 left=53.798 bottom=575.7137424 right=294.045335552>and which are “False” (those whose marginal probability fall beneath</word> <word top=575.8682304 left=53.798 bottom=586.6727424 right=294.047128832>the threshold). This threshold depends on the requirements of the</word> <word top=586.8272304 left=53.798 bottom=597.6317424 right=294.047128832>application. Applications that require critically high accuracy can</word> <word top=597.7862304 left=53.798 bottom=608.5907424 right=294.047128832>set a high threshold value to ensure only candidates with a high</word> <word top=608.7442304 left=53.798 bottom=619.5487424 right=220.6447712>probability of being true are classified as such.</word> </paragraph><paragraph><word top=619.7032304 left=63.761 bottom=630.5077424 right=294.36201344>As shown in Figure 2, supervision and classification are typically</word> <word top=630.6622304 left=53.798 bottom=641.4667424 right=295.613729274>executed for several iterations as users develop a KBC application.</word> <word top=641.6212304 left=53.52 bottom=652.4257424 right=295.531833408>This feedback loop allows users to quickly receive feedback and im-</word> <word top=652.5802304 left=53.798 bottom=663.3847424 right=294.04451961>prove their labeling functions, and avoids the overhead of rerunning</word> <word top=663.5392304 left=53.798 bottom=674.3437424 right=279.258128>candidate extraction and materializing features (see Section 6).</word> <word top=677.4002628 left=53.798 bottom=691.5930019 right=67.434375>3.3</word> <word top=676.985717 left=78.344 bottom=691.6802747 right=269.0559437>SystemX’s Programming Model for KBC</word> <word top=695.2882304 left=53.52 bottom=706.0927424 right=295.530972634>Traditionally, machine-learned based KBC focuses on feature engi-</word> <word top=706.2472304 left=53.798 bottom=717.0517424 right=294.047128832>neering to obtain high quality KBs. This requires that users rerun</word> </paragraph><paragraph><word top=92.6712304 left=317.955 bottom=103.4757424 right=558.202335552>feature extraction, learning and inference after every modification of</word> <word top=103.6302304 left=317.955 bottom=114.4347424 right=559.688260096>the features used during KBC. With SystemX’s machine learning ap-</word> <word top=114.5892304 left=317.955 bottom=125.3937424 right=558.2007216>proach features are generated automatically. This puts emphasis on</word> <word top=125.5472304 left=317.659 bottom=136.3517424 right=558.201401843>(1) specifying the relation candidates, and (2) providing supervision</word> <word top=136.5062304 left=317.955 bottom=147.3107424 right=500.9950896>rules via data programming and labeling functions.</word> <word top=146.3623632 left=327.918 bottom=158.3235408 right=558.2015312>SystemX’s programming paradigm obviates the need for feature</word> <word top=158.4242304 left=317.955 bottom=169.2287424 right=558.43752>engineering and introduces two modes of operation for SystemX</word> <word top=169.3832304 left=317.955 bottom=180.1877424 right=559.685889197>applications: (1) development, and (2) production. During develop-</word> <word top=180.3422304 left=317.955 bottom=191.1467424 right=558.204128832>ment, labeling functions are iteratively improved. LFs are applied</word> <word top=191.3012304 left=317.955 bottom=202.1057424 right=558.35046048>to a small sample of labeled candidates and evaluated by the user</word> <word top=202.2602304 left=317.955 bottom=213.0647424 right=558.200614003>on their accuracy and coverage (the fraction of candidates receiving</word> <word top=213.2192304 left=317.955 bottom=224.0237424 right=559.686400282>non-zero labels). In practice, approximately 20 iterations are suffi-</word> <word top=224.1782304 left=317.955 bottom=234.9827424 right=558.204128832>cient for our users to generate a sufficiently tuned set of labeling</word> <word top=235.1372304 left=317.955 bottom=245.9417424 right=466.765140288>functions (see Section 6). In production,</word> <word top=235.1372304 left=469.399109952 bottom=245.9417424 right=559.694882496>the finalized LFs are ap-</word> <word top=246.0952304 left=317.955 bottom=256.8997424 right=558.204128832>plied to the entire set of candidates and learning and inference are</word> <word top=257.0542304 left=317.955 bottom=267.8587424 right=482.2995856>performed only once to generate the final KB.</word> </paragraph><paragraph><word top=268.0132304 left=327.918 bottom=278.8177424 right=558.198285312>On average, only a small number of LFs are needed to achieve</word> <word top=278.9722304 left=317.955 bottom=289.7767424 right=559.687726067>high quality KBC (see Section 6). For example, in the ELECTRON-</word> <word top=289.9312304 left=551.085788736 bottom=300.7357424 right=558.20116512>to</word> <word top=291.7048041 left=318.179 bottom=300.3483896 right=548.012824128>ICS application, 16 labeling functions on average is sufficient</word> <word top=300.8902304 left=317.955 bottom=311.6947424 right=558.202335552>achieve an average F1 score of over 75 F1 points. Finally, in contrast</word> <word top=311.8492304 left=317.955 bottom=322.6537424 right=558.35046048>to the common belief that users rely mostly on textual signals for</word> <word top=322.8082304 left=317.955 bottom=333.6127424 right=474.493280448>supervising the KBC process, we find that</word> <word top=322.8082304 left=477.163833024 bottom=333.6127424 right=559.694882496>tabular and visual pat-</word> <word top=333.7672304 left=317.955 bottom=344.5717424 right=558.515083584>terns are valuable forms of weak supervision for KBC over richly</word> <word top=344.7262304 left=317.955 bottom=355.5307424 right=427.7665008>formatted data (see Section 6).</word> </paragraph><paragraph><word top=360.0889216 left=317.955 bottom=375.6426368 right=323.9326>4</word> <word top=360.0889216 left=335.8878 bottom=375.6426368 right=421.1449425>KBC IN SystemX</word> <word top=379.9682304 left=317.534 bottom=390.7727424 right=512.1934>We now review each component of SystemX in detail.</word> </paragraph><paragraph><word top=395.4772628 left=317.955 bottom=409.6700019 right=331.591375>4.1</word> <word top=395.4772628 left=342.500475 bottom=409.6700019 right=514.4060728>Candidate Generation from RF Data</word> <word top=413.9142304 left=317.955 bottom=424.7187424 right=558.204128832>Candidate generation from richly formatted data relies on access</word> <word top=424.8732304 left=317.955 bottom=435.6777424 right=558.200329984>to document-level contexts, which is provided by SystemX’s data</word> <word top=435.8322304 left=317.955 bottom=446.6367424 right=558.198569664>model. Due to the significantly increased context needed for KBC</word> <word top=446.7912304 left=317.955 bottom=457.5957424 right=559.694882496>from richly formatted data, naïvely materializing all possible can-</word> <word top=457.7502304 left=317.955 bottom=468.5547424 right=559.694882496>didates is intractable as the number of candidates grows combina-</word> <word top=468.7092304 left=317.955 bottom=479.5137424 right=558.200676768>torially with the number of relation arguments. This combinatorial</word> <word top=479.6682304 left=317.955 bottom=490.4727424 right=558.35046048>explosion can lead to performance issues for KBC systems. For</word> <word top=490.6272304 left=317.955 bottom=501.4317424 right=559.694205504>example, in the ELECTRONICS domain, just 100 documents can gen-</word> <word top=501.5862304 left=317.955 bottom=512.3907424 right=558.204128832>erate over 1M candidates. In addition, we find that the majority of</word> <word top=512.5442304 left=317.955 bottom=523.3487424 right=558.204084>these candidates do not express true relations, creating a significant</word> <word top=523.5032304 left=317.955 bottom=534.3077424 right=528.6026352>class imbalance that can hinder learning performance [18].</word> </paragraph><paragraph><word top=534.4622304 left=327.918 bottom=545.2667424 right=558.205484288>To address this combinatorial explosion, SystemX allows users</word> <word top=545.4212304 left=317.955 bottom=556.2257424 right=558.20014775>to specify throttlers, in addition to matchers, to prune away excess</word> <word top=556.3802304 left=317.955 bottom=567.1847424 right=460.6911216>candidates. We find that throttlers must:</word> </paragraph><paragraph><word top=569.3322304 left=333.895 bottom=580.1367424 right=545.7936832>• keep high accuracy by only filtering negative candidates.</word> <word top=582.2832304 left=333.895 bottom=593.0877424 right=476.053024>• seek high coverage of the candidates.</word> </paragraph><paragraph><word top=595.2342304 left=317.955 bottom=606.0387424 right=559.694882496>and can be viewed as a knob that allows users to tradeoff preci-</word> <word top=606.1932304 left=317.955 bottom=616.9977424 right=558.204128832>sion and recall and promote scalability by reducing the number of</word> <word top=617.1522304 left=317.955 bottom=627.9567424 right=459.9110448>candidates to be classified during KBC.</word> </paragraph><paragraph><word top=628.1112304 left=327.918 bottom=638.9157424 right=561.979236864>Figure 4 shows how using throttlers affects the quality-performance</word> <word top=639.0702304 left=317.955 bottom=649.8747424 right=559.69166464>tradeoff in the ELECTRONICS domain. We see that throttling signifi-</word> <word top=650.0292304 left=317.955 bottom=660.8337424 right=558.197735789>cantly improves system performance. However, increased throttling</word> <word top=660.9882304 left=317.955 bottom=671.7927424 right=559.687727309>does not monotonically improve quality as it hurts recall. This trade-</word> <word top=671.9472304 left=317.955 bottom=682.7517424 right=558.203590848>off captures the fundamental tension between optimizing for system</word> <word top=682.9062304 left=317.955 bottom=693.7107424 right=559.690533792>performance and optimizing for end-to-end quality. When no candi-</word> <word top=693.8652304 left=317.955 bottom=704.6697424 right=558.202326586>dates are pruned, the class imbalance resulting from many negative</word> <word top=704.8232304 left=317.955 bottom=715.6277424 right=558.204128832>candidates to the relatively small number of positive candidates</word> </paragraph></div><div id=7><paragraph><word top=54.19144 left=53.798 bottom=65.2918432 right=214.7628128>SystemX: Knowledge Base Construction</word> <word top=65.15044 left=53.798 bottom=76.2508432 right=163.367408>from Richly Formatted Data</word> </paragraph><figure bbox=89.807223988,51.857677616,196.997096378,290.42082644></figure><paragraph><word top=209.5166912 left=53.798 bottom=221.1819776 right=294.0437216>Figure 4: Tradeoff between (a) quality and (b) execution time</word> <word top=220.4756912 left=53.475 bottom=232.1409776 right=270.01356>when pruning the number of candidates using throttlers.</word> </paragraph><paragraph><word top=253.3772304 left=53.798 bottom=264.1817424 right=294.047128832>harms quality. Thus, as a rule of thumb, we recommend that users</word> <word top=264.3362304 left=53.798 bottom=275.1407424 right=294.28052>use throttlers to balance negative and positive candidates. SystemX</word> <word top=275.2952304 left=53.798 bottom=286.0997424 right=294.045335552>provides users with mechanisms to evaluate this balance over a small</word> <word top=286.2542304 left=53.798 bottom=297.0587424 right=221.783504>holdout development labeled set of candidates.</word> </paragraph><paragraph><word top=300.5024374 left=63.761 bottom=312.3977818 right=295.534427917>Takeaways. SystemX’s data model is necessary to perform can-</word> <word top=312.5662304 left=53.798 bottom=323.3707424 right=295.53582919>didate generation with richly formatted data. Pruning negative can-</word> <word top=323.5252304 left=53.798 bottom=334.3297424 right=294.047128832>didates via throttlers to balance negative and positive candidates</word> <word top=334.4842304 left=53.798 bottom=345.2887424 right=294.042637696>not only ensures the scalability of SystemX but also improves the</word> <word top=345.4422304 left=53.798 bottom=356.2467424 right=164.9539072>precision of SystemX’s output.</word> <word top=359.7432628 left=53.798 bottom=373.9360019 right=67.434375>4.2</word> <word top=359.7432628 left=78.343475 bottom=373.9360019 right=293.8418364>Multimodal LSTM for Richly Formatted Data</word> <word top=377.7782304 left=53.377 bottom=388.5827424 right=297.892189952>We now describe SystemX’s deep learning model in detail. SystemX’s</word> <word top=388.7372304 left=53.798 bottom=399.5417424 right=294.04608873>model extends a bidirectional LSTM (Bi-LSTM), the de-facto deep</word> <word top=399.6962304 left=53.798 bottom=410.5007424 right=295.534976704>learning standard for natural language processing [23], with addi-</word> <word top=410.6552304 left=53.798 bottom=421.4597424 right=74.631968384>tional</word> <word top=410.6552304 left=77.842118912 bottom=421.4597424 right=219.546028544>features (the extended feature library)</word> <word top=410.6552304 left=222.747033344 bottom=421.4597424 right=294.358083584>to capture not only</word> <word top=421.6142304 left=53.798 bottom=432.4187424 right=294.047128832>textual features but also features from other data modalities such</word> <word top=432.5732304 left=53.798 bottom=443.3777424 right=295.531543251>as structural, tabular, and visual semantics. In Section 5.3.2 we per-</word> <word top=443.5322304 left=53.798 bottom=454.3367424 right=294.04865312>form an ablation study demonstrating that features from non-textual</word> <word top=454.4912304 left=53.798 bottom=465.2957424 right=294.361670144>modes are key to obtaining high-quality KBs. We find that the quality</word> <word top=465.4502304 left=53.798 bottom=476.2547424 right=294.045335552>of the output KB can deteriorate up to 33 F1 points when non-textual</word> <word top=476.4082304 left=53.798 bottom=487.2127424 right=294.372104704>features are removed. Figure 5 illustrates SystemX’s LSTM. We now</word> <word top=487.3672304 left=53.798 bottom=498.1717424 right=217.7831776>review each component of SystemX’s LSTM.</word> </paragraph><paragraph><word top=503.1626912 left=53.798 bottom=514.8279776 right=191.9791904>Bidirectional LSTM with Attention</word> <word top=504.0862304 left=198.53 bottom=514.8907424 right=247.679142272>Traditionally,</word> <word top=504.0862304 left=250.953312896 bottom=514.8907424 right=294.358937984>the primary</word> <word top=515.0452304 left=53.798 bottom=525.8497424 right=113.38241792>source of signal</word> <word top=515.0452304 left=116.510256896 bottom=525.8497424 right=127.174175744>for</word> <word top=515.0452304 left=130.30201472 bottom=525.8497424 right=294.047128832>relation extraction comes from unstructured</word> <word top=526.0042304 left=53.798 bottom=536.8087424 right=294.041268416>text. In order to understand textual signals, SystemX uses a long</word> <word top=536.9632304 left=53.798 bottom=547.7677424 right=193.462412288>short-term memory network (LSTM)</word> <word top=536.9632304 left=197.038391936 bottom=547.7677424 right=232.478087936>to extract</word> <word top=536.9632304 left=236.044921856 bottom=547.7677424 right=260.81155328>textual</word> <word top=536.9632304 left=264.3783872 bottom=547.7677424 right=295.61104832>features.</word> <word top=547.9222304 left=53.798 bottom=558.7267424 right=279.309636544>For mention candidates, SystemX builds a Bi-LSTM to get</word> <word top=547.9222304 left=282.867324736 bottom=558.7267424 right=294.043404352>the</word> <word top=558.8812304 left=53.798 bottom=569.6857424 right=294.044196819>textual features of the mention candidate from the both directions of</word> <word top=569.8402304 left=53.798 bottom=580.6447424 right=294.044115789>sentences containing the candidate. For sentence si containing the</word> <word top=582.9930064 left=53.798 bottom=591.6276496 right=294.044542189>ith mention candidate in the document, the textual features hik of</word> <word top=592.0242304 left=53.798 bottom=602.8287424 right=294.04882016>each word wik is encoded by both forward (defined as superscript F</word> <word top=602.9832304 left=53.798 bottom=613.7877424 right=294.041827392>in equations) and backward (defined as superscript B) LSTM, which</word> <word top=613.9412304 left=53.798 bottom=624.7457424 right=294.04237664>summarizes information about the whole sentence with a focus on</word> <word top=626.8280064 left=53.798 bottom=635.4626496 right=160.3015808>wik. This takes the structure:</word> </paragraph><paragraph><word top=644.1580064 left=111.739 bottom=652.7926496 right=121.86361931>hF</word> <word top=648.2329302 left=117.316 bottom=655.2365403 right=176.38461931>ik = LST M(hF</word> <word top=648.6549302 left=171.837 bottom=655.6585403 right=236.09668608>i(k−1), Φ(si, k))</word> <word top=661.6340064 left=111.739 bottom=670.2686496 right=123.14652359>hB</word> <word top=665.7089302 left=117.316 bottom=672.7125403 right=177.66752359>ik = LST M(hB</word> <word top=666.1309302 left=171.837 bottom=673.1345403 right=236.09768608>i(k+1), Φ(si, k))</word> <word top=679.1100064 left=111.739 bottom=687.7446496 right=150.60561931>hik = [hF</word> <word top=683.1849302 left=146.058 bottom=690.1885403 right=170.11452359>ik, hB</word> <word top=683.1849302 left=164.284 bottom=690.1885403 right=175.68607264>ik]</word> </paragraph><paragraph><word top=694.3412304 left=53.475 bottom=705.1457424 right=295.52980599>where Φ(si, k) is the word embedding [38] which is the representa-</word> <word top=705.8662304 left=53.798 bottom=716.6707424 right=243.7036>tion of the semantics of the kth word in sentence si.</word> </paragraph><section_header><word top=65.15044 left=375.8352224 bottom=76.2508432 right=558.202832>SIGMOD’18, June 2018, Houston, Texas USA</word> </section_header><paragraph><word top=92.6712304 left=327.918 bottom=103.4757424 right=348.99890304>Then,</word> <word top=92.6712304 left=351.907244544 bottom=103.4757424 right=558.198285312>the textual feature representation for mention candidate</word> <word top=105.5580064 left=317.955 bottom=114.1926496 right=530.251680704>ti is calculated by an attention mechanism [44] to model</word> <word top=103.6302304 left=533.031982016 bottom=114.4347424 right=559.691779136>the im-</word> <word top=114.5892304 left=317.955 bottom=125.3937424 right=558.203974554>portance of different words from the sentence si and aggregate the</word> <word top=125.5472304 left=317.955 bottom=136.3517424 right=558.204128832>feature representation of those informative words to form a final</word> <word top=136.5062304 left=317.955 bottom=147.3107424 right=398.1235824>feature representation,</word> </paragraph><section_header><word top=159.3790064 left=386.413 bottom=168.0136496 right=489.87434304>uik = tanh(Wwhik + bw)</word> </section_header><paragraph><word top=227.7492304 left=317.632 bottom=238.5537424 right=558.205395891>where uik is the hidden representation of hik, and αik is to model</word> <word top=238.7082304 left=317.955 bottom=249.5127424 right=558.2026032>the importance of each word in the sentence si. Special candidate</word> <word top=249.6672304 left=317.955 bottom=260.4717424 right=558.204128832>markers (shown in red in Figure 5) are added to the sentences to</word> <word top=260.6262304 left=317.955 bottom=271.4307424 right=559.319907648>draw emphasized attention to the candidates themselves. Finally,</word> <word top=271.5852304 left=317.955 bottom=282.3897424 right=558.203904672>the textual features of candidate relations is the concatenation of its</word> <word top=282.5442304 left=317.955 bottom=293.3487424 right=452.6346>mentions’ textual features [t1, ..., tn].</word> </paragraph><paragraph><word top=298.3396912 left=317.955 bottom=310.0049776 right=558.200025875>Extended Feature Library Features for structural, tabular, visual</word> <word top=310.2222304 left=317.955 bottom=321.0267424 right=559.694882496>modalities are generated by leveraging the data model, which pre-</word> <word top=321.1802304 left=317.955 bottom=331.9847424 right=558.204128832>serves each modality’s semantics. For each candidate, such as the</word> <word top=332.1392304 left=317.955 bottom=342.9437424 right=558.202686656>candidate (SMBT3904, 200) shown in Figure 5, SystemX locates</word> <word top=343.0982304 left=317.955 bottom=353.9027424 right=558.204128832>each mention in the data model and traverses the directed acyclic</word> <word top=354.0572304 left=317.955 bottom=364.8617424 right=558.200676768>graph to compute features from the modality information stored in</word> <word top=365.0162304 left=317.955 bottom=375.8207424 right=558.200365248>the nodes of the graph. For example, SystemX can traverse sibling</word> <word top=375.9752304 left=317.955 bottom=386.7797424 right=558.204128832>nodes to add tabular features such as featurizing a node based on</word> <word top=386.9342304 left=317.955 bottom=397.7387424 right=558.43752>the other mentions in the same row or column. Similarly, SystemX</word> <word top=397.8932304 left=317.955 bottom=408.6977424 right=558.199663565>can traverse the data model to extract structural features from tags</word> <word top=408.8522304 left=317.955 bottom=419.6567424 right=558.204128832>stored while parsing the document along with the hierarchy of the</word> <word top=419.8112304 left=317.955 bottom=430.6157424 right=527.0783472>document elements themselves. We review each modality:</word> <word top=429.9632544 left=327.918 bottom=441.5119776 right=365.7920736>Structural</word> <word top=429.9632544 left=368.894448 bottom=441.5119776 right=559.690144832>features. These provide signals intrinsic to a docu-</word> <word top=441.7282304 left=317.955 bottom=452.5327424 right=559.689487603>ment’s structure. These features allow SystemX to learn from struc-</word> <word top=452.6872304 left=317.955 bottom=463.4917424 right=559.692838157>tural attributes such as parent and sibling relationships and a candi-</word> <word top=463.6462304 left=317.955 bottom=474.4507424 right=558.202335552>date’s associated XML/HTML tags. For example, structural features</word> <word top=474.6052304 left=317.955 bottom=485.4097424 right=558.525592205>capture tag attributes such as font sizes and styles, shown in yellow</word> <word top=485.5642304 left=317.955 bottom=496.3687424 right=558.351715776>in Figure 5, which are often used to highlight important keywords or</word> <word top=496.5232304 left=317.955 bottom=507.3277424 right=559.691687552>information. SystemX extracts many structural features from the par-</word> <word top=507.4822304 left=317.955 bottom=518.2867424 right=559.766891654>ents and siblings of mentions inside the data model of the document.</word> <word top=517.3383632 left=317.955 bottom=529.2995408 right=558.204621696>SystemX also tracks the structural distance of relations, which helps</word> <word top=529.4002304 left=317.632 bottom=540.2047424 right=558.200512>when candidate mentions are visually distant, but structurally close</word> <word top=540.3592304 left=317.955 bottom=551.1637424 right=558.204128832>together. As an example, featurizing a candidate with the lowest</word> <word top=551.3172304 left=317.955 bottom=562.1217424 right=558.198569664>common ancestor in the data model is a positive signal for linking</word> <word top=562.2762304 left=317.955 bottom=573.0807424 right=430.7702448>table captions to table contents.</word> <word top=572.4282544 left=327.918 bottom=583.9769776 right=558.205657024>Tabular features. These are a special subset of structural features</word> <word top=584.1942304 left=317.955 bottom=594.9987424 right=558.197735789>since tables are very common structures inside documents and have</word> <word top=595.1532304 left=317.955 bottom=605.9577424 right=559.694882496>high information density. Table features are drawn from the grid-</word> <word top=606.1122304 left=317.955 bottom=616.9167424 right=559.319907648>like representation of rows and columns stored in the data model,</word> <word top=617.0712304 left=317.955 bottom=627.8757424 right=558.204128832>shown in green in Figure 5. In addition to the tabular location of</word> <word top=628.0302304 left=317.955 bottom=638.8347424 right=558.202224576>mentions, SystemX also featurizes relations with special signals</word> <word top=638.9892304 left=317.955 bottom=649.7937424 right=558.197565427>such as being in the same row or column. Consider, for example, a</word> <word top=649.9482304 left=317.955 bottom=660.7527424 right=558.204128832>table that contains cells with multiple lines of text; recording that</word> <word top=660.9062304 left=317.955 bottom=671.7107424 right=534.452673216>two entity mentions are in the same row captures a signal</word> <word top=660.9062304 left=537.434180544 bottom=671.7107424 right=558.204128832>that a</word> <word top=671.8652304 left=317.731 bottom=682.6697424 right=470.6439856>visual alignment feature could easily miss.</word> <word top=682.0172544 left=327.918 bottom=693.5659776 right=558.19805344>Visual features. These provide signals observed from a visual</word> <word top=693.7832304 left=317.955 bottom=704.5877424 right=558.202335552>rendering of a document. In cases where tabular or structural features</word> <word top=704.7422304 left=317.955 bottom=715.5467424 right=558.20014775>are noisy—including nearly all documents converted from PDF to</word> </paragraph></div><div id=8><figure bbox=-28.3459720652,-169.33944385,338.427427935,808.72295615></figure><section_header><word top=65.15044 left=53.798 bottom=76.2508432 right=236.1656096>SIGMOD’18, June 2018, Houston, Texas USA</word> </section_header><figure_caption><word top=262.1776912 left=112.377 bottom=273.8429776 right=499.6255248>Figure 5: An illustration of SystemX’s multimodal LSTM for candidate (SMBT3904, 200) in Figure 1.</word> </figure_caption><paragraph><word top=282.3292304 left=53.798 bottom=293.1337424 right=295.53169568>HTML by generic tools—visual features can provide a complemen-</word> <word top=293.2882304 left=53.798 bottom=304.0927424 right=294.047128832>tary view of the dependencies among text. Visual features encode</word> <word top=304.2472304 left=53.798 bottom=315.0517424 right=295.162907648>many highly-predictive types of semantic information implicitly,</word> <word top=315.2062304 left=53.798 bottom=326.0107424 right=294.194213658>such as position on a page, which may imply when text is a title or</word> <word top=326.1652304 left=53.798 bottom=336.9697424 right=250.431152>header. An example of this is shown in red in Figure 5.</word> <word top=341.9606912 left=53.502 bottom=353.6259776 right=295.615515328>Training All parameters of SystemX’s LSTM are jointly trained.</word> <word top=353.8432304 left=53.52 bottom=364.6477424 right=294.046531315>This includes the parameters of the bidirectional LSTM as well as</word> <word top=364.8022304 left=53.798 bottom=375.6067424 right=294.047128832>the weights of the last softmax layer that correspond to additional</word> <word top=375.7612304 left=53.798 bottom=386.5657424 right=294.04237664>features. We use stochastic gradient descent during training with a</word> <word top=386.7202304 left=53.798 bottom=397.5247424 right=294.28052>momentum of 0.9. Unlike typical LSTMs with attention, SystemX</word> <word top=397.6792304 left=53.798 bottom=408.4837424 right=256.6807328>trains the weights for additional features simultaneously.</word> </paragraph><paragraph><word top=414.5684374 left=63.761 bottom=426.4637818 right=295.537720896>Takeaways. To achieve high quality KBC with richly format-</word> <word top=426.6322304 left=53.798 bottom=437.4367424 right=85.112972672>ted data,</word> <word top=426.6322304 left=87.72865088 bottom=437.4367424 right=92.813675648>it</word> <word top=426.6322304 left=95.420208128 bottom=437.4367424 right=120.397191296>is vital</word> <word top=426.6322304 left=123.012869504 bottom=437.4367424 right=295.61104832>to have features from multiple data modalities.</word> <word top=437.5912304 left=53.52 bottom=448.3957424 right=294.044038656>These features are only obtainable through traversing and accessing</word> <word top=448.5502304 left=53.798 bottom=459.3547424 right=211.7680352>modality attributes stored in the data model.</word> </paragraph><paragraph><word top=465.4922628 left=53.798 bottom=479.6850019 right=67.434375>4.3</word> <word top=465.4922628 left=78.343475 bottom=479.6850019 right=191.2853873>Multimodal Supervision</word> <word top=484.4082304 left=53.798 bottom=495.2127424 right=294.045335552>Unlike KBC from unstructured text, KBC from richly formatted data</word> <word top=495.3672304 left=53.798 bottom=506.1717424 right=294.362037766>requires supervision from multiple modalities of the data. In richly</word> <word top=506.3262304 left=53.798 bottom=517.1307424 right=294.048742784>formatted data, useful patterns for KBC are more sparse and hidden</word> <word top=517.2852304 left=53.798 bottom=528.0897424 right=295.537882496>in non-textual signals, which motivates the need to exploit over-</word> <word top=528.2442304 left=53.798 bottom=539.0487424 right=295.61104832>lap and repetition in a variety of patterns over multiple modalities.</word> <word top=538.0993632 left=53.798 bottom=550.0605408 right=294.044918912>SystemX’s data model allows users to directly express correctness</word> <word top=550.1612304 left=53.798 bottom=560.9657424 right=294.045335552>using textual, structural, tabular, or visual characteristics, in addition</word> <word top=561.1202304 left=53.798 bottom=571.9247424 right=295.524527424>to traditional supervision sources like existing KBs. In the ELEC-</word> <word top=573.8528041 left=53.78 bottom=582.4963896 right=294.197488192>TRONICS domain, over 70% of labeling functions written by our</word> <word top=583.0382304 left=53.798 bottom=593.8427424 right=198.62060288>users are based on non-textual signals.</word> <word top=583.0382304 left=201.766733312 bottom=593.8427424 right=207.35477312>It</word> <word top=583.0382304 left=210.482612096 bottom=593.8427424 right=272.115673088>is acceptable for</word> <word top=583.0382304 left=275.252657792 bottom=593.8427424 right=294.047128832>these</word> <word top=593.9972304 left=53.798 bottom=604.8017424 right=294.047128832>labeling functions to be noisy and conflict with one another. Data</word> <word top=604.9562304 left=53.798 bottom=615.7607424 right=294.045335552>programming theory (see Appendix A.2) shows that with a sufficient</word> <word top=615.9152304 left=53.798 bottom=626.7197424 right=294.047128832>number of labeling functions, data programming can still achieve</word> <word top=626.8742304 left=53.798 bottom=637.6787424 right=201.2235488>quality comparable to using labeled data.</word> </paragraph><paragraph><word top=637.8332304 left=63.761 bottom=648.6377424 right=295.166066394>In Section 5.3.4, we find that using metadata, such as structural,</word> <word top=648.7912304 left=53.798 bottom=659.5957424 right=294.196787014>tabular, and visual cues, results in an increase of 66 F1 points over</word> <word top=659.7502304 left=53.798 bottom=670.5547424 right=295.613984>using textual supervision sources alone in the ELECTRONICS domain.</word> <word top=670.7092304 left=53.798 bottom=681.5137424 right=294.045335552>Using both sources gives a further increase of 2 F1 points over the 66</word> <word top=681.6682304 left=53.798 bottom=692.4727424 right=294.043676768>F1 improvement of metadata alone. We also show that supervision</word> <word top=692.6272304 left=53.798 bottom=703.4317424 right=294.045335552>using information from all modalities, rather than textual information</word> <word top=703.5862304 left=53.798 bottom=714.3907424 right=75.894078848>alone,</word> <word top=703.5862304 left=79.433475584 bottom=714.3907424 right=294.047128832>results in an increase of 43 F1 points on average over a</word> </paragraph><paragraph><word top=282.3292304 left=317.731 bottom=293.1337424 right=558.20190377>variety of domains. Using multiple supervision sources is crucial to</word> <word top=293.2882304 left=317.955 bottom=304.0927424 right=558.20385984>achieving high quality information extraction from richly formatted</word> <word top=304.2472304 left=317.955 bottom=315.0517424 right=459.421120704>data. Furthermore, with this approach,</word> <word top=304.2472304 left=462.082527552 bottom=315.0517424 right=558.204128832>the size of the training set</word> <word top=317.1340064 left=317.955 bottom=325.7686496 right=558.200789773>K scales with the amount of unlabeled input data directly, which is</word> <word top=326.1652304 left=317.731 bottom=336.9697424 right=404.4271216>validated in Section 5.4.</word> </paragraph><paragraph><word top=344.8574374 left=327.918 bottom=356.7527818 right=559.687789376>Takeaways. Supervision using multiple modalities of richly for-</word> <word top=356.9212304 left=317.955 bottom=367.7257424 right=559.694882496>matted data is key to achieving high end-to-end quality. Like mul-</word> <word top=367.8792304 left=317.955 bottom=378.6837424 right=345.904344768>timodal</word> <word top=367.8792304 left=348.949872192 bottom=378.6837424 right=558.515083584>featurization, multimodal supervision is also enabled by</word> <word top=377.7353632 left=317.955 bottom=389.6965408 right=400.9664112>SystemX’s data model.</word> </paragraph><paragraph><word top=397.5832628 left=317.955 bottom=411.7760019 right=331.591375>4.4</word> <word top=397.5832628 left=342.500475 bottom=411.7760019 right=457.4599425>Scaling KBC in SystemX</word> <word top=417.0992304 left=317.534 bottom=427.9037424 right=559.689378342>We use two optimizations to guarantee SystemX’s scalability to mil-</word> <word top=428.0582304 left=317.955 bottom=438.8627424 right=558.198462067>lions of candidates (see Section 4.1): (1) data caching, and (2) data</word> <word top=439.0172304 left=317.955 bottom=449.8217424 right=559.76804832>representations that optimize data access during the KBC process.</word> <word top=449.9762304 left=317.955 bottom=460.7807424 right=559.319907648>Such optimization are standard in database systems. Nonetheless,</word> <word top=460.9342304 left=317.955 bottom=471.7387424 right=558.527457216>their impact on KBC has not been studied in detail. We briefly review</word> <word top=471.8932304 left=317.955 bottom=482.6977424 right=559.77522144>each optimization. A detailed description is provided in Appendix C.</word> </paragraph><paragraph><word top=482.8522304 left=327.918 bottom=493.6567424 right=558.203857402>Each relation candidate to be classified by SystemX’s LSTM as</word> <word top=493.8112304 left=316.763 bottom=504.6157424 right=558.201073472>“True” or “False” is associated with a set of entity mentions (see</word> <word top=504.7702304 left=317.955 bottom=515.5747424 right=558.205493888>Section 3.2). For each relation candidate, SystemX’s multimodal</word> <word top=515.7292304 left=317.955 bottom=526.5337424 right=558.513541363>featurization generates features that describe each individual entity</word> <word top=526.6882304 left=317.955 bottom=537.4927424 right=558.20151961>in isolation and features that jointly describe the set of all entities in</word> <word top=537.6472304 left=317.955 bottom=548.4517424 right=558.198632429>the relation candidate (see Appendix B). Since each entity mention</word> <word top=548.6062304 left=317.955 bottom=559.4107424 right=558.200614003>can be associated with many different relation candidates, we cache</word> <word top=559.5652304 left=317.955 bottom=570.3697424 right=558.202335552>the featurization of each entity mention. Caching during featurization</word> <word top=570.5242304 left=317.955 bottom=581.3287424 right=559.322984>results in a 100× speed up on average in the ELECTRONICS domain,</word> <word top=581.4822304 left=317.955 bottom=592.2867424 right=534.3500976>and only accounts for 10% of featurization’s memory usage.</word> </paragraph><paragraph><word top=592.4412304 left=327.918 bottom=603.2457424 right=559.688765824>Recall from Section 3.3 that SystemX’s programming model in-</word> <word top=603.4002304 left=317.955 bottom=614.2047424 right=444.184337856>troduces two modes of operation:</word> <word top=603.4002304 left=447.449362752 bottom=614.2047424 right=558.204128832>(1) development, where users</word> <word top=614.3592304 left=317.955 bottom=625.1637424 right=559.686705139>iteratively improve the quality of labeling functions without execut-</word> <word top=625.3182304 left=317.955 bottom=636.1227424 right=558.20565312>ing the entire pipeline, and (2) production, where the full pipeline is</word> <word top=636.2772304 left=317.955 bottom=647.0817424 right=558.199555968>executed once to produce the knowledge base. We use different data</word> <word top=647.2362304 left=317.955 bottom=658.0407424 right=558.20622697>representations to implement the abstract data structures of Features</word> <word top=658.1952304 left=317.955 bottom=668.9997424 right=558.204128832>and Labels (a relation that stores the output of labeling functions</word> <word top=669.1542304 left=317.955 bottom=679.9587424 right=558.198569664>after applying them over the generated candidates). Implementing</word> <word top=680.1132304 left=317.955 bottom=690.9177424 right=558.203590848>Features as a list-of-lists structure minimizes runtime in both modes</word> <word top=691.0712304 left=317.955 bottom=701.8757424 right=558.200676768>of operation since it accounts for sparsity. We also find that Labels</word> <word top=702.0302304 left=317.955 bottom=712.8347424 right=558.197565427>implemented as a coordinate list during the development mode are</word> </paragraph></div><div id=9><paragraph><word top=119.4069216 left=53.798 bottom=134.9606368 right=59.7756>5</word> <word top=119.4069216 left=71.7308 bottom=134.9606368 right=159.4102368>EXPERIMENTS</word> <word top=139.4232304 left=53.377 bottom=150.2277424 right=295.524541613>We evaluate SystemX over four applications: ELECTRONICS, AD-</word> <word top=152.1558041 left=53.744 bottom=160.7993896 right=295.536785824>VERTISEMENTS, PALEONTOLOGY, and GENOMICS–each contain-</word> <word top=161.3412304 left=53.798 bottom=172.1457424 right=294.045335552>ing several relation extraction tasks. We seek to answer: (1) how does</word> <word top=171.1973632 left=53.798 bottom=183.1585408 right=294.042756787>SystemX compare against both state-of-the-art KBC techniques and</word> <word top=183.2592304 left=53.798 bottom=194.0637424 right=294.04685984>manually curated knowledge bases? (2) how does modeling context</word> <word top=194.2182304 left=53.798 bottom=205.0227424 right=294.04608873>scopes and multimodality impact quality? and (3) how does scaling</word> <word top=205.1772304 left=53.798 bottom=215.9817424 right=225.2074896>training set size affect SystemX’s performance?</word> </paragraph><section_header><word top=221.1002628 left=53.798 bottom=235.2930019 right=67.434375>5.1</word> </section_header><paragraph><word top=54.19144 left=53.798 bottom=65.2918432 right=214.7628128>SystemX: Knowledge Base Construction</word> <word top=65.15044 left=53.798 bottom=76.2508432 right=163.367408>from Richly Formatted Data</word> </paragraph><paragraph><word top=92.6712304 left=53.798 bottom=103.4757424 right=294.19346048>optimal for fast updates. A list-of-lists implementation is used for</word> <word top=103.6302304 left=53.798 bottom=114.4347424 right=152.4194336>Labels in production mode.</word> </paragraph><section_header><word top=221.1002628 left=78.343475 bottom=235.2930019 right=181.6744702>Experimental Settings</word> </section_header><paragraph><word top=238.9074374 left=63.761 bottom=250.8027818 right=295.529158042>Datasets. The datasets used for evaluation vary in size and for-</word> <word top=250.9712304 left=53.798 bottom=261.7757424 right=229.9249952>mat Table 1 shows the statistics of these datasets.</word> </paragraph><paragraph><word top=266.7666912 left=53.798 bottom=278.4319776 right=96.4690976>Electronics</word> <word top=267.6902304 left=102.439 bottom=278.4947424 right=203.420686656>The ELECTRONICS dataset</word> <word top=267.6902304 left=206.402193984 bottom=278.4947424 right=294.045705408>is a collection of single</word> <word top=278.6492304 left=53.798 bottom=289.4537424 right=295.537882496>bipolar transistor specification datasheets from over 20 manufac-</word> <word top=289.7722304 left=53.798 bottom=300.5767424 right=294.047625664>turers, downloaded from Digi-Key4, a prominent website in the</word> <word top=300.7312304 left=53.798 bottom=311.5357424 right=294.362853709>electronics distribution industry. These documents consist primarily</word> <word top=311.6902304 left=53.798 bottom=322.4947424 right=294.045335552>of tables and often express relations via domain-specific symbols. In</word> <word top=322.6492304 left=53.798 bottom=333.4537424 right=294.045335552>this application, we extract relations between transistor part numbers</word> <word top=333.6082304 left=53.798 bottom=344.4127424 right=294.047572864>and several of their electrical characteristics. We use this dataset</word> <word top=344.4596336 left=53.798 bottom=355.1655152 right=294.04733056>to evaluate the effectiveness of SystemX on datasets that consist</word> <word top=355.4186336 left=53.798 bottom=366.1245152 right=195.2698592>primarily of tables and numerical data.</word> </paragraph><paragraph><word top=371.3216912 left=53.475 bottom=382.9869776 right=113.146392>Advertisements</word> <word top=372.2452304 left=120.359 bottom=383.0497424 right=295.537687744>The ADVERTISEMENTS dataset contains web-</word> <word top=383.2042304 left=53.798 bottom=394.0087424 right=294.046016998>pages that may contain evidence of human trafficking activity. The</word> <word top=394.1622304 left=53.798 bottom=404.9667424 right=295.162907648>information in the corresponding data includes prices of services,</word> <word top=405.1212304 left=53.798 bottom=415.9257424 right=118.202216576>locations, contact</word> <word top=405.1212304 left=120.88191488 bottom=415.9257424 right=295.537882496>information, physical characteristics of the vic-</word> <word top=416.0802304 left=53.798 bottom=426.8847424 right=294.04865312>tims, etc. Here, we extract all attributes associated with a trafficking</word> <word top=427.0392304 left=53.798 bottom=437.8437424 right=150.825028352>advertisement. The output</word> <word top=427.0392304 left=153.80653568 bottom=437.8437424 right=294.047128832>is deployed in production and is used</word> <word top=437.9982304 left=53.798 bottom=448.8027424 right=294.043676768>by law enforcement agencies. This is a very heterogeneous dataset</word> <word top=448.9572304 left=53.475 bottom=459.7617424 right=294.048479386>with millions of webpages over 100s of web domains and 1000s of</word> <word top=459.9162304 left=53.798 bottom=470.7207424 right=294.28052>unique layouts. We use this dataset to examine how robust SystemX</word> <word top=470.7676336 left=53.798 bottom=481.4735152 right=202.371248>is in the presence of extreme data variety.</word> </paragraph><paragraph><word top=486.6706912 left=53.798 bottom=498.3359776 right=295.530090528>Paleontology The PALEONTOLOGY dataset is a collection of well-</word> <word top=498.5532304 left=53.798 bottom=509.3577424 right=295.537882496>curated paleontology journal articles on fossils and ancient organ-</word> <word top=509.5122304 left=53.798 bottom=520.3167424 right=294.043452608>isms. Here, we extract relations between paleontological formations</word> <word top=520.4712304 left=53.798 bottom=531.2757424 right=294.048742784>and their corresponding physical measurements. These papers often</word> <word top=531.4292304 left=53.798 bottom=542.2337424 right=294.358083584>have tables that span multiple pages. Thus, achieving high quality</word> <word top=542.3882304 left=53.798 bottom=553.1927424 right=294.041139277>in this application requires linking content in tables to the text that</word> <word top=553.3472304 left=53.798 bottom=564.1517424 right=294.047128832>references it, which can be separated by 20 pages or more in the</word> <word top=564.3062304 left=53.798 bottom=575.1107424 right=167.187296448>document. We use this dataset</word> <word top=564.1986336 left=170.278552512 bottom=574.9045152 right=294.049442688>to test SystemX’s ability to draw</word> <word top=575.1576336 left=53.798 bottom=585.8635152 right=201.9946592>candidates from document-level contexts.</word> </paragraph><paragraph><word top=591.0606912 left=53.798 bottom=602.7259776 right=294.041794496>Genomics The GENOMICS dataset is a collection of open-access</word> <word top=602.9432304 left=53.798 bottom=613.7477424 right=294.046016998>biomedical papers on gene-wide association studies (GWAS) from</word> <word top=613.9022304 left=53.798 bottom=624.7067424 right=294.04859808>the manually curated GWAS Catalog [40]. Here, we extract relations</word> <word top=624.8612304 left=53.798 bottom=635.6657424 right=294.043452608>between single-nucleotide polymorphisms and human phenotypes</word> <word top=635.8202304 left=53.798 bottom=646.6247424 right=235.048037504>found to be statistically significant. This dataset</word> <word top=635.8202304 left=238.45024832 bottom=646.6247424 right=294.047128832>is published in</word> <word top=646.7792304 left=53.475 bottom=657.5837424 right=294.048164032>XML format, thus, we do not have visual representations. We use</word> <word top=657.6306336 left=53.798 bottom=668.3365152 right=294.04646585>this dataset to evaluate how well the SystemX framework extracts</word> <word top=668.5886336 left=53.798 bottom=679.2945152 right=295.610940723>relations from data that is published natively in a tree-based format.</word> </paragraph><section_header><word top=705.2747745 left=53.606 bottom=711.877572 right=114.3664448>4http://www.digikey.com</word> </section_header><section_header><word top=65.15044 left=375.8352224 bottom=76.2508432 right=558.202832>SIGMOD’18, June 2018, Houston, Texas USA</word> </section_header><table_caption><word top=91.3736912 left=327.515 bottom=103.0389776 right=548.3484656>Table 1: Statistics of the datasets used in our experiments.</word> </table_caption><table><tr><td top=117.85 left=369.53 bottom=122.389999962 right=392.861756592>Dataset</td><td top=117.85 left=408.35 bottom=122.389999962 right=421.231744385>Size</td><td top=117.85 left=430.33 bottom=122.389999962 right=449.011732178>#Docs</td><td top=117.85 left=458.1 bottom=122.389999962 right=475.241723633>#Rels</td><td top=117.85 left=484.34 bottom=122.389999962 right=506.61999115>Format</td></tr><tr><td top=126.55 left=371.8 bottom=130.749999809 right=390.504498291>ELEC.</td><td top=126.55 left=407.19 bottom=130.749999809 right=421.231717529>3GB</td><td top=126.55 left=439.62 bottom=130.749999809 right=449.021733398>7K</td><td top=126.55 left=470.89 bottom=130.749999809 right=475.251724854>4</td><td top=126.55 left=493.84 bottom=130.749999809 right=506.630003777>PDF</td></tr><tr><td top=134.52 left=373.29 bottom=138.719999809 right=389.014456787>ADS.</td><td top=134.52 left=403.7 bottom=138.719999809 right=421.231707764>52GB</td><td top=134.52 left=433.23 bottom=138.719999809 right=449.021748047>9.3M</td><td top=134.52 left=470.89 bottom=138.719999809 right=475.251724854>4</td><td top=134.52 left=486.87 bottom=138.719999809 right=506.630000229>HTML</td></tr><tr><td top=142.49 left=369.94 bottom=146.689999809 right=392.364468994>PALEO.</td><td top=142.49 left=403.7 bottom=146.689999809 right=421.231707764>95GB</td><td top=142.49 left=433.23 bottom=146.689999809 right=449.021748047>0.3M</td><td top=142.49 left=467.4 bottom=146.689999809 right=475.251745605>10</td><td top=142.49 left=493.84 bottom=146.689999809 right=506.630003777>PDF</td></tr><tr><td top=150.46 left=373.14 bottom=154.659999809 right=389.164475098>GEN.</td><td top=150.46 left=401.96 bottom=154.659999809 right=421.231728516>1.8GB</td><td top=150.46 left=437.68 bottom=154.659999809 right=449.02173584>589</td><td top=150.46 left=470.89 bottom=154.659999809 right=475.251724854>4</td><td top=150.46 left=491.13 bottom=154.659999809 right=506.629990463>XML</td></tr></table><paragraph><word top=168.9064374 left=327.918 bottom=180.8017818 right=559.691522029>Comparison Methods. We use two different methods to evalu-</word> <word top=180.9702304 left=317.955 bottom=191.7747424 right=559.687733568>ate the quality of SystemX’s output: the upper bound of state-of-the-</word> <word top=191.9292304 left=317.955 bottom=202.7337424 right=558.201170304>art KBC systems (Oracle) and manually curated knowledge bases</word> <word top=202.7806336 left=317.659 bottom=213.4865152 right=420.9244112>(Existing Knowledge Bases).</word> </paragraph><paragraph><word top=218.6836912 left=317.955 bottom=230.3489776 right=475.327492864>Oracle While SystemX uses a data model</word> <word top=219.6072304 left=477.934025344 bottom=230.4117424 right=512.404274176>to extract</word> <word top=219.6072304 left=515.019952384 bottom=230.4117424 right=558.20608>information</word> <word top=230.5662304 left=317.955 bottom=241.3707424 right=559.76804832>from all modalities together, other state-of-the-art systems do not.</word> <word top=241.5252304 left=317.955 bottom=252.3297424 right=558.204128832>For comparison, we approximate the upper bound of quality of</word> <word top=252.4842304 left=317.955 bottom=263.2887424 right=558.203590848>three state-of-the-art information extraction techniques by assuming</word> <word top=263.4432304 left=317.955 bottom=274.2477424 right=558.515083584>perfect precision in their extracted relations and experimentally</word> <word top=274.4022304 left=317.955 bottom=285.2067424 right=390.9056304>measure their recall.</word> </paragraph><section_header><word top=286.4296912 left=333.895 bottom=298.0949776 right=559.693012736>• Text: For extraction from text, we follow [22, 34]. Candi-</word> </section_header><section_header><word top=298.3122304 left=342.364 bottom=309.1167424 right=559.689254003>dates are extracted from individual sentences, which are pre-</word> </section_header><section_header><word top=309.2712304 left=342.364 bottom=320.0757424 right=559.325594752>processed with standard NLP tools to add part-of-speech tags,</word> </section_header><paragraph><word top=320.2302304 left=342.364 bottom=331.0347424 right=466.145152>linguistic parsing information, etc.</word> <word top=332.2576912 left=333.895 bottom=343.9229776 right=558.203850189>• Table: For tables, we follow [3]. Candidates are drawn from</word> </paragraph><paragraph><word top=344.1402304 left=342.364 bottom=354.9447424 right=529.9321216>individual tables, utilizing its contents and structure.</word> <word top=356.1686912 left=333.895 bottom=367.8339776 right=558.202766784>• Ensemble: We also implement an ensemble, as proposed</word> </paragraph><section_header><word top=368.0512304 left=342.364 bottom=378.8557424 right=558.2019392>in [9]. Candidates resulting form both the Text and Table</word> </section_header><section_header><word top=379.0092304 left=342.364 bottom=389.8137424 right=535.9216768>approaches are merged into a combined candidate set.</word> </section_header><paragraph><word top=396.7976912 left=317.955 bottom=408.4629776 right=558.198769344>Existing Knowledge Base We use existing knowledge bases as</word> <word top=408.6802304 left=317.955 bottom=419.4847424 right=559.692982835>another comparison method. The ELECTRONICS application is com-</word> <word top=419.6392304 left=317.955 bottom=430.4437424 right=366.93037344>pared against</word> <word top=419.6392304 left=369.728966208 bottom=430.4437424 right=559.319907648>the transistor specifications published by Digi-Key,</word> <word top=430.5982304 left=317.632 bottom=441.4027424 right=558.205993664>while GENOMICS is compared to the both GWAS Central [4] and</word> <word top=441.5572304 left=317.955 bottom=452.3617424 right=558.203843974>GWAS Catalog [40], which are the most comprehensive collection</word> <word top=452.5162304 left=317.955 bottom=463.3207424 right=558.206146272>of GWAS data and widely-used public datasets. Knowledge bases</word> <word top=463.4742304 left=317.955 bottom=474.2787424 right=559.326273792>such as these are constructed using a combination of manual entry,</word> <word top=474.4332304 left=317.632 bottom=485.2377424 right=550.5432064>web aggregation, paid third-party services, and automation tools.</word> </paragraph><paragraph><word top=487.709514 left=327.918 bottom=500.9796972 right=558.202614784>SystemX Details. SystemX is implemented in Python, with</word> <word top=500.8582304 left=317.955 bottom=511.6627424 right=558.20385984>database operations being handled by PostgreSQL. All experiments</word> <word top=511.8172304 left=317.955 bottom=522.6217424 right=558.204128832>are executed in Jupyter Notebooks on a machine with four CPUs</word> <word top=522.7762304 left=317.659 bottom=533.5807424 right=559.325716672>(each CPU is a 14-core 2.40 GHz Xeon E5–4657L), 1 TB RAM,</word> <word top=533.7352304 left=317.955 bottom=544.5397424 right=557.5010016>and 12×3TB hard drives, with the Ubuntu 14.04 operating system.</word> <word top=548.1492628 left=317.955 bottom=562.3420019 right=331.591375>5.2</word> <word top=548.1492628 left=342.500475 bottom=562.3420019 right=442.7987404>Experimental Results</word> </paragraph><paragraph><word top=567.5004064 left=327.918 bottom=578.5918432 right=348.07862761>5.2.1</word> <word top=567.5004064 left=357.043942675 bottom=578.5918432 right=559.689415962>Oracle Comparison We compare the end-to-end qual-</word> <word top=578.7732304 left=317.955 bottom=589.5777424 right=558.197655552>ity of SystemX to the upper bound of state-of-the-art systems. In</word> <word top=589.7322304 left=317.677 bottom=600.5367424 right=558.19782496>Table 2, we see that SystemX outperforms the these upper bounds</word> <word top=600.6902304 left=317.955 bottom=611.4947424 right=558.199092237>on each dataset. In ELECTRONICS, we see that SystemX improves</word> <word top=611.6492304 left=317.955 bottom=622.4537424 right=514.368654528>a significant 71 F1 points over a text-only approach.</word> <word top=611.6492304 left=517.42332768 bottom=622.4537424 right=559.319907648>In contrast,</word> <word top=622.6082304 left=317.83 bottom=633.4127424 right=559.319634048>ADVERTISEMENTS has a higher upper bound with text than tables,</word> <word top=633.5672304 left=317.632 bottom=644.3717424 right=558.519321318>which reflects how advertisements rely more on text than the largely</word> <word top=644.5262304 left=317.955 bottom=655.3307424 right=558.426396607>numerical tables found in ELECTRONICS. In the PALEONTOLOGY</word> <word top=655.4852304 left=317.955 bottom=666.2897424 right=522.663829824>dataset, which depends on linking references from text</word> <word top=655.4852304 left=525.627045696 bottom=666.2897424 right=559.319907648>to tables,</word> <word top=666.4442304 left=317.955 bottom=677.2487424 right=558.87542912>the unified approach of SystemX results in an increase of 43 F1</word> <word top=677.4032304 left=317.955 bottom=688.2077424 right=558.202377984>points over the ensemble baseline. In GENOMICS, all candidates</word> <word top=688.3622304 left=317.955 bottom=699.1667424 right=558.515083584>are cross-context, preventing both the text-only and the table-only</word> <word top=699.3212304 left=317.955 bottom=710.1257424 right=482.9457264>approaches from finding any valid candidates.</word> </paragraph></div><div id=10><header><word top=65.15044 left=53.798 bottom=76.2508432 right=236.1656096>SIGMOD’18, June 2018, Houston, Texas USA</word> </header><table_caption><word top=91.3736912 left=53.502 bottom=103.0389776 right=294.7160928>Table 2: End-to-end quality in terms of precision, recall, and F1</word> <word top=102.3326912 left=53.798 bottom=113.9979776 right=295.532144>score for each applications compared to the upper bound state-</word> <word top=113.2916912 left=53.798 bottom=124.9569776 right=124.5070304>of-the-art systems.</word> </table_caption><table><tr><td top=139.77 left=93.33 bottom=144.309999962 right=106.011724548>Sys.</td><td top=140.1 left=120.2 bottom=144.299999809 right=139.661730957>Metric</td><td top=140.1 left=148.77 bottom=144.299999809 right=161.831721802>Text</td><td top=140.1 left=170.92 bottom=144.299999809 right=187.121721191>Table</td><td top=140.1 left=196.2 bottom=144.299999809 right=224.634036255>Ensemble</td><td top=139.65 left=233.67 bottom=144.300000095 right=259.59999649>SystemX</td></tr><tr><td top=0.0 left=0.0 bottom=0.0 right=0.0></td><td top=148.47 left=122.43 bottom=152.669999809 right=137.431731873>Prec.</td><td top=148.47 left=148.75 bottom=152.669999809 right=161.831726074>1.00</td><td top=148.47 left=174.04 bottom=152.669999809 right=187.121726074>1.00</td><td top=148.47 left=211.5 bottom=152.669999809 right=224.581726074>1.00</td><td top=148.47 left=247.4 bottom=152.669999809 right=259.599991226>0.73</td></tr><tr><td top=156.44 left=90.28 bottom=160.639999809 right=109.071725159>ELEC.</td><td top=156.44 left=123.2 bottom=160.639999809 right=136.651721191>Rec.</td><td top=156.44 left=148.75 bottom=160.639999809 right=161.831726074>0.03</td><td top=156.44 left=174.04 bottom=160.639999809 right=187.121726074>0.20</td><td top=156.44 left=211.5 bottom=160.639999809 right=224.581726074>0.21</td><td top=156.44 left=247.4 bottom=160.639999809 right=259.599991226>0.81</td></tr><tr><td top=0.0 left=0.0 bottom=0.0 right=0.0></td><td top=164.41 left=125.81 bottom=168.609999809 right=134.051729736>F1</td><td top=164.41 left=148.75 bottom=168.609999809 right=161.831726074>0.06</td><td top=164.41 left=174.04 bottom=168.609999809 right=187.121726074>0.40</td><td top=164.41 left=211.5 bottom=168.609999809 right=224.581726074>0.42</td><td top=164.08 left=247.4 bottom=168.619999962 right=259.610000992>0.77</td></tr><tr><td top=0.0 left=0.0 bottom=0.0 right=0.0></td><td top=172.78 left=122.43 bottom=176.979999809 right=137.431731873>Prec.</td><td top=172.78 left=148.75 bottom=176.979999809 right=161.831726074>1.00</td><td top=172.78 left=174.04 bottom=176.979999809 right=187.121726074>1.00</td><td top=172.78 left=211.5 bottom=176.979999809 right=224.581726074>1.00</td><td top=172.78 left=247.4 bottom=176.979999809 right=259.599991226>0.87</td></tr><tr><td top=180.75 left=91.76 bottom=184.949999809 right=107.581723938>ADS.</td><td top=180.75 left=123.2 bottom=184.949999809 right=136.651721191>Rec.</td><td top=180.75 left=148.75 bottom=184.949999809 right=161.831726074>0.44</td><td top=180.75 left=174.04 bottom=184.949999809 right=187.121726074>0.37</td><td top=180.75 left=211.5 bottom=184.949999809 right=224.581726074>0.76</td><td top=180.75 left=247.4 bottom=184.949999809 right=259.599991226>0.89</td></tr><tr><td top=0.0 left=0.0 bottom=0.0 right=0.0></td><td top=188.72 left=125.81 bottom=192.919999809 right=134.051729736>F1</td><td top=188.72 left=148.75 bottom=192.919999809 right=161.831726074>0.61</td><td top=188.72 left=174.04 bottom=192.919999809 right=187.121726074>0.54</td><td top=188.72 left=211.5 bottom=192.919999809 right=224.581726074>0.86</td><td top=188.38 left=247.4 bottom=192.919999962 right=259.610000992>0.88</td></tr><tr><td top=0.0 left=0.0 bottom=0.0 right=0.0></td><td top=197.09 left=122.43 bottom=201.289999809 right=137.431731873>Prec.</td><td top=197.09 left=148.75 bottom=201.289999809 right=161.831726074>0.00</td><td top=197.09 left=174.04 bottom=201.289999809 right=187.121726074>1.00</td><td top=197.09 left=211.5 bottom=201.289999809 right=224.581726074>1.00</td><td top=197.09 left=247.4 bottom=201.289999809 right=259.599991226>0.72</td></tr><tr><td top=205.06 left=92.07 bottom=209.259999809 right=110.931724854>ALEO.</td><td top=205.06 left=123.2 bottom=209.259999809 right=136.651721191>Rec.</td><td top=205.06 left=148.75 bottom=209.259999809 right=161.831726074>0.00</td><td top=205.06 left=174.04 bottom=209.259999809 right=187.121726074>0.04</td><td top=205.06 left=211.5 bottom=209.259999809 right=224.581726074>0.04</td><td top=205.06 left=247.4 bottom=209.259999809 right=259.599991226>0.38</td></tr><tr><td top=0.0 left=0.0 bottom=0.0 right=0.0></td><td top=213.8 left=125.81 bottom=217.999999809 right=134.051729736>F1</td><td top=212.06 left=148.75 bottom=218.000005302 right=164.478347778>0.00*</td><td top=213.8 left=174.04 bottom=217.999999809 right=187.111731567>0.08</td><td top=213.8 left=211.51 bottom=217.999999809 right=224.581731567>0.08</td><td top=213.47 left=247.4 bottom=218.009999962 right=259.610000992>0.51</td></tr><tr><td top=0.0 left=0.0 bottom=0.0 right=0.0></td><td top=222.17 left=122.43 bottom=226.369999809 right=137.431731873>Prec.</td><td top=222.17 left=148.75 bottom=226.369999809 right=161.831726074>0.00</td><td top=222.17 left=174.04 bottom=226.369999809 right=187.121726074>0.00</td><td top=222.17 left=211.5 bottom=226.369999809 right=224.581726074>0.00</td><td top=222.17 left=247.4 bottom=226.369999809 right=259.599991226>0.89</td></tr><tr><td top=230.14 left=91.61 bottom=234.339999809 right=107.73172699>GEN.</td><td top=230.14 left=123.2 bottom=234.339999809 right=136.651721191>Rec.</td><td top=230.14 left=148.75 bottom=234.339999809 right=161.831726074>0.00</td><td top=230.14 left=174.04 bottom=234.339999809 right=187.121726074>0.00</td><td top=230.14 left=211.5 bottom=234.339999809 right=224.581726074>0.00</td><td top=230.14 left=247.4 bottom=234.339999809 right=259.599991226>0.81</td></tr><tr><td top=0.0 left=0.0 bottom=0.0 right=0.0></td><td top=0.0 left=0.0 bottom=0.0 right=0.0></td><td top=236.93 left=160.96 bottom=239.340000086 right=163.46>#</td><td top=236.93 left=186.24 bottom=239.340000086 right=188.74>#</td><td top=236.93 left=223.71 bottom=239.340000086 right=225.71>#</td><td top=0.0 left=0.0 bottom=0.0 right=0.0></td></tr><tr><td top=0.0 left=0.0 bottom=0.0 right=0.0></td><td top=239.77 left=125.81 bottom=243.969999809 right=134.051729736>F1</td><td top=239.77 left=148.75 bottom=243.969999809 right=161.831726074>0.00</td><td top=239.77 left=174.04 bottom=243.969999809 right=187.111731567>0.00</td><td top=239.77 left=211.5 bottom=243.969999809 right=224.581726074>0.00</td><td top=239.43 left=247.4 bottom=243.969999962 right=259.610000992>0.85</td></tr></table><table_caption><word top=269.8066912 left=65.004 bottom=281.4719776 right=282.5467968>Table 3: End-to-end quality vs. existing knowledge bases.</word> </table_caption><section_header><word top=562.9152628 left=53.798 bottom=577.1080019 right=67.434375>5.3</word> </section_header><section_header><word top=247.8364224 left=88.837 bottom=254.9736768 right=170.7693312>* Text did not find any candidates.</word> <word top=256.1161536 left=88.837 bottom=263.3191616 right=228.4104816># No full tuples could be created using Text or Table alone</word> </section_header><table><tr><td top=296.28 left=114.18 bottom=300.819999962 right=136.36170929>System</td><td top=296.62 left=176.14 bottom=300.819999809 right=194.931732788>ELEC.</td><td top=296.62 left=226.57 bottom=300.819999809 right=241.809999771>GEN.</td></tr><tr><td top=0.0 left=0.0 bottom=0.0 right=0.0></td><td top=0.0 left=0.0 bottom=0.0 right=0.0></td><td top=304.99 left=208.52 bottom=309.189999809 right=259.290000458>GWAS GWAS</td></tr><tr><td top=309.25 left=101.2 bottom=313.449999809 right=149.351733398>Knowledge Base</td><td top=309.25 left=172.07 bottom=313.449999809 right=199.011711426>Digi-Key</td><td top=312.96 left=208.1 bottom=317.159999809 right=260.289979553>Central Catalog</td></tr><tr><td top=321.32 left=103.04 bottom=325.519999809 right=147.501708069># Entries in KB</td><td top=321.32 left=187.68 bottom=325.519999809 right=199.011726074>376</td><td top=321.32 left=212.94 bottom=325.519999809 right=260.289983215>3,008 4,023</td></tr><tr><td top=328.84 left=94.92 bottom=333.490012016 right=155.664018555># Entries in SystemX</td><td top=329.29 left=187.68 bottom=333.489999809 right=199.011726074>447</td><td top=329.29 left=212.94 bottom=333.489999809 right=260.289983215>6,420 6,420</td></tr><tr><td top=337.26 left=111.58 bottom=341.459999809 right=138.971723633>Coverage</td><td top=337.26 left=185.93 bottom=341.459999809 right=199.011726074>0.99</td><td top=337.26 left=216.43 bottom=341.459999809 right=260.289992981>0.82 0.80</td></tr><tr><td top=345.23 left=111.53 bottom=349.429999809 right=139.011719971>Accuracy</td><td top=345.23 left=185.93 bottom=349.429999809 right=199.011726074>0.87</td><td top=345.23 left=216.43 bottom=349.429999809 right=260.289992981>0.87 0.89</td></tr><tr><td top=353.2 left=93.64 bottom=357.399999809 right=156.891724243># New Correct Entries</td><td top=353.2 left=191.16 bottom=357.399999809 right=199.011715088>17</td><td top=353.2 left=212.94 bottom=357.399999809 right=260.289983215>3,154 2,486</td></tr><tr><td top=361.17 left=87.56 bottom=365.369999809 right=162.981722412>Increase in Correct Entries</td><td top=360.41 left=179.44 bottom=365.37999979 right=199.176999512>1.05×</td><td top=360.41 left=209.93 bottom=365.37999979 right=260.279998474>1.87× 1.42×</td></tr></table><paragraph><word top=379.5464064 left=63.761 bottom=390.6378432 right=83.4629783168>5.2.2</word> <word top=379.5464064 left=92.4280871552 bottom=390.6378432 right=295.536816218>Existing Knowledge Base Comparison We now com-</word> <word top=390.8192304 left=53.798 bottom=401.6237424 right=293.829931583>pare SystemX against existing knowledge bases for ELECTRONICS</word> <word top=401.7782304 left=53.798 bottom=412.5827424 right=294.197428352>and GENOMICS. No manually curated KBs are available for the other</word> <word top=412.7372304 left=53.798 bottom=423.5417424 right=295.53269305>two datasets. In Table 3, we find that SystemX achieves high cover-</word> <word top=423.6962304 left=53.798 bottom=434.5007424 right=294.0437216>age of the manual knowledge bases, while also correctly extracting</word> <word top=434.6552304 left=53.798 bottom=445.4597424 right=295.61675095>novel relation entries with over 85% accuracy in both applications.</word> <word top=445.6142304 left=53.798 bottom=456.4187424 right=294.040464576>In ELECTRONICS, SystemX achieved 99% coverage and extracted</word> <word top=456.5732304 left=53.798 bottom=467.3777424 right=294.041964186>an additional 17 correct entries not found in Digi-Key’s catalog. In</word> <word top=467.5322304 left=53.798 bottom=478.3367424 right=294.791789056>the GENOMICS application, we see that SystemX provides over 80%</word> <word top=478.4912304 left=53.798 bottom=489.2957424 right=294.04547104>coverage of both existing KBs and finds 1.87× and 1.42× more</word> <word top=489.4492304 left=53.798 bottom=500.2537424 right=295.61822144>correct entries than GWAS Central and GWAS Catalog, respectively.</word> </paragraph><paragraph><word top=503.6864374 left=63.761 bottom=515.5817818 right=294.356432493>Takeaways. SystemX achieves over 41 F1 points higher quality</word> <word top=515.7502304 left=53.798 bottom=526.5547424 right=295.532260563>on average when compared against the upper bound of state-of-the-</word> <word top=526.7092304 left=53.798 bottom=537.5137424 right=294.04913024>art approaches. Furthermore, SystemX attains over 80% of existing</word> <word top=537.6682304 left=53.798 bottom=548.4727424 right=294.042119168>public knowledge bases while providing up to 1.87× the number of</word> <word top=548.6262304 left=53.798 bottom=559.4307424 right=176.8259744>correct entries with high accuracy.</word> </paragraph><section_header><word top=562.9152628 left=78.343475 bottom=577.1080019 right=158.0671778>Inclusion Studies</word> </section_header><paragraph><word top=580.9462304 left=53.377 bottom=591.7507424 right=295.168035808>We conducted inclusion studies to assess the effect of context scope,</word> <word top=591.9052304 left=53.798 bottom=602.7097424 right=294.046904672>multimodal features, and multimodal supervisions on the quality of</word> <word top=601.7613632 left=53.798 bottom=613.7225408 right=294.044531968>SystemX. In each study, we change one component of SystemX and</word> <word top=613.8232304 left=53.798 bottom=624.6277424 right=226.3743008>hold the others constant and report the F1 score.</word> </paragraph><paragraph><word top=628.8504064 left=63.761 bottom=639.9418432 right=84.101099072>5.3.1</word> <word top=628.8504064 left=93.063912512 bottom=639.9418432 right=294.043454528>Context Scope Study To evaluate the importance of</word> <word top=640.1232304 left=53.798 bottom=650.9277424 right=294.047128832>addressing the non-local nature of candidates in richly formatted</word> <word top=651.0822304 left=53.798 bottom=661.8867424 right=295.537308646>data, we analyze how the different context scopes contribute to end-</word> <word top=662.0412304 left=53.798 bottom=672.8457424 right=141.45980288>to-end quality. We limit</word> <word top=662.0412304 left=144.093772544 bottom=672.8457424 right=294.047128832>the extracted candidates to four levels of</word> <word top=673.0002304 left=53.798 bottom=683.8047424 right=294.200785818>context scope in ELECTRONICS and report the average F1 score for</word> <word top=683.9592304 left=53.798 bottom=694.7637424 right=294.3575456>each. Figure 6 shows that increasing context scope can significantly</word> <word top=694.9182304 left=53.798 bottom=705.7227424 right=295.531041133>improve the F1 score. Considering document context gives an addi-</word> <word top=705.8772304 left=53.798 bottom=716.6817424 right=294.045492531>tional 71 F1 points (12.8×) over sentence contexts and 47 F1 points</word> </paragraph><figure bbox=89.757551366,345.916596664,180.574143432,517.391236></figure><figure_caption><word top=191.4986912 left=317.955 bottom=203.1639776 right=558.2007216>Figure 6: Average F1 score over four relations when broadening</word> <word top=202.4576912 left=317.955 bottom=214.1229776 right=496.5546>the extraction context scope in ELECTRONICS.</word> </figure_caption><figure bbox=228.433936,316.014677616,392.59458265,548.22504></figure><figure_caption><word top=405.5006912 left=322.875 bottom=417.1659776 right=553.2845808>Figure 7: The impact of each modality in the feature library.</word> </figure_caption><paragraph><word top=424.6682304 left=317.659 bottom=435.4727424 right=558.515505971>(2.6×) over table contexts. The positive correlation between quality</word> <word top=435.6272304 left=317.955 bottom=446.4317424 right=558.204128832>and context scope matches our expectations since larger context</word> <word top=446.5862304 left=317.955 bottom=457.3907424 right=558.20308873>scope is required to form candidates jointly from both table content</word> <word top=457.5452304 left=317.955 bottom=468.3497424 right=464.19519072>and surrounding text. We see a smaller</word> <word top=457.5452304 left=467.249863872 bottom=468.3497424 right=558.204128832>increase of 11 F1 points</word> <word top=468.5042304 left=317.659 bottom=479.3087424 right=558.517287936>(1.2×) in quality between page and document contexts since many</word> <word top=479.4632304 left=317.955 bottom=490.2677424 right=558.201491392>of the ELECTRONICS relation mentions are presented on the first</word> <word top=490.4222304 left=317.955 bottom=501.2267424 right=397.6393968>page of the document.</word> </paragraph><paragraph><word top=504.1244374 left=327.918 bottom=516.0197818 right=559.693005824>Takeaways. Semantics can be distributed in a document or im-</word> <word top=516.1882304 left=317.955 bottom=526.9927424 right=558.204128832>plied in its structure which requires larger context scope than the</word> <word top=527.1472304 left=317.955 bottom=537.9517424 right=554.3272368>traditional sentence-level contexts used in previous KBC systems.</word> </paragraph><paragraph><word top=541.6404064 left=327.918 bottom=552.7318432 right=348.258099072>5.3.2</word> <word top=541.6404064 left=357.220912512 bottom=552.7318432 right=558.201279104>Feature Ablation Study We evaluate our multimodal</word> <word top=552.9132304 left=317.955 bottom=563.7177424 right=500.540313792>feature library, which contains textual, structural,</word> <word top=552.9132304 left=503.421218112 bottom=563.7177424 right=559.694882496>tabular, and vi-</word> <word top=563.8722304 left=317.955 bottom=574.6767424 right=558.20151961>sual features. We analyze how different features benefit information</word> <word top=574.8312304 left=317.955 bottom=585.6357424 right=558.204128832>extraction from richly formatted data by comparing the effects of</word> <word top=585.7902304 left=317.955 bottom=596.5947424 right=558.20622697>disabling one feature type, while leaving all other types enabled and</word> <word top=596.7492304 left=317.955 bottom=607.5537424 right=541.0031664>report the average F1 scores of each configuration in Figure 7.</word> </paragraph><paragraph><word top=607.7082304 left=327.918 bottom=618.5127424 right=558.199540608>We find that removing a single feature set resulted in drops from</word> <word top=618.6672304 left=317.955 bottom=629.4717424 right=558.206165696>2 F1 points (no textual features in PALEONTOLOGY) to 33 F1 points</word> <word top=629.6262304 left=317.659 bottom=640.4307424 right=358.101409216>(no textual</word> <word top=629.6262304 left=361.585931584 bottom=640.4307424 right=516.750479616>features in ADVERTISEMENTS). While it</word> <word top=629.6262304 left=520.235001984 bottom=640.4307424 right=547.59902016>is clear</word> <word top=629.6262304 left=551.083542528 bottom=640.4307424 right=558.198918912>in</word> <word top=640.5842304 left=317.955 bottom=651.3887424 right=558.202335552>Figure 7 that each application depends on different feature types, we</word> <word top=651.5432304 left=317.955 bottom=662.3477424 right=558.202335552>find that it is necessary to incorporate all feature types to achieve the</word> <word top=662.5022304 left=317.955 bottom=673.3067424 right=410.6317104>highest extraction quality.</word> </paragraph><paragraph><word top=673.4612304 left=327.918 bottom=684.2657424 right=559.691836493>The characteristics of each dataset affect how valuable each fea-</word> <word top=684.4202304 left=317.955 bottom=695.2247424 right=558.202698944>ture type is to relation classification. The ADVERTISEMENTS dataset</word> <word top=695.3792304 left=317.955 bottom=706.1837424 right=558.203904672>consists of webpages which often use tables to format and organize</word> <word top=706.3382304 left=317.955 bottom=717.1427424 right=558.35046048>information–many relations can be found within the same cell or</word> </paragraph></div><div id=11><paragraph><word top=54.19144 left=53.798 bottom=65.2918432 right=214.7628128>SystemX: Knowledge Base Construction</word> <word top=65.15044 left=53.798 bottom=76.2508432 right=163.367408>from Richly Formatted Data</word> </paragraph><table_caption><word top=91.3736912 left=53.502 bottom=103.0389776 right=74.5999392>Table</word> <word top=91.3736912 left=79.7825184 bottom=103.0389776 right=136.2798048>4: Comparing</word> <word top=91.3736912 left=141.4713504 bottom=103.0389776 right=185.1466848>approaches</word> <word top=91.3736912 left=190.329264 bottom=103.0389776 right=197.7982752>to</word> <word top=91.3736912 left=202.9808544 bottom=103.0389776 right=252.287088>featurization</word> <word top=91.3736912 left=257.4696672 bottom=103.0389776 right=279.3925152>based</word> <word top=91.3736912 left=284.5750944 bottom=103.0389776 right=294.0436128>on</word> <word top=101.991968 left=53.798 bottom=114.0697088 right=139.965584>SystemX’s data model.</word> </table_caption><table><tr><td top=128.81 left=80.59 bottom=133.349999962 right=93.2817266846>Sys.</td><td top=129.14 left=107.46 bottom=133.339999809 right=126.931725464>Metric</td><td top=129.14 left=136.02 bottom=133.339999809 right=175.241725464>Human-tuned</td><td top=129.14 left=184.33 bottom=133.339999809 right=237.3640271>Bi-LSTM w/ Attn.</td><td top=128.69 left=246.4 bottom=133.340000095 right=272.330011749>SystemX</td></tr><tr><td top=0.0 left=0.0 bottom=0.0 right=0.0></td><td top=137.51 left=109.69 bottom=141.709999809 right=124.691724243>Prec.</td><td top=137.51 left=162.16 bottom=141.709999809 right=175.241726074>0.71</td><td top=137.51 left=224.24 bottom=141.709999809 right=237.321726074>0.42</td><td top=137.51 left=260.14 bottom=141.709999809 right=272.339991226>0.73</td></tr><tr><td top=145.48 left=77.54 bottom=149.680011253 right=96.3317251587>ELEC.</td><td top=145.48 left=110.46 bottom=149.679999809 right=123.921723328>Rec.</td><td top=145.48 left=162.16 bottom=149.679999809 right=175.241726074>0.82</td><td top=145.48 left=224.24 bottom=149.679999809 right=237.321726074>0.50</td><td top=145.48 left=260.14 bottom=149.679999809 right=272.339991226>0.81</td></tr><tr><td top=0.0 left=0.0 bottom=0.0 right=0.0></td><td top=153.45 left=113.07 bottom=157.649999809 right=121.311722107>F1</td><td top=153.45 left=162.16 bottom=157.649999809 right=175.241726074>0.76</td><td top=153.45 left=224.24 bottom=157.649999809 right=237.321726074>0.45</td><td top=153.12 left=260.14 bottom=157.659999962 right=272.339991226>0.77</td></tr><tr><td top=0.0 left=0.0 bottom=0.0 right=0.0></td><td top=161.82 left=109.69 bottom=166.019999809 right=124.691724243>Prec.</td><td top=161.82 left=162.16 bottom=166.019999809 right=175.241726074>0.88</td><td top=161.82 left=224.24 bottom=166.019999809 right=237.321726074>0.51</td><td top=161.82 left=260.14 bottom=166.019999809 right=272.339991226>0.87</td></tr><tr><td top=169.79 left=79.03 bottom=173.990011253 right=94.8417294312>ADS.</td><td top=169.79 left=110.46 bottom=173.989999809 right=123.921723328>Rec.</td><td top=169.79 left=162.16 bottom=173.989999809 right=175.241726074>0.88</td><td top=169.79 left=224.24 bottom=173.989999809 right=237.321726074>0.43</td><td top=169.79 left=260.14 bottom=173.989999809 right=272.339991226>0.89</td></tr><tr><td top=0.0 left=0.0 bottom=0.0 right=0.0></td><td top=177.76 left=113.07 bottom=181.959999809 right=121.311722107>F1</td><td top=177.43 left=162.16 bottom=181.969999962 right=175.241726074>0.88</td><td top=177.76 left=224.24 bottom=181.959999809 right=237.321726074>0.47</td><td top=177.43 left=260.13 bottom=181.969999962 right=272.340000992>0.88</td></tr><tr><td top=0.0 left=0.0 bottom=0.0 right=0.0></td><td top=186.13 left=109.69 bottom=190.329999809 right=124.691724243>Prec.</td><td top=186.13 left=162.16 bottom=190.329999809 right=175.241726074>0.92</td><td top=186.13 left=224.24 bottom=190.329999809 right=237.321726074>0.52</td><td top=186.13 left=260.14 bottom=190.329999809 right=272.339991226>0.76</td></tr><tr><td top=194.1 left=75.68 bottom=198.299999809 right=98.1917263794>PALEO.</td><td top=194.1 left=110.46 bottom=198.299999809 right=123.921723328>Rec.</td><td top=194.1 left=162.16 bottom=198.299999809 right=175.241726074>0.37</td><td top=194.1 left=224.24 bottom=198.299999809 right=237.321726074>0.15</td><td top=194.1 left=260.14 bottom=198.299999809 right=272.339991226>0.38</td></tr><tr><td top=0.0 left=0.0 bottom=0.0 right=0.0></td><td top=202.07 left=113.07 bottom=206.269999809 right=121.311722107>F1</td><td top=201.73 left=162.16 bottom=206.269999962 right=175.241726074>0.53</td><td top=202.07 left=224.24 bottom=206.269999809 right=237.321726074>0.23</td><td top=202.07 left=260.13 bottom=206.269999809 right=272.340000992>0.51</td></tr><tr><td top=0.0 left=0.0 bottom=0.0 right=0.0></td><td top=210.44 left=109.69 bottom=214.639999809 right=124.691724243>Prec.</td><td top=210.44 left=162.16 bottom=214.639999809 right=175.241726074>0.92</td><td top=210.44 left=224.24 bottom=214.639999809 right=237.321726074>0.66</td><td top=210.44 left=260.14 bottom=214.639999809 right=272.339991226>0.89</td></tr><tr><td top=218.41 left=78.88 bottom=222.609999809 right=94.9917248535>GEN.</td><td top=218.41 left=110.46 bottom=222.609999809 right=123.921723328>Rec.</td><td top=218.41 left=162.16 bottom=222.609999809 right=175.241726074>0.82</td><td top=218.41 left=224.24 bottom=222.609999809 right=237.321726074>0.41</td><td top=218.41 left=260.14 bottom=222.609999809 right=272.339991226>0.81</td></tr><tr><td top=0.0 left=0.0 bottom=0.0 right=0.0></td><td top=226.38 left=113.07 bottom=230.579999809 right=121.311722107>F1</td><td top=226.04 left=162.16 bottom=230.579999962 right=175.241726074>0.87</td><td top=226.38 left=224.24 bottom=230.579999809 right=237.321726074>0.47</td><td top=226.38 left=260.13 bottom=230.579999809 right=272.340000992>0.85</td></tr></table><paragraph><word top=241.5242304 left=53.798 bottom=252.3287424 right=294.043452608>phrase. This heavier reliance on textual features is reflected by the</word> <word top=252.4832304 left=53.798 bottom=263.2877424 right=295.524527424>drop of 33 F1 points when textual features are disabled. In ELEC-</word> <word top=265.2158041 left=53.78 bottom=273.8593896 right=294.04380896>TRONICS, both components of the (part, attribute) tuples we extract</word> <word top=274.4012304 left=53.798 bottom=285.2057424 right=295.532260563>are often isolated from other text (e.g. a lone number within an oth-</word> <word top=285.3602304 left=53.798 bottom=296.1647424 right=295.163831187>erwise empty cell). With little to no textual information to rely on,</word> <word top=296.3192304 left=53.475 bottom=307.1237424 right=294.044229312>we only see a small drop of 5 F1 points when textual features are</word> <word top=307.2782304 left=53.798 bottom=318.0827424 right=294.04451961>disabled. We see a drop of 21 F1 points when structural features are</word> <word top=318.2372304 left=53.798 bottom=329.0417424 right=294.04365065>disabled in the PALEONTOLOGY application due to its reliance on</word> <word top=329.1962304 left=53.798 bottom=340.0007424 right=294.047128832>structural features to link between formation names (found in text</word> <word top=340.1552304 left=53.798 bottom=350.9597424 right=294.198571328>sections or table captions) to the table itself. Finally, we see similar</word> <word top=351.1142304 left=53.798 bottom=361.9187424 right=295.528669312>decreases when disabling structural and tabular features in the GE-</word> <word top=363.8458041 left=54.022 bottom=372.4893896 right=294.043704064>NOMICS application (24 and 29 F1 points, respectively). Because</word> <word top=373.0312304 left=53.798 bottom=383.8357424 right=95.557394048>this dataset</word> <word top=373.0312304 left=98.70352448 bottom=383.8357424 right=294.19346048>is published natively in XML, structural and tabular</word> <word top=383.9902304 left=53.798 bottom=394.7947424 right=294.046590848>features are almost perfectly parsed which results in similar impacts</word> <word top=394.9492304 left=53.798 bottom=405.7537424 right=114.7964192>of these features.</word> </paragraph><paragraph><word top=408.6734374 left=63.761 bottom=420.5687818 right=110.444548088>Takeaways.</word> <word top=409.7782304 left=113.931 bottom=420.5827424 right=119.519039808>It</word> <word top=409.7782304 left=123.040145088 bottom=420.5827424 right=250.943151168>is necessary to utilize multimodal</word> <word top=409.7782304 left=254.464256448 bottom=420.5827424 right=294.046967232>features to</word> <word top=420.7372304 left=53.798 bottom=431.5417424 right=286.7271392>provide a robust, domain-agnostic description of real-world data.</word> </paragraph><paragraph><word top=435.2524064 left=63.761 bottom=446.3438432 right=84.101099072>5.3.3</word> <word top=435.2524064 left=93.063912512 bottom=446.3438432 right=294.04075488>Study of Featurization Approaches We study three</word> <word top=446.5252304 left=53.798 bottom=457.3297424 right=294.045335552>different approaches for multimodal featurization: (1) a human-tuned</word> <word top=457.4842304 left=53.798 bottom=468.2887424 right=295.83445824>multimodal feature library that leverages SystemX’s data model—</word> <word top=468.4422304 left=53.798 bottom=479.2467424 right=294.199647296>this requires feature engineering; (2) a Bi-LSTM with attention for</word> <word top=479.4012304 left=53.798 bottom=490.2057424 right=294.047236429>textual features alone—this corresponds to a state-of-the-art LSTM</word> <word top=490.3602304 left=53.798 bottom=501.1647424 right=295.531258829>for information extraction; and (3) SystemX’s approach using a Bi-</word> <word top=501.3192304 left=53.798 bottom=512.1237424 right=295.170080768>LSTM with attention which is extended with features from structural,</word> <word top=512.2782304 left=53.798 bottom=523.0827424 right=161.0989088>tabular, and visual modalities.</word> </paragraph><paragraph><word top=523.2372304 left=63.761 bottom=534.0417424 right=294.361197498>In Table 4, we find that the extended Bi-LSTM approach used by</word> <word top=533.0933632 left=53.798 bottom=545.0545408 right=294.047361773>SystemX is able to extract relations with quality comparable to the</word> <word top=545.1552304 left=53.798 bottom=555.9597424 right=295.61822144>human-tuned approach in all datasets differing by 2 F1 points at most.</word> <word top=556.1142304 left=53.798 bottom=566.9187424 right=294.042215104>Furthermore, SystemX’s approach outperforms a Bi-LSTM alone</word> <word top=567.0732304 left=53.798 bottom=577.8777424 right=295.166979776>by 1.7× to 2.2× by extending representation with structural, tabular,</word> <word top=578.0312304 left=53.798 bottom=588.8357424 right=294.045335552>and visual features. We see that the automated representation learned</word> <word top=588.9902304 left=53.798 bottom=599.7947424 right=170.086457344>by SystemX’s LSTM produces</word> <word top=588.9902304 left=173.644145536 bottom=599.7947424 right=197.523641344>results</word> <word top=588.9902304 left=201.081329536 bottom=599.7947424 right=294.047654656>that are comparable to a</word> <word top=599.9492304 left=53.798 bottom=610.7537424 right=295.555313178>manually-tuned feature representation requiring feature engineering.</word> </paragraph><paragraph><word top=613.6734374 left=63.761 bottom=625.5687818 right=295.53384617>Takeaways. Utilizing deep learning as a basis to obtain the fea-</word> <word top=625.7372304 left=53.798 bottom=636.5417424 right=294.04865312>ture representation needed to extract relations from richly formatted</word> <word top=636.6962304 left=53.798 bottom=647.5007424 right=242.3972576>data obviates the need for direct feature engineering.</word> </paragraph><paragraph><word top=651.2114064 left=63.761 bottom=662.3028432 right=83.7222148736>5.3.4</word> <word top=651.2114064 left=92.6886059072 bottom=662.3028432 right=294.361707968>Supervision Ablation Study We study how using only</word> <word top=662.4842304 left=53.798 bottom=673.2887424 right=263.116276736>textual LFs, only metadata LFs, and the combination of</word> <word top=662.4842304 left=266.180095616 bottom=673.2887424 right=294.047128832>the two</word> <word top=673.4432304 left=53.798 bottom=684.2477424 right=294.358083584>sets affects quality. Textual LFs only operate on textual modality</word> <word top=684.4022304 left=53.798 bottom=695.2067424 right=294.040565427>characteristics (such as traditional distant supervision rules), while</word> <word top=695.3602304 left=53.798 bottom=706.1647424 right=186.786030848>metadata LFs operate on structural,</word> <word top=695.3602304 left=189.996181376 bottom=706.1647424 right=294.358083584>tabular, and visual modality</word> <word top=706.3192304 left=53.798 bottom=717.1237424 right=294.047128832>characteristics. Figure 8 shows that applying metadata-based LFs</word> </paragraph><section_header><word top=65.15044 left=375.8352224 bottom=76.2508432 right=558.202832>SIGMOD’18, June 2018, Houston, Texas USA</word> </section_header><figure bbox=92.56001595,315.701527075,210.46969953,546.9613></figure><figure_caption><word top=221.5356912 left=317.955 bottom=233.2009776 right=427.883064>Figure 8: Study of different</word> <word top=221.5356912 left=431.4068592 bottom=233.2009776 right=559.7698416>supervision resources on quality.</word> <word top=232.4946912 left=317.955 bottom=244.1599776 right=554.5962288>Metadata includes structural, tabular, and visual information.</word> </figure_caption><figure bbox=250.903880384,358.048577216,363.14062601,512.442376384></figure><figure_caption><word top=375.6176912 left=317.955 bottom=387.2829776 right=558.2007216>Figure 9: Quality with increasing training set size. The x-axis</word> <word top=386.5766912 left=317.955 bottom=398.2419776 right=323.9355888>is</word> <word top=386.5766912 left=327.504216 bottom=398.2419776 right=372.4886448>the scale of</word> <word top=386.5766912 left=376.0483056 bottom=398.2419776 right=453.5000688>the base number of</word> <word top=386.5766912 left=457.068696 bottom=398.2419776 right=522.4158192>input documents</word> <word top=386.5766912 left=525.97548 bottom=398.2419776 right=558.2007216>for each</word> <word top=397.5356912 left=317.955 bottom=409.2009776 right=559.1008768>application: ELECTRONICS is 50; ADVERTISEMENTS is 25,000;</word> <word top=408.4946912 left=318.179 bottom=420.1599776 right=476.1751888>PALEONTOLOGY is 44; GENOMICS is 30.</word> </figure_caption><paragraph><word top=429.1142304 left=317.955 bottom=439.9187424 right=558.204128832>can achieve higher quality than traditional textual-level LFs alone</word> <word top=440.0732304 left=317.955 bottom=450.8777424 right=558.203456352>and that the highest quality is achieved when both types of LFs are</word> <word top=451.0322304 left=317.955 bottom=461.8367424 right=558.19826176>used. In the ELECTRONICS application, we see an increase of 66</word> <word top=461.9912304 left=317.955 bottom=472.7957424 right=558.199780147>F1 points (9.2×) when using metadata LFs compared to textual LFs</word> <word top=472.9502304 left=317.955 bottom=483.7547424 right=558.20628096>and a 3 F1 point (1.04×) improvement over metadata LFs when</word> <word top=483.9082304 left=317.955 bottom=494.7127424 right=471.356295744>both types are used. Because this dataset</word> <word top=483.9082304 left=474.447551808 bottom=494.7127424 right=558.204128832>relies more heavily on</word> <word top=494.8672304 left=317.955 bottom=505.6717424 right=558.35046048>distant signals, LFs that can label correctness based on column or</word> <word top=505.8262304 left=317.955 bottom=516.6307424 right=558.202335552>row header content significantly improve extraction quality. In sharp</word> <word top=516.7852304 left=317.955 bottom=527.5897424 right=558.204647808>contrast, the ADVERTISEMENTS application benefits equally from</word> <word top=527.7442304 left=317.955 bottom=538.5487424 right=558.802612557>metadata and textual LFs. Yet, we increase by 20 F1 points (1.2×)</word> <word top=538.7032304 left=317.632 bottom=549.5077424 right=558.201431232>when both types of LFs are applied. The PALEONTOLOGY and</word> <word top=549.6622304 left=318.179 bottom=560.4667424 right=558.802094976>GENOMICS applications show more moderate increases of 40 (4.6×)</word> <word top=560.6212304 left=317.955 bottom=571.4257424 right=559.320388992>and 40 (1.8×) F1 points by using both types over only textual LFs,</word> <word top=571.5802304 left=317.955 bottom=582.3847424 right=558.356647296>respectively, which reflects how each dataset’s characteristics cater</word> <word top=582.5392304 left=317.955 bottom=593.3437424 right=558.201975821>to particular supervision sources. Our experience with SystemX has</word> <word top=593.4972304 left=317.955 bottom=604.3017424 right=558.202335552>shown that users are able to effectively express their intuitions using</word> <word top=603.3533632 left=317.955 bottom=615.3145408 right=459.9742896>SystemX’s weak supervision resources.</word> </paragraph><paragraph><word top=618.3694374 left=327.918 bottom=630.2647818 right=559.68860096>Takeaways. To accommodate effective labeling functions, an in-</word> <word top=630.4332304 left=317.955 bottom=641.2377424 right=559.68735072>formation extraction framework for richly formatted data should pre-</word> <word top=641.3922304 left=317.955 bottom=652.1967424 right=558.198964186>serve information from all data modalities and provide easy access</word> <word top=652.3512304 left=317.955 bottom=663.1557424 right=554.6858928>to this information in order to leverage higher quality supervision.</word> <word top=666.3162628 left=317.955 bottom=680.5090019 right=331.591375>5.4</word> <word top=666.3162628 left=342.500475 bottom=680.5090019 right=510.0969783>Scaling with Quantity of Input Data</word> <word top=684.2392304 left=317.534 bottom=695.0437424 right=558.202325965>We substantiate the effectiveness of user-defined labeling functions</word> <word top=695.1982304 left=317.955 bottom=706.0027424 right=558.197565427>in generating this training data by investigating how increasing the</word> <word top=706.1562304 left=317.955 bottom=716.9607424 right=558.204128832>number of training documents affects average F1 score. Figure 9</word> </paragraph></div><div id=12><header><word top=65.15044 left=53.798 bottom=76.2508432 right=236.1656096>SIGMOD’18, June 2018, Houston, Texas USA</word> </header><paragraph><word top=92.6712304 left=53.798 bottom=103.4757424 right=294.047128832>shows how quality changes with increasing training set size. We</word> <word top=103.6302304 left=53.798 bottom=114.4347424 right=295.1658752>scale a base number of dataset in increments of {1×, 2×, 4×, 10×,</word> <word top=114.5892304 left=53.798 bottom=125.3937424 right=295.617214272>20×} for all four applications while holding other variables constant.</word> </paragraph><paragraph><word top=125.5472304 left=63.761 bottom=136.3517424 right=294.040819059>Each of our four applications show quality improvements as the</word> <word top=136.5062304 left=53.798 bottom=147.3107424 right=294.044762637>number of input documents is increased. When scaling from 1× to</word> <word top=147.4652304 left=53.798 bottom=158.2697424 right=295.166093434>20×, ADVERTISEMENTS quality improves by 38 F1 points (1.7×),</word> <word top=158.4242304 left=53.475 bottom=169.2287424 right=294.201077632>while the other applications improve by 1.2× on average. This larger</word> <word top=169.3832304 left=53.798 bottom=180.1877424 right=261.918186368>relative improvement may be due to the data variety of</word> <word top=169.3832304 left=265.100899712 bottom=180.1877424 right=295.524527424>the AD-</word> <word top=182.1158041 left=53.744 bottom=190.7593896 right=295.161867008>VERTISEMENTS dataset which comes from 1000s of web domains,</word> <word top=191.3012304 left=53.475 bottom=202.1057424 right=178.423935936>whereas the other datasets reflect</word> <word top=191.3012304 left=181.725543744 bottom=202.1057424 right=216.808556352>the lower</word> <word top=191.3012304 left=220.11016416 bottom=202.1057424 right=294.044229312>format variety from</word> <word top=202.2602304 left=53.798 bottom=213.0647424 right=294.047128832>academic journals and a small number of manufacturers. Despite</word> <word top=213.2192304 left=53.798 bottom=224.0237424 right=69.30000896>this,</word> <word top=213.2192304 left=72.171767552 bottom=224.0237424 right=294.047128832>these applications also benefit from increased input data. In</word> <word top=224.1782304 left=54.022 bottom=234.9827424 right=294.04130624>ELECTRONICS, increased data provides SystemX a larger sample of</word> <word top=235.1372304 left=53.798 bottom=245.9417424 right=295.611102118>each manufacturer’s style, which improves learning and inference.</word> <word top=246.0952304 left=53.798 bottom=256.8997424 right=294.048595187>Labeling functions help SystemX avoid cascading inference errors</word> <word top=257.0542304 left=53.798 bottom=267.8587424 right=286.879568>due to lack of supervision that can plague traditional approaches.</word> </paragraph><paragraph><word top=272.9634374 left=63.761 bottom=284.8587818 right=294.04126112>Takeaways. By using user-defined labeling functions to generate</word> <word top=285.0272304 left=53.798 bottom=295.8317424 right=294.04314775>training data, users are able to increase KBC quality by increasing</word> <word top=295.9862304 left=53.798 bottom=306.7907424 right=125.5202336>the training set size.</word> <word top=311.8009216 left=53.798 bottom=327.3546368 right=59.7756>6</word> <word top=311.8009216 left=71.7308 bottom=327.3546368 right=146.6420832>USER STUDY</word> <word top=331.8302304 left=53.52 bottom=342.6347424 right=295.534254336>Traditionally, ground truth data is created through manual annota-</word> <word top=342.7892304 left=53.798 bottom=353.5937424 right=162.35779136>tion, crowdsourcing, or other</word> <word top=342.7892304 left=165.430755968 bottom=353.5937424 right=294.047128832>time-consuming methods and then</word> <word top=353.7482304 left=53.798 bottom=364.5527424 right=146.636284928>used as training data for</word> <word top=353.7482304 left=150.166535936 bottom=364.5527424 right=282.889340672>training a machine learning model.</word> <word top=353.7482304 left=286.428737408 bottom=364.5527424 right=294.047128832>In</word> <word top=363.6043632 left=53.798 bottom=375.5655408 right=230.61230144>SystemX, we use the data programming model</word> <word top=364.7072304 left=233.895617792 bottom=375.5117424 right=295.537824512>for users to pro-</word> <word top=375.6662304 left=53.798 bottom=386.4707424 right=294.045335552>grammatically generate training data, rather than needing to perform</word> <word top=386.6252304 left=53.798 bottom=397.4297424 right=294.047128832>manual annotation–a human-in-the-loop approach. In this section</word> <word top=397.5842304 left=53.475 bottom=408.3887424 right=294.047457216>we qualitatively evaluate the effectiveness of our approach compared</word> <word top=408.5432304 left=53.798 bottom=419.3477424 right=294.045335552>to traditional human labeling and discuss the cognitive load required</word> <word top=419.5022304 left=53.798 bottom=430.3067424 right=99.8673632>by our users.</word> </paragraph><paragraph><word top=430.4612304 left=63.761 bottom=441.2657424 right=294.041285312>We conducted a user study with 10 users, where each user was</word> <word top=441.4192304 left=53.798 bottom=452.2237424 right=294.045335552>asked to complete the relation extraction task of extracting maximum</word> <word top=452.3782304 left=53.798 bottom=463.1827424 right=294.047747328>collector-emitter voltages from the ELECTRONICS dataset. Using the</word> <word top=463.3372304 left=53.798 bottom=474.1417424 right=294.04314775>same experimental settings, we compare the effectiveness of these</word> <word top=474.2962304 left=53.798 bottom=485.1007424 right=294.041462067>two approaches for obtaining training data: (1) manual annotations</word> <word top=485.2552304 left=53.502 bottom=496.0597424 right=294.043792128>(Manual) and (2) using labeling functions (LF). To minimize the</word> <word top=496.2142304 left=53.798 bottom=507.0187424 right=294.043291213>effect of cognitive fatigue and familiarity with the task, half of the</word> <word top=507.1732304 left=53.798 bottom=517.9777424 right=295.16349943>users performed the task of manually annotating training data first,</word> <word top=518.1322304 left=53.798 bottom=528.9367424 right=294.045335552>then of writing labeling functions, while the other half performed the</word> <word top=529.0912304 left=53.798 bottom=539.8957424 right=294.046456352>tasks in the reverse order. We allotted 30 minutes for each task and</word> <word top=540.0502304 left=53.798 bottom=550.8547424 right=294.045335552>evaluate the quality that was achieved using each approach at several</word> <word top=551.0082304 left=53.798 bottom=561.8127424 right=295.53035072>checkpoints. For manual annotations, we set 5 minutes as one evalua-</word> <word top=561.9672304 left=53.798 bottom=572.7717424 right=294.045335552>tion epoch. Similarly, we plot the quality achieved by user’s labeling</word> <word top=572.9262304 left=53.798 bottom=583.7307424 right=294.041569664>functions each time the user performed an iteration of supervision</word> <word top=583.8852304 left=53.798 bottom=594.6897424 right=294.042398042>and classification as part of SystemX’s iterative approach. Note, we</word> <word top=594.8442304 left=53.798 bottom=605.6487424 right=241.4826848>filtered out two outliers and report results of 8 users.</word> </paragraph><paragraph><word top=605.8032304 left=63.761 bottom=616.6077424 right=294.357942694>In Figure 10 (left), we report the quality (F1 score) achieved by</word> <word top=616.7622304 left=53.798 bottom=627.5667424 right=294.045335552>the two different approaches. The average F1 achieved using manual</word> <word top=627.7212304 left=53.798 bottom=638.5257424 right=295.529274752>annotation was 0.26 while the average F1 score using labeling func-</word> <word top=638.6802304 left=53.798 bottom=649.4847424 right=294.047527194>tions was 0.49, an improvement of 1.9×. We found with statistical</word> <word top=649.6392304 left=53.798 bottom=660.4437424 right=294.045335552>significance that all users were able to achieve higher F1 scores using</word> <word top=660.5982304 left=53.798 bottom=671.4027424 right=295.529301651>labeling functions than manually annotating candidates in 30 min-</word> <word top=671.5562304 left=53.798 bottom=682.3607424 right=295.61822144>utes, regardless of the order in which they performed the approaches.</word> <word top=682.5152304 left=53.52 bottom=693.3197424 right=294.048521856>There are two primary reasons for this trend. First, labeling functions</word> <word top=693.4742304 left=53.798 bottom=704.2787424 right=295.530296922>provide a larger set of training data than manual annotations by en-</word> <word top=704.4332304 left=53.798 bottom=715.2377424 right=294.357715962>abling users to apply patterns they find in the data programmatically</word> </paragraph><figure bbox=90.108747314,328.293657186,177.743758738,537.883627649></figure><figure_caption><word top=187.8976912 left=317.955 bottom=199.5629776 right=558.2007216>Figure 10: F1 quality over time with 95% confidence intervals</word> <word top=198.8566912 left=317.659 bottom=210.5219776 right=552.3455536>(left). Modality Distribution of user labeling functions (right).</word> </figure_caption><paragraph><word top=229.0952304 left=317.955 bottom=239.8997424 right=559.694882496>to all candidates–a natural desire they often vocalized while per-</word> <word top=240.0542304 left=317.955 bottom=250.8587424 right=558.200452608>forming manual annotation. On average, our users manually labeled</word> <word top=251.0132304 left=317.955 bottom=261.8177424 right=558.518670144>285 candidates in the allotted time, while the labeling functions they</word> <word top=261.9722304 left=317.955 bottom=272.7767424 right=559.688292192>provided labeled 19,075 candidates. Users provided 7 labeling func-</word> <word top=272.9312304 left=317.955 bottom=283.7357424 right=558.43752>tions on average. Second, labeling functions tend to allow SystemX</word> <word top=283.8902304 left=317.955 bottom=294.6947424 right=558.200452608>to learn more general features, where manual annotations may not</word> <word top=294.8492304 left=317.955 bottom=305.6537424 right=558.35046048>adequately cover the characteristics of the dataset as a whole. For</word> <word top=305.8072304 left=317.955 bottom=316.6117424 right=527.76876>example, labeling functions are easily applied to new data.</word> </paragraph><paragraph><word top=316.7662304 left=327.918 bottom=327.5707424 right=558.200544845>In addition, we found that for richly formatted data, users relied</word> <word top=327.7252304 left=317.955 bottom=338.5297424 right=372.198312768>less on textual</word> <word top=327.7252304 left=375.509066304 bottom=338.5297424 right=484.754787264>information–a primary signal</word> <word top=327.7252304 left=488.0655408 bottom=338.5297424 right=558.204128832>in traditional KBC</word> <word top=338.6842304 left=317.955 bottom=349.4887424 right=558.204128832>tasks–and more on information from other modalities, as shown</word> <word top=349.6432304 left=317.955 bottom=360.4477424 right=558.204128832>in Figure 10 (right). Users utilized the semantics from multiple</word> <word top=360.6022304 left=317.955 bottom=371.4067424 right=558.20151961>modalities of the richly formatted data, with 58.5% of their labeling</word> <word top=371.5612304 left=317.955 bottom=382.3657424 right=558.201788602>functions using tabular information. This reflects the characteristics</word> <word top=382.5202304 left=317.955 bottom=393.3247424 right=549.284245824>of the ELECTRONICS dataset, which contains information that</word> <word top=382.5202304 left=552.101130048 bottom=393.3247424 right=558.201330624>is</word> <word top=393.4792304 left=317.955 bottom=404.2837424 right=558.204128832>primarily found in tables. In our study, the most common labeling</word> <word top=404.4382304 left=317.955 bottom=415.2427424 right=435.4955376>functions in each modality were:</word> </paragraph><section_header><word top=416.4656912 left=333.895 bottom=428.1309776 right=558.200364224>• Tabular: labeling a candidate based on the words found in</word> </section_header><paragraph><word top=428.3482304 left=342.364 bottom=439.1527424 right=431.041696>the same row or column.</word> <word top=440.3756912 left=333.895 bottom=452.0409776 right=559.688330752>• Visual: labeling a candidate based on its placement in a doc-</word> </paragraph><paragraph><word top=452.2582304 left=342.364 bottom=463.0627424 right=491.4393664>ument (e.g., which page it was found on).</word> <word top=464.2866912 left=333.895 bottom=475.9519776 right=544.3221008>• Structural: labeling a candidate based on its tag names.</word> <word top=477.2376912 left=333.895 bottom=488.9029776 right=559.689950438>• Textual: labeling a candidate based on the textual character-</word> </paragraph><section_header><word top=489.1202304 left=342.364 bottom=499.9247424 right=507.300928>istics of the voltage mention (e.g. magnitude).</word> </section_header><paragraph><word top=504.4254374 left=327.918 bottom=516.3207818 right=431.160659584>Takeaways. We found that</word> <word top=505.5302304 left=434.178749824 bottom=516.3347424 right=558.203967232>leveraging labeling functions and</word> <word top=516.4892304 left=317.955 bottom=527.2937424 right=558.199663565>data programming allowed users to create high quality knowledge</word> <word top=527.4482304 left=317.955 bottom=538.2527424 right=558.204128832>bases more effectively than a traditional manual annotation based</word> <word top=538.4072304 left=317.955 bottom=549.2117424 right=353.76052512>approach.</word> <word top=538.4072304 left=356.860926912 bottom=549.2117424 right=559.319907648>In addition, when working with richly formatted data,</word> <word top=549.3662304 left=317.955 bottom=560.1707424 right=559.766846822>users relied heavily on non-textual signals when labeling candidates.</word> <word top=564.5779216 left=317.955 bottom=580.1316368 right=323.9326>7</word> <word top=564.5779216 left=335.8878 bottom=580.1316368 right=416.9201456>CONCLUSION</word> <word top=584.4062304 left=317.955 bottom=595.2107424 right=559.685736768>In this paper, we study how to extract information from richly for-</word> <word top=595.3652304 left=317.955 bottom=606.1697424 right=558.798601152>matted data. We show that key challenges of this problem are (1)</word> <word top=606.3242304 left=317.955 bottom=617.1287424 right=558.206603558>prevalent document-level relations, (2) multimodality, and (3) data</word> <word top=617.2832304 left=317.731 bottom=628.0877424 right=558.197629056>variety. To address these, we propose SystemX, the first KBC system</word> <word top=628.2422304 left=317.955 bottom=639.0467424 right=558.198991309>for richly formatted information extraction. We describe SystemX’s</word> <word top=639.2012304 left=317.955 bottom=650.0057424 right=559.319907648>data model, which enables users to perform candidate extraction,</word> <word top=650.1602304 left=317.955 bottom=660.9647424 right=559.68735072>multimodal featurization, and multimodal supervision through a sim-</word> <word top=661.1182304 left=317.955 bottom=671.9227424 right=558.203373094>ple programming model. We evaluate SystemX on four real-world</word> <word top=672.0772304 left=317.955 bottom=682.8817424 right=558.354880915>domains and achieve an average improvement of 41 F1 points over</word> <word top=683.0362304 left=317.955 bottom=693.8407424 right=559.319907648>the upper bound of state-of-the-art approaches. In some domains,</word> <word top=692.8923632 left=317.955 bottom=704.8535408 right=559.694264>SystemX extracts up to 1.87× the number of correct relations com-</word> <word top=704.9542304 left=317.955 bottom=715.7587424 right=490.8989232>pared to expert-curated public knowledge bases.</word> </paragraph></div><div id=13><section_header><word top=88.4839216 left=53.798 bottom=104.0376368 right=109.3418592>References</word> </section_header><paragraph><word top=54.19144 left=53.798 bottom=65.2918432 right=214.7628128>SystemX: Knowledge Base Construction</word> <word top=65.15044 left=53.798 bottom=76.2508432 right=163.367408>from Richly Formatted Data</word> </paragraph><list><word top=108.9969118 left=57.285 bottom=117.4003408 right=294.914189028>[1] G. Angeli, S. Gupta, M. Jose, C. D. Manning, C. Ré, J. Tibshirani, J. Y. Wu, S. Wu,</word> <word top=116.9679118 left=69.402 bottom=125.3713408 right=274.37555>and C. Zhang. Stanford’s 2014 slot filling systems. TAC KBP, 695, 2014.</word> <word top=124.9379118 left=57.285 bottom=133.3413408 right=294.28822074>[2] D. Bahdanau, K. Cho, and Y. Bengio. Neural machine translation by jointly</word> <word top=132.9079118 left=69.402 bottom=141.3113408 right=264.77195>learning to align and translate. arXiv preprint arXiv:1409.0473, 2014.</word> <word top=140.8779118 left=57.285 bottom=149.2813408 right=212.144109492>[3] D. W. Barowy, S. Gulwani, T. Hart, and B. Zorn.</word> <word top=140.8779118 left=216.191563536 bottom=149.2813408 right=295.198720068>Flashrelate: extracting rela-</word> <word top=148.8479118 left=69.402 bottom=157.2513408 right=240.489342754>tional data from semi-structured spreadsheets using examples.</word> <word top=148.8479118 left=242.981039651 bottom=157.2513408 right=294.04492942>In ACM SIGPLAN</word> <word top=156.7342262 left=69.172 bottom=165.0609434 right=206.869281>Notices, volume 50, pages 218–228. ACM, 2015.</word> <word top=164.7879118 left=57.285 bottom=173.1913408 right=295.016703888>[4] T. Beck, R. K. Hastings, S. Gollapudi, R. C. Free, and A. J. Brookes. Gwas central:</word> <word top=172.7579118 left=69.402 bottom=181.1613408 right=294.042882456>a comprehensive resource for the comparison and interrogation of genome-wide</word> <word top=180.7279118 left=69.402 bottom=189.1313408 right=207.3081172>association studies. EJHG, 22(7):949–952, 2014.</word> <word top=188.6989118 left=57.285 bottom=197.1023408 right=295.199668505>[5] K. Bollacker, C. Evans, P. Paritosh, T. Sturge, and J. Taylor. Freebase: a collabo-</word> <word top=196.6689118 left=69.402 bottom=205.0723408 right=255.656018784>ratively created graph database for structuring human knowledge.</word> <word top=196.6689118 left=258.202571592 bottom=205.0723408 right=294.916319>In SIGMOD,</word> <word top=204.6389118 left=69.402 bottom=213.0423408 right=156.679107>pages 1247–1250. ACM, 2008.</word> <word top=212.6089118 left=57.285 bottom=221.0123408 right=224.030393688>[6] E. Brown, E. Epstein, J. W. Murdock, and T.-H. Fin.</word> <word top=212.6089118 left=228.639796536 bottom=221.0123408 right=294.160181772>Tools and methods for</word> <word top=220.5789118 left=69.402 bottom=228.9823408 right=115.6871106>building watson.</word> <word top=220.4952262 left=118.197 bottom=228.8219434 right=246.1809664>IBM Research. Abgerufen am, 14:2013, 2013.</word> <word top=228.5489118 left=57.285 bottom=236.9523408 right=295.262739552>[7] A. Carlson, J. Betteridge, B. Kisiel, B. Settles, E. R. Hruschka Jr, and T. M.</word> <word top=236.5189118 left=69.402 bottom=244.9223408 right=267.07994004>Mitchell. Toward an architecture for never-ending language learning.</word> <word top=236.5189118 left=269.946590268 bottom=244.9223408 right=294.916319>In AAAI,</word> <word top=244.4889118 left=69.227 bottom=252.8923408 right=136.3010084>volume 5, page 3, 2010.</word> <word top=252.4589118 left=57.285 bottom=260.8623408 right=295.198720068>[8] M. Cosulschi, N. Constantinescu, and M. Gabroveanu. Classifcation and com-</word> <word top=260.4299118 left=69.402 bottom=268.8333408 right=294.04184976>parison of information structures from a web page. Annals of the University of</word> <word top=268.3162262 left=69.172 bottom=276.6429434 right=243.51565>Craiova-Mathematics and Computer Science Series, 31, 2004.</word> <word top=276.3699118 left=57.285 bottom=284.7733408 right=294.917076181>[9] X. Dong, E. Gabrilovich, G. Heitz, W. Horn, N. Lao, K. Murphy, T. Strohmann,</word> <word top=284.3399118 left=69.402 bottom=292.7433408 right=294.044744461>S. Sun, and W. Zhang. Knowledge vault: A web-scale approach to probabilistic</word> <word top=292.3099118 left=69.402 bottom=300.7133408 right=120.7500894>knowledge fusion.</word> <word top=292.3099118 left=123.2606574 bottom=300.7133408 right=240.169207>In SIGKDD, pages 601–610. ACM, 2014.</word> <word top=300.2799118 left=53.798 bottom=308.6833408 right=294.914189028>[10] D. Ferrucci, E. Brown, J. Chu-Carroll, J. Fan, D. Gondek, A. A. Kalyanpur,</word> <word top=308.2499118 left=69.151 bottom=316.6533408 right=294.294135532>A. Lally, J. W. Murdock, E. Nyberg, J. Prager, et al. Building watson: An overview</word> <word top=316.2199118 left=69.402 bottom=324.6233408 right=224.4643172>of the deepqa project. AI magazine, 31(3):59–79, 2010.</word> <word top=324.1899118 left=53.798 bottom=332.5933408 right=294.046369356>[11] H. Gao, G. Barbier, and R. Goolsby. Harnessing the crowdsourcing power of</word> <word top=332.1609118 left=69.402 bottom=340.5643408 right=155.5842204>social media for disaster relief.</word> <word top=332.0772262 left=158.094 bottom=340.4039434 right=284.5073172>IEEE Intelligent Systems, 26(3):10–14, 2011.</word> <word top=340.1309118 left=53.798 bottom=348.5343408 right=264.974410344>[12] W. Gatterbauer, P. Bohunsky, M. Herzog, B. Krüpl, and B. Pollak.</word> <word top=340.1309118 left=269.953703544 bottom=348.5343408 right=294.046369356>Towards</word> <word top=348.1009118 left=69.402 bottom=356.5043408 right=127.880241996>domain-independent</word> <word top=348.1009118 left=129.992884968 bottom=356.5043408 right=244.438382532>information extraction from web tables.</word> <word top=348.1009118 left=248.137286052 bottom=356.5043408 right=294.041891424>In WWW, pages</word> <word top=356.0709118 left=69.227 bottom=364.4743408 right=124.9337144>71–80. ACM, 2007.</word> <word top=364.0409118 left=53.798 bottom=372.4443408 right=294.046369356>[13] V. Govindaraju, C. Zhang, and C. Ré. Understanding tables in context using</word> <word top=372.0109118 left=69.402 bottom=380.4143408 right=128.1004746>standard nlp toolkits.</word> <word top=372.0109118 left=130.6110426 bottom=380.4143408 right=215.4399426>In ACL, pages 658–664, 2013.</word> <word top=379.9809118 left=53.798 bottom=388.3843408 right=295.262739552>[14] A. Graves, M. Liwicki, S. Fernández, R. Bertolami, H. Bunke, and J. Schmidhuber.</word> <word top=387.9509118 left=69.151 bottom=396.3543408 right=276.153217487>A novel connectionist system for unconstrained handwriting recognition.</word> <word top=387.8672262 left=278.668 bottom=396.1939434 right=294.045145314>IEEE</word> <word top=395.8372262 left=69.402 bottom=404.1639434 right=294.7941172>transactions on pattern analysis and machine intelligence, 31(5):855–868, 2009.</word> <word top=403.8919118 left=53.798 bottom=412.2953408 right=294.04622988>[15] A. Graves, A.-r. Mohamed, and G. Hinton. Speech recognition with deep recurrent</word> <word top=411.8619118 left=69.402 bottom=420.2653408 right=116.302004494>neural networks.</word> <word top=411.8619118 left=118.804936183 bottom=420.2653408 right=294.047968557>In Acoustics, speech and signal processing (icassp), 2013 ieee</word> <word top=419.7482262 left=69.402 bottom=428.0749434 right=236.8838934>international conference on, pages 6645–6649. IEEE, 2013.</word> <word top=427.8019118 left=53.798 bottom=436.2053408 right=294.914189028>[16] M. Hewett, D. E. Oliver, D. L. Rubin, K. L. Easton, J. M. Stuart, R. B. Altman,</word> <word top=435.7719118 left=69.402 bottom=444.1753408 right=294.044156867>and T. E. Klein. Pharmgkb: the pharmacogenetics knowledge base. Nucleic acids</word> <word top=443.6582262 left=69.402 bottom=451.9849434 right=157.2991172>research, 30(1):163–165, 2002.</word> <word top=451.7119118 left=53.798 bottom=460.1153408 right=65.4163508>[17]</word> <word top=451.7119118 left=69.402 bottom=460.1153408 right=294.916581>S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural computation,</word> <word top=459.6819118 left=69.402 bottom=468.0853408 right=133.5191172>9(8):1735–1780, 1997.</word> <word top=467.6519118 left=53.798 bottom=476.0553408 right=295.262683762>[18] N. Japkowicz and S. Stephen. The class imbalance problem: A systematic study.</word> <word top=475.5392262 left=69.283 bottom=483.8659434 right=197.2592172>Intelligent data analysis, 6(5):429–449, 2002.</word> <word top=483.5929118 left=53.798 bottom=491.9963408 right=294.04622988>[19] M. Kovacevic, M. Diligenti, M. Gori, and V. Milutinovic. Recognition of common</word> <word top=491.5629118 left=69.402 bottom=499.9663408 right=163.26167682>areas in a web page using visual</word> <word top=491.5629118 left=165.402772896 bottom=499.9663408 right=294.046369356>information: a possible application in a page</word> <word top=499.5329118 left=69.402 bottom=507.9363408 right=107.5626336>classification.</word> <word top=499.5329118 left=110.0732016 bottom=507.9363408 right=218.7310934>In ICDM, pages 250–257. IEEE, 2002.</word> <word top=507.5029118 left=53.798 bottom=515.9063408 right=294.913240714>[20] Y. LeCun, Y. Bengio, and G. Hinton. Deep learning. Nature, 521(7553):436–444,</word> <word top=515.4729118 left=69.402 bottom=523.8763408 right=85.09305>2015.</word> <word top=523.4429118 left=53.798 bottom=531.8463408 right=65.4163508>[21]</word> <word top=523.4429118 left=69.402 bottom=531.8463408 right=149.56862052>J. Li, M.-T. Luong, and D.</word> <word top=523.4429118 left=152.349911436 bottom=531.8463408 right=177.673173996>Jurafsky.</word> <word top=523.4429118 left=183.49894704 bottom=531.8463408 right=283.091924316>A hierarchical neural autoencoder</word> <word top=523.4429118 left=285.866101956 bottom=531.8463408 right=294.160181772>for</word> <word top=531.4129118 left=69.402 bottom=539.8163408 right=259.72995>paragraphs and documents. arXiv preprint arXiv:1506.01057, 2015.</word> <word top=539.3829118 left=53.798 bottom=547.7863408 right=295.198720068>[22] A. Madaan, A. Mittal, G. R. Mausam, G. Ramakrishnan, and S. Sarawagi. Nu-</word> <word top=547.3539118 left=69.402 bottom=555.7573408 right=216.25139874>merical relation extraction with minimal supervision.</word> <word top=547.3539118 left=218.750669184 bottom=555.7573408 right=294.917506422>In AAAI, pages 2764–2771,</word> <word top=555.3239118 left=69.402 bottom=563.7273408 right=85.09305>2016.</word> <word top=563.2939118 left=53.798 bottom=571.6973408 right=294.041766648>[23] C. Manning. Representations for language: From word embeddings to sentence</word> <word top=571.2639118 left=69.402 bottom=579.6673408 right=98.445505908>meanings.</word> <word top=571.2639118 left=105.00394638 bottom=579.6673408 right=294.914189028>https://simons.berkeley.edu/talks/christopher-manning-2017-3-27,</word> <word top=579.2339118 left=69.402 bottom=587.6373408 right=85.09305>2017.</word> <word top=587.2039118 left=53.798 bottom=595.6073408 right=294.161974039>[24] B. Min, R. Grishman, L. Wan, C. Wang, and D. Gondek. Distant supervision for</word> <word top=595.1739118 left=69.402 bottom=603.5773408 right=227.053535988>relation extraction with an incomplete knowledge base.</word> <word top=595.1739118 left=229.927299492 bottom=603.5773408 right=294.043813488>In HLT-NAACL, pages</word> <word top=603.1439118 left=69.227 bottom=611.5473408 right=112.81325>777–782, 2013.</word> <word top=611.1139118 left=53.798 bottom=619.5173408 right=294.046369356>[25] M. Mintz, S. Bills, R. Snow, and D. Jurafsky. Distant supervision for relation</word> <word top=619.0849118 left=69.402 bottom=627.4883408 right=157.2300372>extraction without labeled data.</word> <word top=619.0849118 left=159.7406052 bottom=627.4883408 right=251.5427426>In ACL, pages 1003–1011, 2009.</word> <word top=627.0549118 left=53.798 bottom=635.4583408 right=294.04622988>[26] N. Nakashole, M. Theobald, and G. Weikum. Scalable knowledge harvesting with</word> <word top=635.0249118 left=69.402 bottom=643.4283408 right=154.0430106>high precision and high recall.</word> <word top=635.0249118 left=156.5535786 bottom=643.4283408 right=268.036207>In WSDM, pages 227–236. ACM, 2011.</word> <word top=642.9949118 left=53.798 bottom=651.3983408 right=294.046369356>[27] T.-V. T. Nguyen and A. Moschitti. End-to-end relation extraction using distant</word> <word top=650.9649118 left=69.402 bottom=659.3683408 right=203.5221216>supervision from external semantic repositories.</word> <word top=650.9649118 left=206.0326896 bottom=659.3683408 right=290.9309426>In HLT, pages 277–282, 2011.</word> <word top=658.9349118 left=53.798 bottom=667.3383408 right=65.4163508>[28]</word> <word top=658.9349118 left=69.402 bottom=667.3383408 right=260.654259799>P. Pasupat and P. Liang. Zero-shot entity extraction from web pages.</word> <word top=658.9349118 left=263.146396045 bottom=667.3383408 right=294.91619345>In ACL (1),</word> <word top=666.9049118 left=69.402 bottom=675.3083408 right=130.6110426>pages 391–401, 2014.</word> <word top=674.8749118 left=53.798 bottom=683.2783408 right=294.16493093>[29] G. Penn, J. Hu, H. Luo, and R. T. McDonald. Flexible web document analysis for</word> <word top=682.8449118 left=69.402 bottom=691.2483408 right=176.4568038>delivery to narrow-bandwidth devices.</word> <word top=682.8449118 left=178.9673718 bottom=691.2483408 right=287.8276084>In ICDAR, volume 1, page 1074, 2001.</word> <word top=690.8159118 left=53.798 bottom=699.2193408 right=295.262739552>[30] D. Pinto, M. Branstein, R. Coleman, W. B. Croft, M. King, W. Li, and X. Wei.</word> <word top=698.7859118 left=69.402 bottom=707.1893408 right=265.059769656>Quasm: a system for question answering using semi-structured data.</word> <word top=698.7859118 left=268.260743856 bottom=707.1893408 right=294.916319>In JCDL,</word> <word top=706.7559118 left=69.402 bottom=715.1593408 right=123.6372426>pages 46–55, 2002.</word> </list><list><word top=65.15044 left=375.8352224 bottom=76.2508432 right=558.202832>SIGMOD’18, June 2018, Houston, Texas USA</word> <word top=94.6419118 left=317.955 bottom=103.0453408 right=558.201472482>[31] A. Ratner, C. De Sa, S. Wu, D. Selsam, and C. Ré. Data programming: Creating</word> <word top=102.6119118 left=333.559 bottom=111.0153408 right=522.93195>large training sets, quickly. arXiv preprint arXiv:1605.07723, 2016.</word> <word top=110.5819118 left=317.955 bottom=118.9853408 right=491.267442196>[32] A. J. Ratner, S. H. Bach, H. R. Ehrenberg, and C. Ré.</word> <word top=110.5819118 left=495.63499366 bottom=118.9853408 right=533.747926468>Snorkel: Fast</word> <word top=110.5819118 left=536.07396772 bottom=118.9853408 right=558.203369356>training</word> <word top=118.5519118 left=333.559 bottom=126.9553408 right=385.251176692>set generation for</word> <word top=118.5519118 left=387.982674676 bottom=126.9553408 right=454.420672516>information extraction.</word> <word top=118.5519118 left=460.10418004 bottom=126.9553408 right=512.273909292>In Proceedings of</word> <word top=118.4682262 left=515.005407276 bottom=126.7949434 right=558.204332424>the 2017 ACM</word> <word top=126.4382262 left=333.44 bottom=134.7649434 right=559.420880648>International Conference on Management of Data, pages 1683–1686. ACM, 2017.</word> <word top=134.4919118 left=317.955 bottom=142.8953408 right=558.202218679>[33] C. Ré, A. A. Sadeghian, Z. Shan, J. Shin, F. Wang, S. Wu, and C. Zhang. Feature</word> <word top=142.4619118 left=333.559 bottom=150.8653408 right=559.073319>engineering for knowledge base construction. arXiv preprint arXiv:1407.6439,</word> <word top=150.4329118 left=333.559 bottom=158.8363408 right=349.25005>2014.</word> <word top=158.4029118 left=317.955 bottom=166.8063408 right=329.5733508>[34]</word> <word top=158.4029118 left=333.559 bottom=166.8063408 right=490.263821717>J. Shin, S. Wu, F. Wang, C. De Sa, C. Zhang, and C. Ré.</word> <word top=158.4029118 left=492.785812801 bottom=166.8063408 right=558.204310819>Incremental knowledge</word> <word top=166.3729118 left=333.559 bottom=174.7763408 right=518.4479172>base construction using deepdive. VLDB, 8(11):1310–1321, 2015.</word> <word top=174.3429118 left=317.955 bottom=182.7463408 right=364.960766116>[35] A. Singhal.</word> <word top=174.3429118 left=367.470134622 bottom=182.7463408 right=558.204417511>Introducing the knowledge graph: things, not strings. Official google</word> <word top=182.2292262 left=333.559 bottom=190.5559434 right=365.06595>blog, 2012.</word> <word top=190.2829118 left=317.955 bottom=198.6863408 right=329.5733508>[36]</word> <word top=190.2829118 left=333.559 bottom=198.6863408 right=558.203369356>F. M. Suchanek, G. Kasneci, and G. Weikum. Yago: A large ontology from</word> <word top=198.2529118 left=333.308 bottom=206.6563408 right=558.197957784>wikipedia and wordnet. Web Semantics: Science, Services and Agents on the</word> <word top=206.1392262 left=332.98 bottom=214.4659434 right=439.5882172>World Wide Web, 6(3):203–217, 2008.</word> <word top=214.1929118 left=317.955 bottom=222.5963408 right=549.681664708>[37] A. Tengli, Y. Yang, and N. L. Ma. Learning table extraction from examples.</word> <word top=214.1929118 left=552.278010448 bottom=222.5963408 right=558.203369356>In</word> <word top=222.0802262 left=333.328 bottom=230.4069434 right=406.9365344>COLING, page 987, 2004.</word> <word top=230.1339118 left=317.955 bottom=238.5373408 right=329.5733508>[38]</word> <word top=230.1339118 left=333.559 bottom=238.5373408 right=558.202155915>J. Turian, L. Ratinov, and Y. Bengio. Word representations: a simple and general</word> <word top=238.1039118 left=333.559 bottom=246.5073408 right=437.448912041>method for semi-supervised learning.</word> <word top=238.1039118 left=439.944995459 bottom=246.5073408 right=558.198591705>In Proceedings of the 48th annual meeting</word> <word top=245.9902262 left=333.559 bottom=254.3169434 right=558.320521373>of the association for computational linguistics, pages 384–394. Association for</word> <word top=254.0439118 left=333.559 bottom=262.4473408 right=426.938182>Computational Linguistics, 2010.</word> <word top=262.0139118 left=317.955 bottom=270.4173408 right=329.5733508>[39]</word> <word top=262.0139118 left=333.559 bottom=270.4173408 right=466.705300168>P. Verga, D. Belanger, E. Strubell, B. Roth,</word> <word top=262.0139118 left=470.034313336 bottom=270.4173408 right=526.086928216>and A. McCallum.</word> <word top=262.0139118 left=533.662567156 bottom=270.4173408 right=559.355720068>Multilin-</word> <word top=269.9839118 left=333.559 bottom=278.3873408 right=513.389730556>gual relation extraction using compositional universal schema.</word> <word top=269.9002262 left=517.191 bottom=278.2269434 right=558.19903614>arXiv preprint</word> <word top=277.8702262 left=333.559 bottom=286.1969434 right=403.67995>arXiv:1511.06396, 2015.</word> <word top=285.9239118 left=317.955 bottom=294.3273408 right=559.071189028>[40] D. Welter, J. MacArthur, J. Morales, T. Burdett, P. Hall, H. Junkins, A. Klemm,</word> <word top=293.8949118 left=333.559 bottom=302.2983408 right=558.20322988>P. Flicek, T. Manolio, L. Hindorff, et al. The nhgri gwas catalog, a curated resource</word> <word top=301.8649118 left=333.559 bottom=310.2683408 right=540.3796336>of snp-trait associations. Nucleic acids research, 42:D1001–D1006, 2014.</word> <word top=309.8349118 left=317.955 bottom=318.2383408 right=559.071189028>[41] Y. Wu, M. Schuster, Z. Chen, Q. V. Le, M. Norouzi, W. Macherey, M. Krikun,</word> <word top=317.8049118 left=333.154 bottom=326.2083408 right=559.3561768>Y. Cao, Q. Gao, K. Macherey, et al. Google’s neural machine translation sys-</word> <word top=325.7749118 left=333.559 bottom=334.1783408 right=558.204618376>tem: Bridging the gap between human and machine translation. arXiv preprint</word> <word top=333.6612262 left=333.559 bottom=341.9879434 right=403.67995>arXiv:1609.08144, 2016.</word> <word top=341.7149118 left=317.955 bottom=350.1183408 right=558.317844283>[42] M. Yahya, S. E. Whang, R. Gupta, and A. Halevy. ReNoun : Fact Extraction for</word> <word top=349.6849118 left=333.559 bottom=358.0883408 right=479.2759426>Nominal Attributes. EMNLP, pages 325–335, 2014.</word> <word top=357.6549118 left=317.955 bottom=366.0583408 right=509.672695156>[43] Y. Yang and H. Zhang. Html page analysis based on visual cues.</word> <word top=357.6549118 left=512.023702612 bottom=366.0583408 right=558.200911804>In ICDAR, pages</word> <word top=365.6259118 left=333.559 bottom=374.0293408 right=395.7374008>859–864. IEEE, 2001.</word> <word top=373.5959118 left=317.955 bottom=381.9993408 right=558.203369356>[44] Z. Yang, D. Yang, C. Dyer, X. He, A. J. Smola, and E. H. Hovy. Hierarchical</word> <word top=381.5659118 left=333.559 bottom=389.9693408 right=461.374527448>attention networks for document classification.</word> <word top=381.5659118 left=463.677694636 bottom=389.9693408 right=559.071519916>In HLT-NAACL, pages 1480–1489,</word> <word top=389.5359118 left=333.559 bottom=397.9393408 right=349.25005>2016.</word> <word top=397.5059118 left=317.955 bottom=405.9093408 right=559.355915334>[45] Y. Zhang, A. Chaganty, A. Paranjape, D. Chen, J. Bolton, P. Qi, and C. D. Man-</word> <word top=405.4759118 left=333.559 bottom=413.8793408 right=559.42509543>ning. Stanford at tac kbp 2016: Sealing pipeline leaks and understanding chinese.</word> <word top=413.3622262 left=333.342 bottom=421.6889434 right=408.21295>Proceedings of TAC, 2016.</word> <word top=446.2149216 left=317.955 bottom=461.7686368 right=326.5866544>A</word> <word top=446.2149216 left=338.5418544 bottom=461.7686368 right=469.367608>DATA PROGRAMMING</word> <word top=468.4872304 left=317.955 bottom=479.2917424 right=558.202326586>Machine-learning based KBC systems rely heavily on ground truth</word> <word top=479.4462304 left=317.955 bottom=490.2507424 right=416.244527424>data (called training data)</word> <word top=479.4462304 left=419.41809504 bottom=490.2507424 right=559.320296256>to achieve high quality. Traditionally,</word> <word top=490.4052304 left=317.955 bottom=501.2097424 right=558.202335552>manual annotations or incomplete KBs are used to construct training</word> <word top=501.3642304 left=317.955 bottom=512.1687424 right=559.686032659>data for machine-learning based KBC systems. However, these re-</word> <word top=512.3232304 left=317.955 bottom=523.1277424 right=558.351715776>sources are either costly to obtain or may have limited coverage over</word> <word top=523.2822304 left=317.955 bottom=534.0867424 right=558.198964186>the candidates considered during the KBC process. To address this</word> <word top=534.2412304 left=317.955 bottom=545.0457424 right=558.201793523>challenge, SystemX builds upon the newly introduced paradigm of</word> <word top=545.2002304 left=317.955 bottom=556.0047424 right=559.68594048>data programming [31], which enables domain experts to program-</word> <word top=556.1592304 left=317.955 bottom=566.9637424 right=558.204128832>matically generate large training data sets by leveraging multiple</word> <word top=567.1172304 left=317.632 bottom=577.9217424 right=497.2020928>weak supervision sources and domain knowledge.</word> <word top=578.0762304 left=327.918 bottom=588.8807424 right=558.353762688>Data programming provides a simple, unifying framework for</word> <word top=589.0352304 left=317.632 bottom=599.8397424 right=558.201794195>weak supervision, in which training labels are noisy and may come</word> <word top=599.9942304 left=317.955 bottom=610.7987424 right=559.318293696>from multiple, potentially overlapping sources. In data programming,</word> <word top=610.9532304 left=317.955 bottom=621.7577424 right=559.322768>users encode this weak supervision in the form of labeling functions,</word> <word top=621.9122304 left=317.632 bottom=632.7167424 right=558.201794195>which are user-defined functions that each provide a label for some</word> <word top=632.8712304 left=317.955 bottom=643.6757424 right=558.513711725>subset of the data, and collectively generate a large and potentially</word> <word top=643.8302304 left=317.955 bottom=654.6347424 right=558.203447386>overlapping set of training labels. Many different weak supervision</word> <word top=654.7892304 left=317.955 bottom=665.5937424 right=558.204128832>approaches can be expressed as labeling functions. This includes</word> <word top=665.7482304 left=317.955 bottom=676.5527424 right=559.687601779>strategies which utilize existing knowledge bases, individual annota-</word> <word top=676.7062304 left=317.955 bottom=687.5107424 right=558.518670144>tor’s labels (as in crowdsourcing), or user-defined functions that rely</word> <word top=687.6652304 left=317.955 bottom=698.4697424 right=558.205500691>on domain-specific patterns and dictionaries to assign labels to the</word> <word top=698.6242304 left=317.955 bottom=709.4287424 right=355.8111408>input data.</word> </list></div><div id=14><list><word top=65.15044 left=53.798 bottom=76.2508432 right=236.1656096>SIGMOD’18, June 2018, Houston, Texas USA</word> <word top=92.6712304 left=63.761 bottom=103.4757424 right=294.041285312>The aforementioned sources of supervision can have varying</word> <word top=103.6302304 left=53.798 bottom=114.4347424 right=295.537882496>degrees of accuracy, and may conflict with each other. Data pro-</word> <word top=114.5892304 left=53.798 bottom=125.3937424 right=294.046904672>gramming relies on a generative probabilistic model to estimate the</word> <word top=125.5472304 left=53.798 bottom=136.3517424 right=294.044994829>accuracy of each labeling function by reasoning about the conflicts</word> <word top=136.5062304 left=53.798 bottom=147.3107424 right=295.533309632>and overlap across labeling functions. The estimated labeling func-</word> <word top=147.4652304 left=53.798 bottom=158.2697424 right=283.886225024>tion accuracies are in turn used to assign a probabilistic label</word> <word top=147.4652304 left=286.931752448 bottom=158.2697424 right=294.047128832>to</word> <word top=158.4242304 left=53.798 bottom=169.2287424 right=294.047128832>each candidate, which are used in conjunction with a noise-aware</word> <word top=169.3832304 left=53.798 bottom=180.1877424 right=288.0362336>discriminative model to train a machine learning model for KBC.</word> <word top=183.2362628 left=53.798 bottom=197.4290019 right=69.8561952>A.1</word> <word top=183.2362628 left=80.7652952 bottom=197.4290019 right=244.489068>Components of Data Programming</word> <word top=201.1212304 left=53.52 bottom=211.9257424 right=264.9746112>The main components in data programming are as follows:</word> <word top=216.9166912 left=53.798 bottom=228.5819776 right=290.3213152>Candidates A set of candidates C are probabilistically classified.</word> <word top=233.6356912 left=53.798 bottom=245.3009776 right=295.529370432>Labeling Functions Labeling functions are used to programmati-</word> <word top=245.5182304 left=53.798 bottom=256.3227424 right=295.534224205>cally provide labels for training data. A labeling function is a user-</word> <word top=256.4772304 left=53.798 bottom=267.2817424 right=295.609891654>defined procedure that takes a candidate as input and outputs a label.</word> <word top=267.4362304 left=53.798 bottom=278.2407424 right=294.047128832>Labels can be as simple as true or false for binary tasks, or one of</word> <word top=278.3952304 left=53.798 bottom=289.1997424 right=294.045335552>many classes for more complex multiclass tasks. Since each labeling</word> <word top=289.3542304 left=53.798 bottom=300.1587424 right=294.361670144>function is applied to all candidates and labeling functions are rarely</word> <word top=300.3132304 left=53.798 bottom=311.1177424 right=294.041964186>perfectly accurate, there may be disagreements between them. The</word> <word top=311.2722304 left=53.798 bottom=322.0767424 right=294.04608873>labeling functions provided by the user for binary classification can</word> <word top=322.2312304 left=53.798 bottom=333.0357424 right=293.54516906>be more formally defined as follows: For each labeling function λi</word> <word top=333.1902304 left=53.798 bottom=343.9947424 right=138.37616906>and r ∈ C, we have λi</word> <word top=335.1180064 left=142.086 bottom=343.7526496 right=294.049032704>: r (cid:55)→ {−1, 0, 1} where +1 or −1 denotes</word> <word top=344.1492304 left=53.798 bottom=354.9537424 right=295.537882496>a candidate as true or false, and 0 abstains. The output of apply-</word> <word top=355.1072304 left=53.798 bottom=365.9117424 right=294.270851776>ing a set of l labeling functions to k candidates is the label matrix</word> <word top=368.2600064 left=53.798 bottom=376.8946496 right=122.8046>Λ ∈ {−1, 0, 1}k×l.</word> <word top=382.1276912 left=53.798 bottom=393.7929776 right=294.043411533>Output Data programming frameworks output a confidence value</word> <word top=396.2040064 left=53.798 bottom=404.8386496 right=279.9186>p for the classification for each candidate as a vector Y ∈ {p}k.</word> <word top=405.2352304 left=63.761 bottom=416.0397424 right=295.531119424>To perform data programming in SystemX, we rely on a data pro-</word> <word top=416.4102304 left=53.798 bottom=427.2147424 right=294.041096192>gramming engine, Snorkel5. Snorkel accepts candidates, and labels</word> <word top=427.3692304 left=53.798 bottom=438.1737424 right=294.041964186>as input, and produces marginal probabilities for each candidate as</word> <word top=438.3272304 left=53.798 bottom=449.1317424 right=294.049603558>output. These input and output components are stored as relational</word> <word top=449.2862304 left=53.798 bottom=460.0907424 right=225.8721824>tables, whose schemas are detailed in Section 3.</word> <word top=463.1392628 left=53.798 bottom=477.3320019 right=69.8561952>A.2</word> <word top=463.1392628 left=80.7652952 bottom=477.3320019 right=190.5544776>Theoretical Guarantees</word> <word top=481.0252304 left=53.377 bottom=491.8297424 right=294.357096659>While data programming uses labeling functions to generate noisy</word> <word top=491.9842304 left=53.798 bottom=502.7887424 right=295.53035072>training data, it theoretically achieves a learning rate similar to meth-</word> <word top=502.9422304 left=53.798 bottom=513.7467424 right=294.04276192>ods that use manually labeled data [31]. In the typical supervised</word> <word top=514.0662304 left=53.798 bottom=524.8707424 right=247.788036608>learning setup, users are required to manually label</word> <word top=515.9940064 left=251.082 bottom=524.6286496 right=295.530908736>˜O(ϵ−2) ex-</word> <word top=525.0242304 left=53.798 bottom=535.8287424 right=156.660002816>amples for the target model</word> <word top=525.0242304 left=159.559198592 bottom=535.8287424 right=294.0471648>to achieve an expected loss of ϵ. To</word> <word top=535.9832304 left=53.798 bottom=546.7877424 right=294.361670144>achieve this rate, data programming only requires the user to specify</word> <word top=546.9422304 left=53.798 bottom=557.7467424 right=294.047128832>a constant number of labeling functions that does not depend on</word> <word top=559.8290064 left=53.798 bottom=568.4636496 right=295.163306944>ϵ. Let β be the minimum coverage across labeling functions (i.e.,</word> <word top=568.8602304 left=53.798 bottom=579.6647424 right=294.046447386>the probability that a labeling function provides a label for an input</word> <word top=579.8192304 left=53.798 bottom=590.6237424 right=294.047758912>point), and γ be the minimum reliability of labeling functions, where</word> <word top=592.7060064 left=53.798 bottom=601.3406496 right=295.611305088>γ = 2 · a − 1 with a denoting the accuracy of a labeling function.</word> <word top=601.7372304 left=53.52 bottom=612.5417424 right=295.533537024>Then under the assumptions that: (1) labeling functions are condition-</word> <word top=612.6962304 left=53.798 bottom=623.5007424 right=294.195307558>ally independent given the true labels of input data, (2) the number</word> <word top=623.8192304 left=53.798 bottom=634.6237424 right=215.641771059>of user-provided labeling functions is at least</word> <word top=625.7470064 left=217.886 bottom=634.3816496 right=294.647373875>˜O(γ−3β−1), and (3)</word> <word top=635.2422304 left=53.798 bottom=646.0467424 right=294.04947648>there are k = ˜O(ϵ−2) candidates, data programming achieves an</word> <word top=646.2012304 left=53.798 bottom=657.0057424 right=273.495561408>expected loss ϵ. Despite the strict assumptions with respect</word> <word top=646.2012304 left=276.129531072 bottom=657.0057424 right=295.536765888>to la-</word> <word top=657.1602304 left=53.798 bottom=667.9647424 right=294.047128832>beling functions, we find that using data programming to develop</word> <word top=668.1192304 left=53.798 bottom=678.9237424 right=294.047128832>KBC systems for richly formatted data leads to high-quality KBs</word> <word top=679.0782304 left=53.502 bottom=689.8827424 right=294.04911817>(across diverse real-world applications) even when some of the data</word> <word top=690.0372304 left=53.798 bottom=700.8417424 right=256.2682784>programming assumptions are not met (see Section 5.4).</word> <word top=707.0617745 left=53.798 bottom=713.664572 right=105.783328>5snorkel.stanford.edu</word> </list><list><word top=88.4839216 left=317.955 bottom=104.0376368 right=325.9291184>B</word> <word top=88.4839216 left=337.8843184 bottom=104.0376368 right=520.9782064>EXTENDED FEATURE LIBRARY</word> <word top=106.6743632 left=317.955 bottom=118.6355408 right=558.198337408>SystemX augments a bidirectional LSTM with features from an</word> <word top=118.7362304 left=317.955 bottom=129.5407424 right=559.694667302>extended feature library in order to better model the multiple modal-</word> <word top=129.6952304 left=317.955 bottom=140.4997424 right=558.204128832>ities of richly formatted data. In addition, these extended features</word> <word top=140.6542304 left=317.955 bottom=151.4587424 right=558.200004288>can provide signals drawn from the large contexts since they can be</word> <word top=151.6132304 left=317.955 bottom=162.4177424 right=558.199265856>calculated using SystemX’s data model of the document, rather than</word> <word top=162.5722304 left=317.955 bottom=173.3767424 right=558.20622697>being limited to a single sentence or table. In Section 5, we find that</word> <word top=173.5302304 left=317.955 bottom=184.3347424 right=558.515083584>including multimodal features is critical to achieving high quality</word> <word top=184.4892304 left=317.955 bottom=195.2937424 right=558.205043405>relation extraction. The provided extended feature library serves as</word> <word top=195.4482304 left=317.955 bottom=206.2527424 right=558.515083584>a baseline example of these types of features, which can be easily</word> <word top=206.4072304 left=317.955 bottom=217.2117424 right=559.326157229>enhanced in the future. However, even with these baseline features,</word> <word top=217.3662304 left=317.955 bottom=228.1707424 right=558.351213658>our users have been able to build high quality knowledge bases for</word> <word top=228.3252304 left=317.955 bottom=239.1297424 right=382.2082224>their applications.</word> <word top=239.2842304 left=327.918 bottom=250.0887424 right=558.198285312>The extended feature library consists of a baseline set of features</word> <word top=250.2432304 left=317.955 bottom=261.0477424 right=558.200452608>from the structural, tabular, and visual modalities. Table 5 lists the</word> <word top=261.2022304 left=317.955 bottom=272.0067424 right=558.45716064>details of extended feature library. When input documents are in PDF</word> <word top=272.1612304 left=317.955 bottom=282.9657424 right=492.556093248>format, we use tools such as Poppler to convert</word> <word top=272.1612304 left=495.10775136 bottom=282.9657424 right=558.643123776>them into HTML</word> <word top=283.1202304 left=317.955 bottom=293.9247424 right=558.204236429>format in order to extract tabular and structural information such as</word> <word top=294.0782304 left=317.955 bottom=304.8827424 right=559.319907648>parent-child relationships, or row and column numbers. Similarly,</word> <word top=305.0372304 left=317.632 bottom=315.8417424 right=558.450495232>when HTML/XML documents are provided, we render them as PDF</word> <word top=315.9962304 left=317.955 bottom=326.8007424 right=558.202335552>in order to extract visual information such as character positions and</word> <word top=326.9552304 left=317.955 bottom=337.7597424 right=558.19937664>bounding boxes. As shown in Table 5, the features are represented</word> <word top=337.9142304 left=317.955 bottom=348.7187424 right=558.203321856>as strings. This feature space is then mapped into one-dimensional</word> <word top=348.8732304 left=317.955 bottom=359.6777424 right=558.20308873>bit vector for each candidate, where each bit represents whether the</word> <word top=359.8322304 left=317.955 bottom=370.6367424 right=462.6188976>candidate has the corresponding feature.</word> <word top=373.2999216 left=317.955 bottom=388.8536368 right=326.5866544>C</word> <word top=372.845624 left=338.542 bottom=388.9492784 right=443.7834656>SystemX AT SCALE</word> <word top=392.5932304 left=317.534 bottom=403.3977424 right=558.20383232>We discuss design decisions for KBC from richly formatted data</word> <word top=403.5522304 left=317.955 bottom=414.3567424 right=558.204128832>by analyzing the benefits of caching, as well as discussing data</word> <word top=414.5112304 left=317.955 bottom=425.3157424 right=558.206603558>representation choices with respect to performance for the abstract</word> <word top=425.4702304 left=317.955 bottom=436.2747424 right=435.3236>data structures used in SystemX.</word> <word top=439.0832628 left=317.955 bottom=453.2760019 right=334.0131952>C.1</word> <word top=439.0832628 left=344.9222952 bottom=453.2760019 right=408.8605303>Data Caching</word> <word top=456.9352304 left=317.534 bottom=467.7397424 right=559.694585984>With richly formatted data, which frequently requires document-</word> <word top=467.8942304 left=317.955 bottom=478.6987424 right=558.199555968>level context, thousands of candidates need to be featurized for each</word> <word top=478.8532304 left=317.955 bottom=489.6577424 right=558.203447386>document. Candidate features from the extended feature library are</word> <word top=489.8122304 left=317.955 bottom=500.6167424 right=558.201788602>computed at both the mention level and relation level, by traversing</word> <word top=500.7712304 left=317.955 bottom=511.5757424 right=559.694882496>the data model accessing modality attributes. Because each men-</word> <word top=511.7302304 left=317.955 bottom=522.5347424 right=558.200291213>tion of a relation is part of many candidates, naïve featurization of</word> <word top=522.6892304 left=317.955 bottom=533.4937424 right=558.205043405>candidates can result in the redundant computation of thousands of</word> <word top=533.6482304 left=317.955 bottom=544.4527424 right=558.19937664>mention features. This pattern highlights the value of data caching</word> <word top=544.6072304 left=317.632 bottom=555.4117424 right=559.771568742>when performing multimodal featurization on richly formatted data.</word> <word top=555.5662304 left=327.918 bottom=566.3707424 right=558.198285312>Traditional KBC systems that operate on single sentences of</word> <word top=566.5242304 left=317.955 bottom=577.3287424 right=558.20308873>unstructured text pragmatically assume that only a small number of</word> <word top=577.4832304 left=317.955 bottom=588.2877424 right=558.203321856>candidates will need to be featurized for each sentence, and do not</word> <word top=588.4422304 left=317.955 bottom=599.2467424 right=440.9560752>cache mention features as a result.</word> <word top=603.4476336 left=327.918 bottom=614.1535152 right=466.569209088>Example C.1 (Inefficient Featurization).</word> <word top=603.5552304 left=471.054 bottom=614.3597424 right=558.35356032>In Figure 1, the transistor</word> <word top=614.5142304 left=317.955 bottom=625.3187424 right=558.203904672>part mention MMBT3904 could be matched with up to 15 different</word> <word top=625.4732304 left=317.955 bottom=636.2777424 right=558.200676768>numerical values in the datasheet. Without caching, the features of</word> <word top=636.4322304 left=317.955 bottom=647.2367424 right=558.202335552>the MMBT3904 would be unnecessarily recalculated 14 times, once</word> <word top=647.3912304 left=317.955 bottom=658.1957424 right=558.204128832>for each candidate. In real documents 100s of feature calculations</word> <word top=658.3502304 left=317.632 bottom=669.1547424 right=379.9574464>would be wasted.</word> <word top=673.4632304 left=317.955 bottom=684.2677424 right=558.2007216>In Example C.1, eliminating unnecessary feature computations can</word> <word top=684.4222304 left=317.955 bottom=695.2267424 right=490.244376>improve performance by an order of magnitude.</word> <word top=695.3812304 left=327.918 bottom=706.1857424 right=558.202265024>To optimize the feature generation process, SystemX implements</word> <word top=706.3392304 left=317.955 bottom=717.1437424 right=558.204128832>a document-level caching scheme for mention features. The first</word> </list></div><div id=15><list><word top=54.19144 left=53.798 bottom=65.2918432 right=214.7628128>SystemX: Knowledge Base Construction</word> <word top=65.15044 left=53.798 bottom=76.2508432 right=163.367408>from Richly Formatted Data</word> <word top=100.4146912 left=53.798 bottom=112.0799776 right=435.31832>prefixes represent the feature templates and the remainder of the string represents a feature’s value.</word> <word top=123.7276912 left=58.78 bottom=135.3929776 right=109.2787648>Feature Type</word> <word top=123.7276912 left=119.24 bottom=135.3929776 right=139.6564928>Arity</word> <word top=123.7276912 left=153.609 bottom=135.3929776 right=197.9388816>Description</word> <word top=136.0082304 left=58.78 bottom=146.8127424 right=94.1434816>Structural</word> <word top=136.0082304 left=119.24 bottom=146.8127424 right=141.6470336>Unary</word> <word top=136.0082304 left=153.609 bottom=146.8127424 right=246.7519632>HTML tag of the mention</word> <word top=138.5926062 left=440.245 bottom=145.9011486 right=473.71924>TAG_<h1></word> <word top=146.9672304 left=58.78 bottom=157.7717424 right=94.1434816>Structural</word> <word top=146.9672304 left=119.24 bottom=157.7717424 right=141.6470336>Unary</word> <word top=146.9672304 left=153.609 bottom=157.7717424 right=268.9886352>HTML attributes of the mention</word> <word top=157.9262304 left=58.78 bottom=168.7307424 right=94.1434816>Structural</word> <word top=157.9262304 left=119.24 bottom=168.7307424 right=141.6470336>Unary</word> <word top=157.9262304 left=153.609 bottom=168.7307424 right=277.3811856>HTML tag of the mention’s parent</word> <word top=168.8852304 left=58.78 bottom=179.6897424 right=94.1434816>Structural</word> <word top=168.8852304 left=119.24 bottom=179.6897424 right=141.6470336>Unary</word> <word top=168.8852304 left=153.609 bottom=179.6897424 right=312.2873808>HTML tag of the mention’s previous sibling</word> <word top=179.8442304 left=58.78 bottom=190.6487424 right=94.1434816>Structural</word> <word top=179.8442304 left=119.24 bottom=190.6487424 right=141.6470336>Unary</word> <word top=179.8442304 left=153.609 bottom=190.6487424 right=296.936904>HTML tag of the mention’s next sibling</word> <word top=190.8032304 left=58.78 bottom=201.6077424 right=94.1434816>Structural</word> <word top=190.8032304 left=119.24 bottom=201.6077424 right=141.6470336>Unary</word> <word top=190.8032304 left=153.609 bottom=201.6077424 right=286.1234256>Position of a node among its siblings</word> <word top=193.3876062 left=440.245 bottom=200.6961486 right=482.0878>NODE_POS_1</word> <word top=201.7622304 left=58.78 bottom=212.5667424 right=94.1434816>Structural</word> <word top=201.7622304 left=119.24 bottom=212.5667424 right=141.6470336>Unary</word> <word top=201.7622304 left=153.609 bottom=212.5667424 right=329.9153232>HTML class sequence of the mention’s ancestors</word> <word top=212.7212304 left=58.78 bottom=223.5257424 right=94.1434816>Structural</word> <word top=212.7212304 left=119.24 bottom=223.5257424 right=141.6470336>Unary</word> <word top=212.7212304 left=153.609 bottom=223.5257424 right=323.4415824>HTML tag sequence of the mention’s ancestors</word> <word top=224.1942304 left=58.78 bottom=234.9987424 right=94.1434816>Structural</word> <word top=224.1942304 left=119.24 bottom=234.9987424 right=141.6470336>Unary</word> <word top=224.1942304 left=153.609 bottom=234.9987424 right=290.3376336>HTML id’s of the mention’s ancestors</word> <word top=235.1532304 left=58.78 bottom=245.9577424 right=94.1434816>Structural</word> <word top=235.1532304 left=119.24 bottom=245.9577424 right=143.6465408>Binary</word> <word top=235.1532304 left=153.609 bottom=245.9577424 right=430.2852048>HTML tags shared between mentions on the path to the root of the document</word> <word top=246.1122304 left=58.78 bottom=256.9167424 right=94.1434816>Structural</word> <word top=246.1122304 left=119.24 bottom=256.9167424 right=143.6465408>Binary</word> <word top=246.1122304 left=153.609 bottom=256.9167424 right=423.0134544>Minimum distance between two mentions to their lowest common ancestor</word> <word top=257.9832304 left=58.78 bottom=268.7877424 right=85.768864>Tabular</word> <word top=257.9832304 left=119.24 bottom=268.7877424 right=141.6470336>Unary</word> <word top=257.9832304 left=153.609 bottom=268.7877424 right=301.0250788>N-grams in the same cell as the mentiona</word> <word top=260.5676062 left=440.245 bottom=267.8761486 right=477.35535>CELL_cevb</word> <word top=268.9422304 left=58.78 bottom=279.7467424 right=85.768864>Tabular</word> <word top=268.9422304 left=119.24 bottom=279.7467424 right=141.6470336>Unary</word> <word top=268.9422304 left=153.609 bottom=279.7467424 right=254.4989328>Row number of the mention</word> <word top=271.5266062 left=440.245 bottom=278.8351486 right=477.90352>ROW_NUM_5</word> <word top=279.9012304 left=58.78 bottom=290.7057424 right=85.768864>Tabular</word> <word top=279.9012304 left=119.24 bottom=290.7057424 right=141.6470336>Unary</word> <word top=279.9012304 left=153.609 bottom=290.7057424 right=266.6842704>Column number of the mention</word> <word top=282.4856062 left=440.245 bottom=289.7941486 right=477.90352>COL_NUM_3</word> <word top=290.8602304 left=58.78 bottom=301.6647424 right=85.768864>Tabular</word> <word top=290.8602304 left=119.24 bottom=301.6647424 right=141.6470336>Unary</word> <word top=290.8602304 left=153.609 bottom=301.6647424 right=279.1475664>Number of rows the mention spans</word> <word top=293.4446062 left=440.245 bottom=300.7531486 right=482.0878>ROW_SPAN_1</word> <word top=301.8192304 left=58.78 bottom=312.6237424 right=85.768864>Tabular</word> <word top=301.8192304 left=119.24 bottom=312.6237424 right=141.6470336>Unary</word> <word top=301.8192304 left=153.609 bottom=312.6237424 right=292.3281744>Number of columns the mention spans</word> <word top=304.4036062 left=440.245 bottom=311.7121486 right=482.0878>COL_SPAN_1</word> <word top=312.7782304 left=58.78 bottom=323.5827424 right=85.768864>Tabular</word> <word top=312.7782304 left=119.24 bottom=323.5827424 right=141.6470336>Unary</word> <word top=312.7782304 left=153.609 bottom=323.5827424 right=324.7148112>Row header n-grams in the table of the mention</word> <word top=323.7372304 left=58.78 bottom=334.5417424 right=85.768864>Tabular</word> <word top=323.7372304 left=119.24 bottom=334.5417424 right=141.6470336>Unary</word> <word top=323.7372304 left=153.609 bottom=334.5417424 right=336.9001488>Column header n-grams in the table of the mention</word> <word top=334.6962304 left=58.78 bottom=345.5007424 right=85.768864>Tabular</word> <word top=334.6962304 left=119.24 bottom=345.5007424 right=141.6470336>Unary</word> <word top=334.6962304 left=153.609 bottom=345.5007424 right=404.0380788>N-grams from all Cells that are in the same row as the given mentiona</word> <word top=345.6552304 left=58.78 bottom=356.4597424 right=85.768864>Tabular</word> <word top=345.6552304 left=119.24 bottom=356.4597424 right=141.6470336>Unary</word> <word top=345.6552304 left=153.609 bottom=356.4597424 right=417.2190788>N-grams from all Cells that are in the same column as the given mentiona</word> <word top=357.1282304 left=58.78 bottom=367.9327424 right=85.768864>Tabular</word> <word top=357.1282304 left=119.24 bottom=367.9327424 right=143.6465408>Binary</word> <word top=357.1282304 left=153.609 bottom=367.9327424 right=311.148648>Whether two mentions are in the same table</word> <word top=359.7126062 left=440.245 bottom=367.0211486 right=485.72435>SAME_TABLEb</word> <word top=368.6012304 left=58.78 bottom=379.4057424 right=85.768864>Tabular</word> <word top=368.6012304 left=119.24 bottom=379.4057424 right=143.6465408>Binary</word> <word top=368.6012304 left=153.609 bottom=379.4057424 right=372.702984>Row number difference if two mentions are in the same table</word> <word top=380.0752304 left=58.78 bottom=390.8797424 right=85.768864>Tabular</word> <word top=380.0752304 left=119.24 bottom=390.8797424 right=143.6465408>Binary</word> <word top=380.0752304 left=153.609 bottom=390.8797424 right=384.8883216>Column number difference if two mentions are in the same table</word> <word top=391.5482304 left=58.78 bottom=402.3527424 right=85.768864>Tabular</word> <word top=391.5482304 left=119.24 bottom=402.3527424 right=143.6465408>Binary</word> <word top=391.5482304 left=153.609 bottom=402.3527424 right=369.1791888>Manhattan distance between two mentions in the same table</word> <word top=403.0212304 left=58.78 bottom=413.8257424 right=85.768864>Tabular</word> <word top=403.0212304 left=119.24 bottom=413.8257424 right=143.6465408>Binary</word> <word top=403.0212304 left=153.609 bottom=413.8257424 right=306.665448>Whether two mentions are in the same cell</word> <word top=405.6056062 left=440.245 bottom=412.9141486 right=481.54035>SAME_CELLb</word> <word top=414.4952304 left=58.78 bottom=425.2997424 right=85.768864>Tabular</word> <word top=414.4952304 left=119.24 bottom=425.2997424 right=143.6465408>Binary</word> <word top=414.4952304 left=153.609 bottom=425.2997424 right=352.0085328>Mention distance in words of mentions in the same cell</word> <word top=425.9682304 left=58.78 bottom=436.7727424 right=85.768864>Tabular</word> <word top=425.9682304 left=119.24 bottom=436.7727424 right=143.6465408>Binary</word> <word top=425.9682304 left=153.609 bottom=436.7727424 right=349.1034192>Mention distance in chars of mentions in the same cell</word> <word top=437.4412304 left=58.78 bottom=448.2457424 right=85.768864>Tabular</word> <word top=437.4412304 left=119.24 bottom=448.2457424 right=143.6465408>Binary</word> <word top=437.4412304 left=153.609 bottom=448.2457424 right=355.218504>Whether two mentions in a cell are in the same sentence</word> <word top=448.9152304 left=58.78 bottom=459.7197424 right=85.768864>Tabular</word> <word top=448.9152304 left=119.24 bottom=459.7197424 right=143.6465408>Binary</word> <word top=448.9152304 left=153.609 bottom=459.7197424 right=323.3698512>Whether two mention are in the different tables</word> <word top=451.4996062 left=440.245 bottom=458.8081486 right=485.72435>DIFF_TABLEb</word> <word top=460.3882304 left=58.78 bottom=471.1927424 right=85.768864>Tabular</word> <word top=460.3882304 left=119.24 bottom=471.1927424 right=143.6465408>Binary</word> <word top=460.3882304 left=153.609 bottom=471.1927424 right=364.0145424>Row number difference of two mentions in different tables</word> <word top=471.8612304 left=58.78 bottom=482.6657424 right=85.768864>Tabular</word> <word top=471.8612304 left=119.24 bottom=482.6657424 right=143.6465408>Binary</word> <word top=471.8612304 left=153.609 bottom=482.6657424 right=397.1095248>Column number difference if two mentions are in the different table</word> <word top=483.3352304 left=58.78 bottom=494.1397424 right=85.768864>Tabular</word> <word top=483.3352304 left=119.24 bottom=494.1397424 right=143.6465408>Binary</word> <word top=483.3352304 left=153.609 bottom=494.1397424 right=371.6897808>Manhattan distance between two mentions in different tables</word> <word top=494.6922304 left=58.78 bottom=505.4967424 right=81.6532864>Visual</word> <word top=494.6922304 left=119.24 bottom=505.4967424 right=141.6470336>Unary</word> <word top=494.6922304 left=153.609 bottom=505.4967424 right=361.5560788>N-grams of all lemmas visually aligned with the mentiona</word> <word top=505.6512304 left=58.78 bottom=516.4557424 right=81.6532864>Visual</word> <word top=505.6512304 left=119.24 bottom=516.4557424 right=141.6470336>Unary</word> <word top=505.6512304 left=153.609 bottom=516.4557424 right=255.0817488>Page number of the mention</word> <word top=508.2356062 left=440.245 bottom=515.5441486 right=465.35068>PAGE_1</word> <word top=516.6102304 left=58.78 bottom=527.4147424 right=81.6532864>Visual</word> <word top=516.6102304 left=119.24 bottom=527.4147424 right=143.6465408>Binary</word> <word top=516.6102304 left=153.609 bottom=527.4147424 right=312.6370704>Whether two mentions are on the same page</word> <word top=519.1946062 left=440.245 bottom=526.5031486 right=477.90352>SAME_PAGE</word> <word top=528.0832304 left=58.78 bottom=538.8877424 right=81.6532864>Visual</word> <word top=528.0832304 left=119.24 bottom=538.8877424 right=143.6465408>Binary</word> <word top=528.0832304 left=153.609 bottom=538.8877424 right=322.6077072>Whether two mentions are horizontally aligned</word> <word top=539.0422304 left=58.78 bottom=549.8467424 right=81.6532864>Visual</word> <word top=539.0422304 left=119.24 bottom=549.8467424 right=143.6465408>Binary</word> <word top=539.0422304 left=153.609 bottom=549.8467424 right=313.0046928>Whether two mentions are vertically aligned</word> <word top=550.5152304 left=58.78 bottom=561.3197424 right=81.6532864>Visual</word> <word top=550.5152304 left=119.24 bottom=561.3197424 right=143.6465408>Binary</word> <word top=550.5152304 left=153.609 bottom=561.3197424 right=411.124008>Whether two mentions’ left bounding box borders are vertically aligned</word> <word top=561.9892304 left=58.78 bottom=572.7937424 right=81.6532864>Visual</word> <word top=561.9892304 left=119.24 bottom=572.7937424 right=143.6465408>Binary</word> <word top=561.9892304 left=153.609 bottom=572.7937424 right=416.1093264>Whether two mentions’ right bounding box borders are vertically aligned</word> <word top=573.4622304 left=58.78 bottom=584.2667424 right=81.6532864>Visual</word> <word top=573.4622304 left=119.24 bottom=584.2667424 right=143.6465408>Binary</word> <word top=573.4622304 left=153.609 bottom=584.2667424 right=422.1885456>Whether the center of two mentions’ bounding boxes are vertically aligned</word> <word top=588.5202997 left=60.971 bottom=597.2839032 right=197.6373888>a All N-grams are 1-grams by default.</word> <word top=599.6432997 left=60.971 bottom=608.4069032 right=482.8137408>b This feature was not present in the example candidate. The values shown are example values from other documents.</word> <word top=610.6022997 left=60.971 bottom=619.3659032 right=474.9771072>c In this example, the mention is 200, which forms part of the feature prefix. The value is shown in square brackets.</word> <word top=639.0292304 left=53.798 bottom=649.8337424 right=295.618113843>computation of a mention feature requires traversing the data model.</word> <word top=649.9882304 left=53.52 bottom=660.7927424 right=74.60090304>Then,</word> <word top=649.9882304 left=77.536681728 bottom=660.7927424 right=111.96120192>the result</word> <word top=649.9882304 left=114.88783488 bottom=660.7927424 right=294.043500672>is cached for fast access if the feature is needed</word> <word top=660.9472304 left=53.798 bottom=671.7517424 right=294.04685984>again. All features are cached until all candidates in a document are</word> <word top=660.9472304 left=317.955 bottom=671.7517424 right=440.974008>memory footprint of featurization.</word> <word top=671.9062304 left=53.798 bottom=682.7107424 right=294.28052>fully featurized, after which the cache is flushed. Because SystemX</word> <word top=682.8652304 left=53.798 bottom=693.6697424 right=294.047128832>operates on documents atomically, caching a single document at</word> <word top=693.8242304 left=53.798 bottom=704.6287424 right=294.358083584>a time improves performance without adding significant memory</word> <word top=704.7832304 left=53.798 bottom=715.5877424 right=294.048177472>overhead. In the ELECTRONICS application, we find that caching</word> </list><list><word top=65.15044 left=375.8352224 bottom=76.2508432 right=558.202832>SIGMOD’18, June 2018, Houston, Texas USA</word> <word top=123.7276912 left=440.245 bottom=135.3929776 right=497.9527504>Example Value</word> <word top=138.5926062 left=440.245 bottom=145.9011486 right=473.71924>TAG_<h1></word> <word top=149.5516062 left=440.245 bottom=156.8601486 right=553.22056>HTML_ATTR_font-family:Arial</word> <word top=160.5106062 left=440.245 bottom=167.8191486 right=498.82492>PARENT_TAG_<p></word> <word top=171.4696062 left=440.245 bottom=178.7781486 right=511.37776>PREV_SIB_TAG_<td></word> <word top=182.4286062 left=440.245 bottom=189.7371486 right=511.37776>NEXT_SIB_TAG_<h1></word> <word top=193.3876062 left=440.245 bottom=200.6961486 right=482.0878>NODE_POS_1</word> <word top=204.3466062 left=440.245 bottom=211.6551486 right=519.74632>ANCESTOR_CLASS_<s1></word> <word top=215.3056062 left=440.245 bottom=222.6141486 right=536.48344>ANCESTOR_TAG_<body>_<p></word> <word top=226.7786062 left=440.245 bottom=234.0871486 right=502.46135>ANCESTOR_ID_l1b</word> <word top=237.7376062 left=440.245 bottom=245.0461486 right=532.29916>COMMON_ANCESTOR_<body></word> <word top=248.6966062 left=440.245 bottom=256.0051486 right=536.48344>LOWEST_ANCESTOR_DEPTH_1</word> <word top=260.5676062 left=440.245 bottom=267.8761486 right=477.35535>CELL_cevb</word> <word top=271.5266062 left=440.245 bottom=278.8351486 right=477.90352>ROW_NUM_5</word> <word top=282.4856062 left=440.245 bottom=289.7941486 right=477.90352>COL_NUM_3</word> <word top=293.4446062 left=440.245 bottom=300.7531486 right=482.0878>ROW_SPAN_1</word> <word top=304.4036062 left=440.245 bottom=311.7121486 right=482.0878>COL_SPAN_1</word> <word top=315.3626062 left=440.245 bottom=322.6711486 right=515.56204>ROW_HEAD_collector</word> <word top=326.3216062 left=440.245 bottom=333.6301486 right=498.82492>COL_HEAD_value</word> <word top=337.2806062 left=440.245 bottom=344.5891486 right=493.6860788>ROW_200_[ma]c</word> <word top=348.2396062 left=440.245 bottom=355.5481486 right=489.5010788>COL_200_[6]c</word> <word top=359.7126062 left=440.245 bottom=367.0211486 right=485.72435>SAME_TABLEb</word> <word top=371.1856062 left=440.245 bottom=378.4941486 right=531.75135>SAME_TABLE_ROW_DIFF_1b</word> <word top=382.6596062 left=440.245 bottom=389.9681486 right=531.75135>SAME_TABLE_COL_DIFF_3b</word> <word top=394.1326062 left=440.245 bottom=401.4411486 right=561.04135>SAME_TABLE_MANHATTAN_DIST_10b</word> <word top=405.6056062 left=440.245 bottom=412.9141486 right=481.54035>SAME_CELLb</word> <word top=417.0796062 left=440.245 bottom=424.3881486 right=489.90835>WORD_DIFF_1b</word> <word top=428.5526062 left=440.245 bottom=435.8611486 right=489.90835>CHAR_DIFF_1b</word> <word top=440.0256062 left=440.245 bottom=447.3341486 right=489.90835>SAME_PHRASEb</word> <word top=451.4996062 left=440.245 bottom=458.8081486 right=485.72435>DIFF_TABLEb</word> <word top=462.9726062 left=440.245 bottom=470.2811486 right=531.75135>DIFF_TABLE_ROW_DIFF_4b</word> <word top=474.4456062 left=440.245 bottom=481.7541486 right=531.75135>DIFF_TABLE_COL_DIFF_2b</word> <word top=485.9196062 left=440.245 bottom=493.2281486 right=556.85735>DIFF_TABLE_MANHATTAN_DIST_7b</word> <word top=497.2766062 left=440.245 bottom=504.5851486 right=503.0092>ALIGNED_current</word> <word top=508.2356062 left=440.245 bottom=515.5441486 right=465.35068>PAGE_1</word> <word top=519.1946062 left=440.245 bottom=526.5031486 right=477.90352>SAME_PAGE</word> <word top=530.6676062 left=440.245 bottom=537.9761486 right=494.09335>HORZ_ALIGNEDb</word> <word top=541.6266062 left=440.245 bottom=548.9351486 right=490.45636>VERT_ALIGNED</word> <word top=553.0996062 left=440.245 bottom=560.4081486 right=515.01435>VERT_ALIGNED_LEFTb</word> <word top=564.5736062 left=440.245 bottom=571.8821486 right=519.19835>VERT_ALIGNED_RIGHTb</word> <word top=576.0466062 left=440.245 bottom=583.3551486 right=523.38335>VERT_ALIGNED_CENTERb</word> <word top=639.0292304 left=317.955 bottom=649.8337424 right=558.203043712>achieves over 100× speed up on average and in some cases even</word> <word top=649.9882304 left=317.955 bottom=660.7927424 right=558.201734912>over 1000×, while only accounting for approximately 10% of the</word> <word top=660.9472304 left=317.955 bottom=671.7517424 right=440.974008>memory footprint of featurization.</word> <word top=685.1814374 left=327.918 bottom=697.0767818 right=559.69106336>Takeaways. When performing feature generation from richly for-</word> <word top=697.2452304 left=317.955 bottom=708.0497424 right=558.20006592>matted data, caching the intermediate results can yield over 1000×</word> </list><table_caption><word top=89.4556912 left=53.502 bottom=101.1209776 right=558.2039648>Table 5: Features from SystemX’s feature library. Example values are drawn from the example candidate in Figure 1. Capitalized</word> <word top=100.4146912 left=53.798 bottom=112.0799776 right=435.31832>prefixes represent the feature templates and the remainder of the string represents a feature’s value.</word> </table_caption><table><tr><td top=127.69 left=58.78 bottom=133.519999924 right=110.400803833>Feature Type</td><td top=127.69 left=119.24 bottom=133.519999924 right=140.770799866>Arity</td><td top=127.69 left=153.61 bottom=133.519999924 right=199.060805664>Description</td><td top=127.69 left=440.24 bottom=133.519999924 right=497.950010529>Example Value</td></tr><tr><td top=139.47 left=58.78 bottom=144.870000095 right=95.2608044434>Structural</td><td top=139.47 left=119.24 bottom=144.870000095 right=142.760805359>Unary</td><td top=139.47 left=153.61 bottom=144.870000095 right=248.003997192>HTML tag of the mention</td><td top=141.22 left=440.24 bottom=144.870000095 right=473.71000885>TAG_<h1></td></tr><tr><td top=150.43 left=58.78 bottom=155.830000095 right=95.2608044434>Structural</td><td top=150.43 left=119.24 bottom=155.830000095 right=142.760805359>Unary</td><td top=150.43 left=153.61 bottom=155.830000095 right=270.244017944>HTML attributes of the mention</td><td top=152.18 left=440.24 bottom=155.830000095 right=553.219988098>HTML_ATTR_font-family:Arial</td></tr><tr><td top=161.39 left=58.78 bottom=166.790000095 right=95.2608044434>Structural</td><td top=161.39 left=119.24 bottom=166.790000095 right=142.760805359>Unary</td><td top=161.39 left=153.61 bottom=166.790000095 right=278.634002075>HTML tag of the mention’s parent</td><td top=163.14 left=440.24 bottom=166.790000095 right=498.820024719>PARENT_TAG_<p></td></tr><tr><td top=172.35 left=58.78 bottom=177.750000095 right=95.2608044434>Structural</td><td top=172.35 left=119.24 bottom=177.750000095 right=142.760805359>Unary</td><td top=172.35 left=153.61 bottom=177.750000095 right=313.533995972>HTML tag of the mention’s previous sibling</td><td top=174.1 left=440.24 bottom=177.750000095 right=511.370012512>PREV_SIB_TAG_<td></td></tr><tr><td top=183.31 left=58.78 bottom=188.710000095 right=95.2608044434>Structural</td><td top=183.31 left=119.24 bottom=188.710000095 right=142.760805359>Unary</td><td top=183.31 left=153.61 bottom=188.710000095 right=298.184020386>HTML tag of the mention’s next sibling</td><td top=185.06 left=440.24 bottom=188.710000095 right=511.370012512>NEXT_SIB_TAG_<h1></td></tr><tr><td top=194.27 left=58.78 bottom=199.670000095 right=95.2608044434>Structural</td><td top=194.27 left=119.24 bottom=199.670000095 right=142.760805359>Unary</td><td top=194.27 left=153.61 bottom=199.670000095 right=287.384002075>Position of a node among its siblings</td><td top=196.02 left=440.24 bottom=199.670000095 right=482.080003967>NODE_POS_1</td></tr><tr><td top=205.23 left=58.78 bottom=210.630000095 right=95.2608044434>Structural</td><td top=205.23 left=119.24 bottom=210.630000095 right=142.760805359>Unary</td><td top=205.23 left=153.61 bottom=210.630000095 right=331.173980103>HTML class sequence of the mention’s ancestors</td><td top=206.98 left=440.24 bottom=210.630000095 right=519.740007629>ANCESTOR_CLASS_<s1></td></tr><tr><td top=216.19 left=58.78 bottom=221.590000095 right=95.2608044434>Structural</td><td top=216.19 left=119.24 bottom=221.590000095 right=142.760805359>Unary</td><td top=216.19 left=153.61 bottom=221.590000095 right=324.693999634>HTML tag sequence of the mention’s ancestors</td><td top=217.93 left=440.24 bottom=221.580000095 right=536.479997864>ANCESTOR_TAG_<body>_<p></td></tr><tr><td top=227.66 left=58.78 bottom=233.060000095 right=95.2608044434>Structural</td><td top=227.66 left=119.24 bottom=233.060000095 right=142.760805359>Unary</td><td top=227.66 left=153.61 bottom=233.060000095 right=291.354949341>HTML id’s of the mention’s ancestors</td><td top=225.43 left=440.24 bottom=233.060011082 right=502.469995728>bANCESTOR_ID_l1</td></tr><tr><td top=238.62 left=58.78 bottom=244.020000095 right=95.2608044434>Structural</td><td top=238.62 left=119.24 bottom=244.020000095 right=144.760805359>Binary</td><td top=238.62 left=153.61 bottom=244.020000095 right=431.53401123>HTML tags shared between mentions on the path to the root of the document</td><td top=240.37 left=440.24 bottom=244.020000095 right=532.289995422>COMMON_ANCESTOR_<body></td></tr><tr><td top=249.58 left=58.78 bottom=254.980000095 right=95.2608044434>Structural</td><td top=249.58 left=119.24 bottom=254.980000095 right=144.760805359>Binary</td><td top=249.58 left=153.61 bottom=254.980000095 right=424.274001465>Minimum distance between two mentions to their lowest common ancestor</td><td top=251.33 left=440.24 bottom=254.980000095 right=536.479997864>LOWEST_ANCESTOR_DEPTH_1</td></tr><tr><td top=261.45 left=58.78 bottom=266.850000095 right=86.8907940674>Tabular</td><td top=261.45 left=119.24 bottom=266.850000095 right=142.760805359>Unary</td><td top=259.22 left=153.61 bottom=266.850011082 right=301.939086304>N-grams in the same cell as the mentiona</td><td top=259.22 left=440.24 bottom=266.850011082 right=477.360010376>bCELL_cev</td></tr><tr><td top=272.41 left=58.78 bottom=277.810000095 right=86.8907940674>Tabular</td><td top=272.41 left=119.24 bottom=277.810000095 right=142.760805359>Unary</td><td top=272.41 left=153.61 bottom=277.810000095 right=255.753997192>Row number of the mention</td><td top=274.16 left=440.24 bottom=277.810000095 right=477.900011292>ROW_NUM_5</td></tr><tr><td top=283.37 left=58.78 bottom=288.770000095 right=86.8907940674>Tabular</td><td top=283.37 left=119.24 bottom=288.770000095 right=142.760805359>Unary</td><td top=283.37 left=153.61 bottom=288.770000095 right=267.934020386>Column number of the mention</td><td top=285.11 left=440.24 bottom=288.760000095 right=477.900011292>COL_NUM_3</td></tr><tr><td top=294.33 left=58.78 bottom=299.730000095 right=86.8907940674>Tabular</td><td top=294.33 left=119.24 bottom=299.730000095 right=142.760805359>Unary</td><td top=294.33 left=153.61 bottom=299.730000095 right=280.403991089>Number of rows the mention spans</td><td top=296.07 left=440.24 bottom=299.720000095 right=482.080003967>ROW_SPAN_1</td></tr><tr><td top=305.28 left=58.78 bottom=310.680000095 right=86.8907940674>Tabular</td><td top=305.28 left=119.24 bottom=310.680000095 right=142.760805359>Unary</td><td top=305.28 left=153.61 bottom=310.680000095 right=293.583983765>Number of columns the mention spans</td><td top=307.03 left=440.24 bottom=310.680000095 right=482.080003967>COL_SPAN_1</td></tr><tr><td top=316.24 left=58.78 bottom=321.640000095 right=86.8907940674>Tabular</td><td top=316.24 left=119.24 bottom=321.640000095 right=142.760805359>Unary</td><td top=316.24 left=153.61 bottom=321.640000095 right=325.964019165>Row header n-grams in the table of the mention</td><td top=317.99 left=440.24 bottom=321.640000095 right=515.560014954>ROW_HEAD_collector</td></tr><tr><td top=327.2 left=58.78 bottom=332.600000095 right=86.8907940674>Tabular</td><td top=327.2 left=119.24 bottom=332.600000095 right=142.760805359>Unary</td><td top=327.2 left=153.61 bottom=332.600000095 right=338.154021606>Column header n-grams in the table of the mention</td><td top=328.95 left=440.24 bottom=332.600000095 right=498.820024719>COL_HEAD_value</td></tr><tr><td top=338.16 left=58.78 bottom=343.560000095 right=86.8907940674>Tabular</td><td top=338.16 left=119.24 bottom=343.560000095 right=142.760805359>Unary</td><td top=335.93 left=153.61 bottom=343.560011082 right=404.949096069>N-grams from all Cells that are in the same row as the given mentiona</td><td top=335.93 left=440.24 bottom=343.560011082 right=493.690000763>cROW_200_[ma]</td></tr><tr><td top=349.12 left=58.78 bottom=354.520000095 right=86.8907940674>Tabular</td><td top=349.12 left=119.24 bottom=354.520000095 right=142.760805359>Unary</td><td top=346.89 left=153.61 bottom=354.519980564 right=418.474013672>N-grams from all Cells that are in the same column as the given mentiona</td><td top=346.89 left=440.24 bottom=354.519980564 right=489.499998322>COL_200_[6]c</td></tr><tr><td top=360.59 left=58.78 bottom=365.990000095 right=86.8907940674>Tabular</td><td top=360.59 left=119.24 bottom=365.990000095 right=144.760805359>Binary</td><td top=360.59 left=153.61 bottom=365.990000095 right=312.164977417>Whether two mentions are in the same table</td><td top=358.36 left=440.24 bottom=365.990011082 right=485.730005493>bSAME_TABLE</td></tr><tr><td top=372.07 left=58.78 bottom=377.470000095 right=86.8907940674>Tabular</td><td top=372.07 left=119.24 bottom=377.470000095 right=144.760805359>Binary</td><td top=372.07 left=153.61 bottom=377.470000095 right=373.71496521>Row number difference if two mentions are in the same table</td><td top=369.83 left=440.24 bottom=377.460011082 right=531.749994507>bSAME_TABLE_ROW_DIFF_1</td></tr><tr><td top=383.54 left=58.78 bottom=388.940000095 right=86.8907940674>Tabular</td><td top=383.54 left=119.24 bottom=388.940000095 right=144.760805359>Binary</td><td top=383.54 left=153.61 bottom=388.940000095 right=385.904967651>Column number difference if two mentions are in the same table</td><td top=381.31 left=440.24 bottom=388.940011082 right=531.749994507>bSAME_TABLE_COL_DIFF_3</td></tr><tr><td top=395.01 left=58.78 bottom=400.410000095 right=86.8907940674>Tabular</td><td top=395.01 left=119.24 bottom=400.410000095 right=144.760805359>Binary</td><td top=395.01 left=153.61 bottom=400.410000095 right=370.434020386>Manhattan distance between two mentions in the same table</td><td top=396.76 left=440.24 bottom=400.410000095 right=557.399980774>SAME_TABLE_MANHATTAN_DIST_10</td></tr><tr><td top=406.49 left=58.78 bottom=411.890000095 right=86.8907940674>Tabular</td><td top=406.49 left=119.24 bottom=411.890000095 right=144.760805359>Binary</td><td top=406.49 left=153.61 bottom=411.890000095 right=307.674956665>Whether two mentions are in the same cell</td><td top=404.25 left=440.24 bottom=411.880011082 right=481.540003052>bSAME_CELL</td></tr><tr><td top=417.96 left=58.78 bottom=423.360000095 right=86.8907940674>Tabular</td><td top=417.96 left=119.24 bottom=423.360000095 right=144.760805359>Binary</td><td top=417.96 left=153.61 bottom=423.360000095 right=353.26397644>Mention distance in words of mentions in the same cell</td><td top=415.73 left=440.24 bottom=423.359980564 right=489.909998169>WORD_DIFF_1b</td></tr><tr><td top=429.43 left=58.78 bottom=434.830000095 right=86.8907940674>Tabular</td><td top=429.43 left=119.24 bottom=434.830000095 right=144.760805359>Binary</td><td top=429.43 left=153.61 bottom=434.830000095 right=350.353972778>Mention distance in chars of mentions in the same cell</td><td top=427.2 left=440.24 bottom=434.829980564 right=489.909998169>CHAR_DIFF_1b</td></tr><tr><td top=440.91 left=58.78 bottom=446.310000095 right=86.8907940674>Tabular</td><td top=440.91 left=119.24 bottom=446.310000095 right=144.760805359>Binary</td><td top=440.91 left=153.61 bottom=446.310000095 right=356.473998413>Whether two mentions in a cell are in the same sentence</td><td top=438.67 left=440.24 bottom=446.299980564 right=489.909998169>SAME_PHRASEb</td></tr><tr><td top=452.38 left=58.78 bottom=457.780000095 right=86.8907940674>Tabular</td><td top=452.38 left=119.24 bottom=457.780000095 right=144.760805359>Binary</td><td top=452.38 left=153.61 bottom=457.780000095 right=324.38494812>Whether two mention are in the different tables</td><td top=450.15 left=440.24 bottom=457.780011082 right=485.730005493>bDIFF_TABLE</td></tr><tr><td top=463.85 left=58.78 bottom=469.250000095 right=86.8907940674>Tabular</td><td top=463.85 left=119.24 bottom=469.250000095 right=144.760805359>Binary</td><td top=463.85 left=153.61 bottom=469.250000095 right=365.034942017>Row number difference of two mentions in different tables</td><td top=461.62 left=440.24 bottom=469.250011082 right=531.749994507>bDIFF_TABLE_ROW_DIFF_4</td></tr><tr><td top=475.33 left=58.78 bottom=480.730000095 right=86.8907940674>Tabular</td><td top=475.33 left=119.24 bottom=480.730000095 right=144.760805359>Binary</td><td top=475.33 left=153.61 bottom=480.730000095 right=398.124968872>Column number difference if two mentions are in the different table</td><td top=473.09 left=440.24 bottom=480.720011082 right=531.749994507>bDIFF_TABLE_COL_DIFF_2</td></tr><tr><td top=486.8 left=58.78 bottom=492.200000095 right=86.8907940674>Tabular</td><td top=486.8 left=119.24 bottom=492.200000095 right=144.760805359>Binary</td><td top=486.8 left=153.61 bottom=492.200000095 right=372.943999634>Manhattan distance between two mentions in different tables</td><td top=484.57 left=440.24 bottom=492.199980564 right=556.859979858>DIFF_TABLE_MANHATTAN_DIST_7b</td></tr><tr><td top=498.16 left=58.78 bottom=503.560000095 right=82.7707989502>Visual</td><td top=498.16 left=119.24 bottom=503.560000095 right=142.760805359>Unary</td><td top=495.92 left=153.61 bottom=503.55999033 right=362.813994751>N-grams of all lemmas visually aligned with the mentiona</td><td top=499.91 left=440.24 bottom=503.560000095 right=503.000017395>ALIGNED_current</td></tr><tr><td top=509.12 left=58.78 bottom=514.520000095 right=82.7707989502>Visual</td><td top=509.12 left=119.24 bottom=514.520000095 right=142.760805359>Unary</td><td top=509.12 left=153.61 bottom=514.520000095 right=256.334014282>Page number of the mention</td><td top=510.86 left=440.24 bottom=514.510000095 right=465.350023499>PAGE_1</td></tr><tr><td top=520.08 left=58.78 bottom=525.480000095 right=82.7707989502>Visual</td><td top=520.08 left=119.24 bottom=525.480000095 right=144.760805359>Binary</td><td top=520.08 left=153.61 bottom=525.480000095 right=313.894011841>Whether two mentions are on the same page</td><td top=521.82 left=440.24 bottom=525.470000095 right=477.900011292>SAME_PAGE</td></tr><tr><td top=531.55 left=58.78 bottom=536.950000095 right=82.7707989502>Visual</td><td top=531.55 left=119.24 bottom=536.950000095 right=144.760805359>Binary</td><td top=531.55 left=153.61 bottom=536.950000095 right=323.614959106>Whether two mentions are horizontally aligned</td><td top=529.31 left=440.24 bottom=536.94999033 right=494.10000061>bHORZ_ALIGNED</td></tr><tr><td top=542.51 left=58.78 bottom=547.910000095 right=82.7707989502>Visual</td><td top=542.51 left=119.24 bottom=547.910000095 right=144.760805359>Binary</td><td top=542.51 left=153.61 bottom=547.910000095 right=314.253997192>Whether two mentions are vertically aligned</td><td top=544.26 left=440.24 bottom=547.910000095 right=490.449999084>VERT_ALIGNED</td></tr><tr><td top=553.98 left=58.78 bottom=559.380000095 right=82.7707989502>Visual</td><td top=553.98 left=119.24 bottom=559.380000095 right=144.760805359>Binary</td><td top=553.98 left=153.61 bottom=559.380000095 right=412.374038086>Whether two mentions’ left bounding box borders are vertically aligned</td><td top=551.75 left=440.24 bottom=559.379980564 right=515.020014038>VERT_ALIGNED_LEFTb</td></tr><tr><td top=565.45 left=58.78 bottom=570.850000095 right=82.7707989502>Visual</td><td top=565.45 left=119.24 bottom=570.850000095 right=144.760805359>Binary</td><td top=565.45 left=153.61 bottom=570.850000095 right=417.124953613>Whether two mentions’ right bounding box borders are vertically aligned</td><td top=563.22 left=440.24 bottom=570.850041599 right=519.200006714>bVERT_ALIGNED_RIGHT</td></tr><tr><td top=576.93 left=58.78 bottom=582.330000095 right=82.7707989502>Visual</td><td top=576.93 left=119.24 bottom=582.330000095 right=144.760805359>Binary</td><td top=576.93 left=153.61 bottom=582.330000095 right=423.204970703>Whether the center of two mentions’ bounding boxes are vertically aligned</td><td top=574.69 left=440.24 bottom=582.32999033 right=523.390009155>bVERT_ALIGNED_CENTER</td></tr></table></div><div id=16><header><word top=65.15044 left=53.798 bottom=76.2508432 right=236.1656096>SIGMOD’18, June 2018, Houston, Texas USA</word> </header><list><word top=92.6712304 left=53.798 bottom=103.4757424 right=294.047128832>improvements in featurization runtime without adding significant</word> <word top=103.6302304 left=53.798 bottom=114.4347424 right=124.2470048>memory overheads.</word> <word top=125.2042628 left=53.798 bottom=139.3970019 right=69.8561952>C.2</word> <word top=125.2042628 left=80.7652952 bottom=139.3970019 right=180.8562877>Data Representations</word> <word top=145.6642304 left=53.52 bottom=156.4687424 right=295.288629696>The SystemX programming model involves two modes of operation:</word> <word top=156.6232304 left=53.502 bottom=167.4277424 right=294.35564352>(1) development and (2) production. In development, users iteratively</word> <word top=167.5822304 left=53.798 bottom=178.3867424 right=295.161293696>improve the quality of their labeling functions through error analysis,</word> <word top=178.5412304 left=53.475 bottom=189.3457424 right=294.044229312>without executing the full pipeline as in previous techniques such</word> <word top=189.5002304 left=53.798 bottom=200.3047424 right=294.044816256>as incremental KBC [34]. Once labeling functions are finalized, the</word> <word top=199.3563632 left=53.798 bottom=211.3175408 right=230.2028336>SystemX pipeline is only run once in production.</word> <word top=211.4172304 left=63.761 bottom=222.2217424 right=294.042333888>In both modes of operation, SystemX produces two abstract data</word> <word top=222.3762304 left=53.798 bottom=233.1807424 right=294.043745216>structures (Features and Labels as described in Section 3). These</word> <word top=233.3352304 left=53.798 bottom=244.1397424 right=294.046204608>data structures have three access patterns: (1) materialization, where</word> <word top=244.2942304 left=53.798 bottom=255.0987424 right=295.53726336>the data structure is created; (2) updates, which include inserts, dele-</word> <word top=255.2532304 left=53.798 bottom=266.0577424 right=294.041837568>tions, and value changes; and (3) queries, where users can inspect the</word> <word top=266.2122304 left=53.798 bottom=277.0167424 right=294.2678816>features and labels to make informed updates to labeling functions.</word> <word top=277.1712304 left=63.761 bottom=287.9757424 right=294.042458304>Both Features and Labels can be viewed as matrices, where</word> <word top=288.1302304 left=53.798 bottom=298.9347424 right=295.61104832>each row represents annotations for a candidate (see Section 3.2).</word> <word top=296.5338064 left=53.798 bottom=309.6964816 right=295.163307072>Features are dynamically named during multimodal featurization,</word> <word top=310.0482304 left=53.798 bottom=320.8527424 right=294.35531328>but are static for the lifetime of a candidate; Labels are statically</word> <word top=321.0062304 left=53.798 bottom=331.8107424 right=295.61104832>named in in classification, but are updated during development.</word> <word top=331.9652304 left=53.52 bottom=342.7697424 right=166.139022336>Typically Features are sparse:</word> <word top=331.9652304 left=169.248569856 bottom=342.7697424 right=295.166047232>in the ELECTRONICS application,</word> <word top=342.9242304 left=53.798 bottom=353.7287424 right=294.043452608>each candidate has about 100 features while the number of unique</word> <word top=353.8832304 left=53.798 bottom=364.6877424 right=294.04591296>features can be more than 10M. Labels are also sparse, where the</word> <word top=364.8422304 left=53.798 bottom=375.6467424 right=261.140799488>number of unique labels corresponds to the number of</word> <word top=364.8422304 left=264.579593216 bottom=375.6467424 right=294.047128832>labeling</word> <word top=375.8012304 left=53.798 bottom=386.6057424 right=89.4125408>functions.</word> <word top=386.7602304 left=63.761 bottom=397.5647424 right=294.043795904>The data representation that is implemented to store these abstract</word> <word top=397.7192304 left=53.798 bottom=408.5237424 right=294.045335552>data structures can significantly affect overall system runtime. In the</word> <word top=408.6782304 left=54.022 bottom=419.4827424 right=294.200462854>ELECTRONICS application, multimodal featurization accounts for</word> <word top=419.6372304 left=53.798 bottom=430.4417424 right=295.61104832>50% of end-to-end runtime while classification accounts for 15%.</word> <word top=430.5952304 left=53.377 bottom=441.3997424 right=294.04112969>We discuss two common sparse matrix representations that can be</word> <word top=441.5542304 left=53.798 bottom=452.3587424 right=168.0927008>materialized in a SQL database.</word> <word top=453.5826912 left=69.738 bottom=465.2479776 right=295.169490496>• List of lists (LIL): each row stores a list of (column _key,</word> <word top=465.4652304 left=77.983 bottom=476.2697424 right=294.37092448>value) pairs. Zero-valued pairs are omitted. An entire row</word> <word top=476.4242304 left=78.207 bottom=487.2287424 right=294.0461808>can be retrieved in a single query. However, updating values</word> <word top=487.3832304 left=78.207 bottom=498.1877424 right=188.5116528>requires iterating over sublists.</word> <word top=499.4106912 left=69.738 bottom=511.0759776 right=295.165526656>• Coordinate list (COO): rows store (row_key, column_key,</word> <word top=511.2932304 left=77.983 bottom=522.0977424 right=295.166602816>value) triples. Zero-valued triples are omitted. With COO,</word> <word top=522.2522304 left=78.207 bottom=533.0567424 right=295.616735424>multiple queries must be performed to fetch a row’s attributes.</word> <word top=533.2112304 left=78.207 bottom=544.0157424 right=244.0495344>However, updating values takes constant time.</word> <word top=546.1622304 left=63.761 bottom=556.9667424 right=294.045063936>The choice of data representation for Features and Labels reflects</word> <word top=557.1212304 left=53.798 bottom=567.9257424 right=295.61104832>their different access patterns, as well as the mode of operation.</word> <word top=568.0802304 left=53.798 bottom=578.8847424 right=294.355804096>During development, Features are materialized once, but frequently</word> <word top=579.0392304 left=53.798 bottom=589.8437424 right=294.041614106>queried during the iterative KBC process. Labels are updated each</word> <word top=589.9982304 left=53.798 bottom=600.8027424 right=202.55326592>time a user modifies labeling functions.</word> <word top=589.9982304 left=205.745124992 bottom=600.8027424 right=294.04046624>In production, Features</word> <word top=600.9572304 left=53.798 bottom=611.7617424 right=294.045203072>access pattern remains the same. However, Labels are not updated</word> <word top=611.9162304 left=53.798 bottom=622.7207424 right=257.9987936>once the user has finalized their set of labeling functions.</word> <word top=622.8742304 left=63.761 bottom=633.6787424 right=295.53543648>From the access patterns in the SystemX pipeline, and the char-</word> <word top=633.8332304 left=53.798 bottom=644.6377424 right=295.531857075>acteristics of each sparse matrix representation, we find that imple-</word> <word top=644.7922304 left=53.798 bottom=655.5967424 right=294.044696384>menting Features as a LIL minimizes runtime both in production</word> <word top=655.7512304 left=53.798 bottom=666.5557424 right=294.04508809>and development. Labels, however, should be implemented as COO</word> <word top=666.7102304 left=53.798 bottom=677.5147424 right=294.047084>to support fast insertions during iterative KBC and reduce runtimes</word> <word top=677.6692304 left=53.798 bottom=688.4737424 right=294.044772992>foe each iteration. In production, Labels can also be implemented as</word> <word top=688.6282304 left=53.798 bottom=699.4327424 right=295.535910522>LIL to avoid the computation overhead of COO. In the ELECTRON-</word> <word top=701.3608041 left=54.022 bottom=710.0043896 right=294.043869293>ICS application, we find that LIL provides 1.4× speedup over COO</word> </list><list><word top=92.6712304 left=317.955 bottom=103.4757424 right=558.202803776>in production, and COO provides over 5.8× speedup over LIL when</word> <word top=103.6302304 left=317.955 bottom=114.4347424 right=431.0392368>adding a new labeling function.</word> <word top=117.3304374 left=327.918 bottom=129.2257818 right=558.202987648>Takeaways. We find that Labels should be implemented as a</word> <word top=129.3942304 left=317.955 bottom=140.1987424 right=558.356055514>coordinate list during development, which supports fast updates for</word> <word top=140.3532304 left=317.955 bottom=151.1577424 right=558.201045568>supervision, while Features should use a list of lists, which provides</word> <word top=151.3122304 left=317.955 bottom=162.1167424 right=558.204128832>faster query times. In production, both features and labels should</word> <word top=162.2712304 left=317.955 bottom=173.0757424 right=427.5333744>use a list of list representation.</word> <word top=175.8779216 left=317.955 bottom=191.4316368 right=326.5866544>D</word> <word top=175.8779216 left=338.5418544 bottom=191.4316368 right=437.2559408>RELATED WORK</word> <word top=195.1712304 left=317.534 bottom=205.9757424 right=492.9526496>We briefly review prior work in a few categories.</word> <word top=210.9666912 left=317.955 bottom=222.6319776 right=559.694199424>Context Scope Existing KBC systems often restrict relation men-</word> <word top=222.8492304 left=317.955 bottom=233.6537424 right=558.204128832>tions to specific context scopes within documents such as single</word> <word top=233.8082304 left=317.955 bottom=244.6127424 right=558.514920512>sentences [22, 42] or tables [7]. Others perform KBC from richly</word> <word top=244.7672304 left=317.955 bottom=255.5717424 right=558.204084>formatted data by ensembling candidates discovered using separate</word> <word top=255.7262304 left=317.955 bottom=266.5307424 right=558.199561664>extraction tasks [9, 13], which overlooks candidates composed of</word> <word top=266.6852304 left=317.955 bottom=277.4897424 right=558.204128832>mentions that must be found jointly from document-level context</word> <word top=277.6442304 left=317.955 bottom=288.4487424 right=344.1010224>scopes.</word> <word top=293.4396912 left=317.955 bottom=305.1049776 right=559.319789632>Multimodality In unstructured data information extraction systems,</word> <word top=305.3222304 left=317.955 bottom=316.1267424 right=559.68794624>only textual features [25] are utilized. Recognizing the need to repre-</word> <word top=316.2812304 left=317.955 bottom=327.0857424 right=558.20151961>sent layout information as well when working with richly formatted</word> <word top=327.2402304 left=317.955 bottom=338.0447424 right=558.202326586>data, various additional feature libraries have been proposed. Some</word> <word top=338.1982304 left=317.955 bottom=349.0027424 right=559.686705139>have relied predominantly on structural features, usually in the con-</word> <word top=349.1572304 left=317.955 bottom=359.9617424 right=558.5125584>text of web tables [29, 30, 37]. Others have built systems that rely</word> <word top=360.1162304 left=317.955 bottom=370.9207424 right=558.203017088>only on visual information [12, 43]. There have been instances of</word> <word top=371.0752304 left=317.731 bottom=381.8797424 right=559.688801882>visual information being used to supplement a tree-based represen-</word> <word top=382.0342304 left=317.955 bottom=392.8387424 right=558.357402176>tation of a document [8, 19], but these systems were designed for</word> <word top=392.9932304 left=317.955 bottom=403.7977424 right=559.76849664>other tasks, such as document classification and page segmentation.</word> <word top=403.9522304 left=317.955 bottom=414.7567424 right=558.198462067>By utilizing our deep learning based featurization approach, which</word> <word top=414.9112304 left=317.955 bottom=425.7157424 right=558.204018317>supports all of these representations, SystemX obviates the need to</word> <word top=425.8702304 left=317.955 bottom=436.6747424 right=558.204128832>focus on feature engineering and frees the user to iterate over the</word> <word top=436.8292304 left=317.955 bottom=447.6337424 right=496.6822512>supervision and learning stages of the framework.</word> <word top=452.6246912 left=317.955 bottom=464.2899776 right=558.203541696>Supervision Sources Distant supervision is one effective way to</word> <word top=464.5062304 left=317.955 bottom=475.3107424 right=558.19937664>programmatically create training data in machine learning area. In</word> <word top=475.4652304 left=317.955 bottom=486.2697424 right=558.201994829>this paradigm, facts from existing knowledge bases are paired with</word> <word top=486.4242304 left=317.955 bottom=497.2287424 right=558.204128832>unlabeled documents to create noisy or “weakly” labeled training</word> <word top=497.3832304 left=317.955 bottom=508.1877424 right=559.691132147>examples [1, 24, 25, 27]. Besides existing knowledge bases, crowd-</word> <word top=508.3422304 left=317.955 bottom=519.1467424 right=558.198655424>sourcing [11] and heuristics from domain experts [28] have also</word> <word top=519.3012304 left=317.955 bottom=530.1057424 right=558.204128832>proven to be effective weak supervision sources. In our work, we</word> <word top=530.2602304 left=317.955 bottom=541.0647424 right=559.694882496>show that by incorporating all kinds of supervision in one frame-</word> <word top=541.2192304 left=317.632 bottom=552.0237424 right=558.201229312>work in a noise-aware way, we are able to achieve high quality in</word> <word top=552.1782304 left=317.955 bottom=562.9827424 right=558.204128832>knowledge base construction. Furthermore, we empower users to</word> <word top=563.1372304 left=317.955 bottom=573.9417424 right=558.202335552>add supervision based on intuition from any modality of data through</word> <word top=574.0962304 left=317.955 bottom=584.9007424 right=407.8610928>our programming model.</word> </list></div></html>