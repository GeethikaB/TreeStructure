<html><div id=1></div><div id=2><header>SIGMOD’18, June 2018, Houston, Texas USA </header><paragraph>the interpretation of some numerical values depends on their units reported in another table column (e.g., 200 mA). Limiting the context scope to a single sentence or a single table misses many potential relations, up to 97% in the ELECTRONICS application, for example. On the other hand, considering all possible entity pairs throughout the document as candidates renders the ex- traction problem unnecessarily computationally intractable due to the combinatorial explosion of candidates. Multimodality Classical KBC systems usually model input data as unstructured text [22, 25, 34]. With richly formatted data semantics are expressed through multiple modalities–textual, structural, tabular, and visual: </paragraph><paragraph>Example 1.3 (Multimodality). In Figure 1, important information (e.g., the transistor names in the header) is expressed in larger, bold fonts (displayed in yellow). Furthermore, the meaning of a table entry depends on other entries with which it is visually aligned (shown by the red arrow). For instance, the semantics of a numeric value are specified by an aligned unit. Semantics from different modalities can vary significantly but can convey complementary information. Data Variety With richly formatted data, the same information can be presented in many different formats and styles, in addition to linguistic variations. For example, tabular data can be oriented vertically or horizontally and numerical values have many formats: </paragraph><paragraph>Example 1.4 (Data Variety). In Figure 1, numeric intervals are expressed as “-65 . . . 150”, but other datasheets show intervals as “-65 ∼ 150”, or “-65 to 150”. Similarly, some attribute names are synonymous with symbols, like “Collector current” and “Ic”. Data variety requires KBC systems to be generalizable and robust against changing input data. </paragraph><paragraph>Our Approach. We introduce SystemX, a machine-learning based system for KBC from richly formatted data. SystemX takes as input richly formatted documents which may be of diverse formats, including PDF, HTML, and XML. SystemX parses the documents and analyzes the corresponding multimodal, document-level con- texts to extract entity relations. The final output is a knowledge base with entity relations classified to be correct. SystemX’s machine- learning based approach comes with a series of technical challenges. Technical Challenges The challenges in designing SystemX are: (1) Reasoning about entity-relation candidates that are manifested in heterogeneous formats (e.g., text and tables) and span across an entire document requires SystemX’s machine learning model to analyze heterogeneous, document-level contexts. While deep learning models such as recurrent neural networks [2] are effective with sentence- or paragraph-level context [21], they fall short with document-level contexts, such as contexts that span both textual and visual features (e.g., information conveyed via fonts or align- ment) [20]. Developing such models is an open challenge and active area of research [20]. (2) The heterogeneity of contexts in richly formatted data magni- fies the need for large amounts of training data. Manual annotation is prohibitively expensive, especially when domain expertise is re- quired. At the same time, human-curated KBs, which can be used </paragraph><paragraph>to generate training data, may exhibit low coverage or not exist altogether. Alternatively, weak supervision sources can be used to programmatically create large training sets, but it is often unclear how to consistently apply these sources to richly formatted data. Whereas patterns in unstructured data can be identified based on text alone, expressing patterns consistently across different modalities in richly formatted data is challenging. (3) Considering candidates across an entire document leads to a combinatorial explosion of possible candidates, and thus random variables, which need to be considered during learning and inference. This leads to a fundamental tension between building a practical KBC system and learning accurate models that exhibit high recall. In addition, the combinatorial explosion of possible candidates results in a large class imbalance, where the number of “True” candidates is much smaller than the number of “False” candidates. Therefore, techniques to prune candidates to balance running time and end-to- end quality are required. Technical Contributions Our main contributions are as follows: (1) To account for the breadth of signals in richly formatted data, we design a new data model that preserves structural and semantic information across different data modalities. The role of SystemX’s data model is twofold: (1) to allow users to specify multimodal domain knowledge that SystemX leverages to automate the KBC process over richly formatted data, and (2) to provide SystemX’s machine learning model with the necessary representation to reason about document-wide context (see Section 3). (2) We empirically show that existing deep learning models [45] tailored for text information extraction (such as long short-term mem- ory networks (LSTM) [17]) struggle to capture the multimodality of richly formatted data. We introduce a multimodal LSTM network that combines textual context with universal features that correspond to structural and visual properties of the input documents. These features are inherently captured by SystemX’s data model and are generated automatically (see Section 4.2). We also introduce a series of data layout optimizations to ensure the scalability of SystemX to millions of document-wide candidates (see Section 4.4). (3) SystemX introduces a programming model where no develop- ment cycles are spent on feature engineering. Users only need to specify candidates, the potential entries in the target KB, and provide lightweight supervision rules which capture a user’s domain knowl- edge and programmatically label subsets of candidates, which are used for training SystemX’s deep learning model (see Section 4.3). We conduct a user study to evaluate SystemX’s programming model. We find that when working with richly formatted data users uti- lize the semantics from multiple modalities of the data, including both structural and textual information in the document. Our study demonstrates that given 30 minutes, SystemX’s programming model allows users on average to attain F1 scores that are 23.3 points higher than traditional forms of supervision that rely on manually labeling candidates (see Section 6). </paragraph><paragraph>Summary of Results. SystemX-based systems have been put in production in a range of academic and industrial uses cases, includ- ing a major online retailer. SystemX comes with several advance- ments over prior KBC systems (see Appendix D): (1) In contrast to prior systems that focus on adjacent textual data, system can ex- tract document-level entity relations expressed in diverse formats, </paragraph></div><div id=3><paragraph>SystemX: Knowledge Base Construction from Richly Formatted Data </paragraph><paragraph>ranging from textual to tabular formats; (2) SystemX reasons about multimodal context, i.e., both textual and visual characteristics of the input documents, to extract more accurate entity relations; (3) In con- trast to prior KBC systems that rely heavily on feature engineering to achieve high quality [33], SystemX obviates the need for feature engineering by extending a bidirectional LSTM—the de-facto deep learning standard in natural language processing [23]—to obtain a representation needed to automate relation extraction from richly formatted data. We evaluate SystemX in four real-world richly for- matted information extraction applications and show that SystemX enables users to build high-quality KBs, achieving an average im- provement of 41 F1 points over state-of-the-art KBC systems. 2 BACKGROUND We review concepts and terminology used in the next sections. 2.1 Knowledge Base Construction The input to a KBC system is a collection of documents. The output of the system is a relational database containing facts extracted from the input and stored in an appropriate schema. To describe the KBC process, we adopt the standard terminology from the KBC community2. There are four types of objects that play integral roles in KBC systems: (1) entities, (2) relations, (3) mentions of entities, and (4) relation mentions. </paragraph><paragraph>An entity e in a knowledge base corresponds to a distinct real- world person, place, or object. These entities can be grouped into different entity types T1, T2, . . . , Tn. Furthermore, entities partici- pate in relationships. A relationship between n entities is represented as an n-ary relation R(e1, e2, . . . , en) and is described by a schema SR(T1, T2, . . . , Tn) where ei ∈ Ti. A mention m is a span of text that refers to an entity. KBC systems typically assume that all men- tions of entities in a document have a corresponding span of text that refers to them. A relation mention candidate is an n-ary tuple c = (m1, m2, . . . , mn) that represents an instance of a relation R(e1, e2, . . . , en) in a document. A candidate classified as true is called a relation mention, denoted by rR. </paragraph><paragraph>Example 2.1 (KBC). Consider the HasCollectorCurrent task in Figure 1. SystemX takes a corpus of transistor datasheets as input and constructs a KB containing the (transistor part, current) binary relation as output. Parts like SMBT3904 and currents like 200mA are entities. The spans of text that read “SMBT3904” and “200” (boxed in blue and green, respectively) are mentions of those two entities, and together they form a candidate. If the evidence in the document suggests that these two mentions are related, then the output KB will include the relation mention (SMBT3904, 200mA) of the HasCollectorCurrent relation. </paragraph><section_header>The KBC problem is defined as follows: </section_header><paragraph>Definition 2.2 (Knowledge Base Construction). Given a set of documents D and a KB schema SR(T1, T2, . . . , Tn), where each Ti corresponds to an entity type, extract a set of relations rR from D, which populate the schema’s relational tables. </paragraph><paragraph>Like other machine-learning based KBC systems [7, 34], SystemX converts KBC to a statistical learning and inference problem: each 2http://www.itl.nist.gov/iad/mig/tests/ace/ </paragraph><section_header>SIGMOD’18, June 2018, Houston, Texas USA </section_header><paragraph>candidate is assigned a Boolean random variable that can take the value “True” if the corresponding relation mention is correct, or “False” otherwise. In machine-learning based KBC systems, each candidate is associated with certain features that provide evidence on the value that the corresponding random variable should take. Machine-learning based KBC systems use machine learning to max- imize the probability of correctly classifying candidates, given their features and ground truth examples. 2.2 Recurrent Neural Networks The machine learning model we use in SystemX is based on a recurrent neural network (RNN). RNNs have obtained state-of-the- art results in many NLP tasks, including information extraction [14, 15, 41]. RNNs take sequential data as input. For each element in the input sequence, the information from previous inputs can affect the network output for the current element. For sequential data {x1, ..., xT }, the structure of an RNN is mathematically described as: </paragraph><section_header>ht = f(xt, ht−1), y = g({h1, ..., hT }) </section_header><paragraph>where ht is the hidden state for element t, and y is the representation generated by considering the sequence of hidden states {h1, ..., hT }. Functions f and g correspond to nonlinear transformations. In typical RNNs, f = tanh(Whxt + Uhht−1 + bh) where Wh, Uh, and bh are parameter metrics and a vector, and g is chosen based on the task in hand. </paragraph><paragraph>Long Short-term Memory (LSTM) LSTM [17] networks are a special type of RNN which introduce new structures referred to as gates, which control the flow of information which can capture long- term dependencies. There are three types of gates: “input” gates it control which values are updated in a memory cell; “forget” gates ft control which values remain in memory; and “output” gates ot control which values in memory are used to compute the output of the cell. The final structure of an LSTM is given by: </paragraph><paragraph>it = σ(Wixt + Uiht−1 + bi) ft = σ(Wfxt + Ufht−1 + bf) ot = σ(Woxt + Uoht−1 + bo) ct = ft ◦ ct−1 + it ◦ tanh(Wcxt + Ucht−1 + bc) ht = ot ◦ tanh(ct) </paragraph><paragraph>where ct is the cell state vector, W, U, b are parameter metrics and a vector, σ is the sigmoid function, and ◦ is the Hadamard product. </paragraph><paragraph>Bidirectional LSTMs consist of forward and backward LSTMs. The forward LSTM fF reads the sequence from x1 to xT and calcu- lates a sequence of forward hidden states (hF 1 , ..., hF T ). The backward LSTM fB reads the sequence from xT to x1 and calculates a se- quence of backward hidden states (hB 1 , ..., hB T ). The final hidden state for the sequential is the concatenation the forward and back- ward hidden states, e.g., hi = [hF i , hB i ]. </paragraph><paragraph>Attention Previous work explored using pooling strategies to train the model, such as max pooling [39], which makes it difficult for RNNs to capture all the information contained in a sequence. In order to benefit by giving more attention to all parts of the sequence, the attention mechanism is proposed to capture the information of whole sequence [44]. SystemX uses a bidirectional LSTM with attention to automatically represent textual features of relation candidates from </paragraph></div><div id=4><figure bbox=61.8190025583,101.6316585,245.149002558,590.5116585></figure><section_header>SIGMOD’18, June 2018, Houston, Texas USA </section_header><figure bbox=242.728992,49.216824,679.803048,388.29276></figure><paragraph>SystemX’s data model is a directed acyclic graph (DAG) that contains a hierarchy of contexts, whose structure reflects the intuitive hierarchy of document components. In this graph, each node is a context (represented as boxes in Figure 3). The root of the DAG </paragraph><figure bbox=662.131656,356.757624,677.752776,376.284024></figure><figure bbox=110.9673341,450.414607,124.62416295,464.07143585></figure><figure_caption>Figure 2: An overview of SystemX KBC over richly formatted data. Given a set of richly formatted documents and a series of lightweight inputs from the user, SystemX extracts facts and stores them in a relational database. </figure_caption><section_header>multiple common types of contexts, and multiple modality semantics into a single model. SystemX’s data model serves as the formal </section_header><section_header>3https://poppler.freedesktop.org/ </section_header></div><div id=5><paragraph>SystemX: Knowledge Base Construction from Richly Formatted Data </paragraph><paragraph>representation of the intermediate data utilized in all future stages of the extraction process. 3.2 User Inputs and SystemX’s Pipeline The SystemX processing pipeline follows three phases. We briefly describe each phase in turn and focus on the user inputs required by each phase. SystemX’s internals are described in Section 4. </paragraph><paragraph>(1) KBC Initialization The first phase in SystemX’s pipeline is to initialize the target KB where the extracted relations will be stored. During this phase, SystemX requires that the user specifies a target schema that corresponds to the relations to be extracted. The target schema SR(T1, . . . , Tn) defines a relation R to be extracted from the input documents. An example of such a schema is provided below. </paragraph><paragraph>Example 3.2 (Relation Schema). An example SQL schema for the relation in Figure 1 is: </paragraph><paragraph>Throttlers Users can optionally provide throttlers, which act as hard filtering rules to reduce the number of candidates that are materialized. Throttlers are also Python functions, but rather than accepting spans of text as input, they operate on candidates, and output whether or not a candidate meets the specified condition. Throttlers limit the number of candidates considered by SystemX. </paragraph><section_header>SIGMOD’18, June 2018, Houston, Texas USA </section_header><paragraph>Example 3.4 (Throttler). Continuing the example shown in Fig- ure 1, the user provides a throttler which only keeps currents that have “Value” as a column header. </paragraph><paragraph>Given the input matchers and throttlers, SystemX extracts rela- tion candidates by traversing its data model representation of each document. By applying matchers to each leaf of the data model, SystemX can generate sets of mentions for each component of the schema. The cross-product of these mentions produces candidates: </paragraph><section_header>Candidate(idcandidate, mention1, . . . , mentionn) </section_header><paragraph>where mentions are spans of text and contain pointers to their context in the data model of their respective document. The output of this phase is a set of candidates C. </paragraph><paragraph>(3) Training a multimodal LSTM for KBC In this phase, SystemX trains a multimodal long short-term memory network (LSTM) to classify the candidates generated during Phase 1 as “True” or “False” mentions of target relations. SystemX’s multimodal LSTM combines both visual and textual features. Recent work has also proposed the use of LSTMs for KBC but have focused only on textual data [45]. In Section 5.3.3 we experimentally demonstrate that state-of-the-art LSTMs struggle to capture the multimodal characteristics of richly formatted data which results in poor-quality KBs (see Section 5.3.3). SystemX uses a bidirectional LSTM (reviewed in Section 2.2) to capture textual features and extends it with additional structural, tab- ular and visual features captured by SystemX’s data model. The full LSTM used by SystemX is described in Section 4.2. The training phase in SystemX is split in two sub-phases: (1) a multimodal fea- turization phase, and (2) a phase where supervision data is collected by the user. We describe each in turn: </paragraph><paragraph>Multimodal Featurization Here, SystemX traverses its internal data model instance for each input document and automatically generates features that correspond to structural, tabular, and visual modalities as described Section 4.2. These features provide an extended feature library (shown as feature_lib, below), which augments the textual features learned by the LSTM. All generated features are stored in a relation: </paragraph><section_header>Features(idcandidate, LSTMtextual, feature_libothers) </section_header><paragraph>A detailed list of the features extracted during this phase is provided in Appendix B. No user input is required during this step. SystemX’s LSTM obviates the need for feature engineering. </paragraph><paragraph>Supervision To train its multimodal LSTM, SystemX requires that users provide some form of supervision. Collecting sufficient train- ing data for multi-context deep learning models is a well-established challenge. Taking into account a context of more than a handful of words for text-based deep learning models requires very large training corpora, as stated by LeCun et al. [20]. </paragraph><paragraph>To soften the burden of traditional supervision, SystemX uses a supervision paradigm referred to as data programming [31]. Data </paragraph></div><div id=6><header>SIGMOD’18, June 2018, Houston, Texas USA </header><paragraph>programming is a human-in-the-loop paradigm for training machine learning systems. In data programming users only need to specify lightweight functions, referred to as labeling functions (LFs), that programmatically assign labels to the input data. A detailed overview of data programming is provided in Appendix A. </paragraph><paragraph>As a result, SystemX requires that users specify a set of labeling functions that noisily label the candidates from Phase 2. Labeling functions in SystemX correspond to Python functions that take a candidate as input and assign +1 to label a candidate as “True”, 0 to abstain, or −1 to label a candidate as “False”. An example labeling function is shown below: </paragraph><paragraph>Example 3.5 (Labeling Functions). Looking at the datasheet in Figure 1, users can express patterns such as having the Part and Current y-aligned on the visual rendering of the page. Similarly, users can write a rule that labels a candidate as true if it has the word “current” in the same row as the candidate current mention. </paragraph><paragraph>As shown in Example 3.5, SystemX’s internal data model allows users to specify labeling functions that capture supervision patterns across any modality of the data (see Section 4.3). In our user-study we find that it is common for users to write labeling functions that span multiple modalities and consider both textual and visual pat- terns of the input data (see Section 6). </paragraph><paragraph>The user-specified labeling functions together with the candidates generated by SystemX are passed as input to Snorkel [32], a data programming engine, which converts the noisy labels generated by the input labeling functions to denoised labeled data used to train SystemX’s multimodal LSTM model (see Appendix A). </paragraph><paragraph>Classification SystemX uses its trained LSTM to assign a marginal probability to each candidate. The last layer of SystemX’s LSTM is a softmax classifier (described in Section 4.2) that computes the probability of a candidate being a “True” entity relation. In SystemX, users can specify a threshold over the output marginal probabilities to determine which candidates will be satisfied as “True” (those whose marginal probability of being true exceeds the specified threshold) and which are “False” (those whose marginal probability fall beneath the threshold). This threshold depends on the requirements of the application. Applications that require critically high accuracy can set a high threshold value to ensure only candidates with a high probability of being true are classified as such. </paragraph><paragraph>As shown in Figure 2, supervision and classification are typically executed for several iterations as users develop a KBC application. This feedback loop allows users to quickly receive feedback and im- prove their labeling functions, and avoids the overhead of rerunning candidate extraction and materializing features (see Section 6). 3.3 SystemX’s Programming Model for KBC Traditionally, machine-learned based KBC focuses on feature engi- neering to obtain high quality KBs. This requires that users rerun </paragraph><paragraph>feature extraction, learning and inference after every modification of the features used during KBC. With SystemX’s machine learning ap- proach features are generated automatically. This puts emphasis on (1) specifying the relation candidates, and (2) providing supervision rules via data programming and labeling functions. SystemX’s programming paradigm obviates the need for feature engineering and introduces two modes of operation for SystemX applications: (1) development, and (2) production. During develop- ment, labeling functions are iteratively improved. LFs are applied to a small sample of labeled candidates and evaluated by the user on their accuracy and coverage (the fraction of candidates receiving non-zero labels). In practice, approximately 20 iterations are suffi- cient for our users to generate a sufficiently tuned set of labeling functions (see Section 6). In production, the finalized LFs are ap- plied to the entire set of candidates and learning and inference are performed only once to generate the final KB. </paragraph><paragraph>On average, only a small number of LFs are needed to achieve high quality KBC (see Section 6). For example, in the ELECTRON- to ICS application, 16 labeling functions on average is sufficient achieve an average F1 score of over 75 F1 points. Finally, in contrast to the common belief that users rely mostly on textual signals for supervising the KBC process, we find that tabular and visual pat- terns are valuable forms of weak supervision for KBC over richly formatted data (see Section 6). </paragraph><paragraph>4 KBC IN SystemX We now review each component of SystemX in detail. </paragraph><paragraph>4.1 Candidate Generation from RF Data Candidate generation from richly formatted data relies on access to document-level contexts, which is provided by SystemX’s data model. Due to the significantly increased context needed for KBC from richly formatted data, naïvely materializing all possible can- didates is intractable as the number of candidates grows combina- torially with the number of relation arguments. This combinatorial explosion can lead to performance issues for KBC systems. For example, in the ELECTRONICS domain, just 100 documents can gen- erate over 1M candidates. In addition, we find that the majority of these candidates do not express true relations, creating a significant class imbalance that can hinder learning performance [18]. </paragraph><paragraph>To address this combinatorial explosion, SystemX allows users to specify throttlers, in addition to matchers, to prune away excess candidates. We find that throttlers must: </paragraph><paragraph>• keep high accuracy by only filtering negative candidates. • seek high coverage of the candidates. </paragraph><paragraph>and can be viewed as a knob that allows users to tradeoff preci- sion and recall and promote scalability by reducing the number of candidates to be classified during KBC. </paragraph><paragraph>Figure 4 shows how using throttlers affects the quality-performance tradeoff in the ELECTRONICS domain. We see that throttling signifi- cantly improves system performance. However, increased throttling does not monotonically improve quality as it hurts recall. This trade- off captures the fundamental tension between optimizing for system performance and optimizing for end-to-end quality. When no candi- dates are pruned, the class imbalance resulting from many negative candidates to the relatively small number of positive candidates </paragraph></div><div id=7><paragraph>SystemX: Knowledge Base Construction from Richly Formatted Data </paragraph><figure bbox=89.807223988,51.857677616,196.997096378,290.42082644></figure><figure bbox=153.14086636,78.60278,156.74446636,82.20638></figure><figure bbox=109.690099,78.60278,113.293699,82.20638></figure><figure bbox=133.62196696,113.248391,137.22556696,116.851991></figure><figure bbox=110.02955812,113.248391,113.63315812,116.851991></figure><figure bbox=123.2684638,144.3042158,126.8720638,147.9078158></figure><figure bbox=111.27424156,144.3042158,114.87784156,147.9078158></figure><figure bbox=120.4396378,151.650755,124.0432378,155.254355></figure><figure bbox=117.72396484,151.650755,121.32756484,155.254355></figure><figure bbox=113.14126672,156.4093088,116.74486672,160.0129088></figure><figure bbox=128.30377408,156.4093088,131.90737408,160.0129088></figure><figure bbox=105.44686,161.0008958,109.05046,164.6044958></figure><figure bbox=147.20033176,161.0008958,150.80393176,164.6044958></figure><paragraph>Figure 4: Tradeoff between (a) quality and (b) execution time when pruning the number of candidates using throttlers. </paragraph><paragraph>harms quality. Thus, as a rule of thumb, we recommend that users use throttlers to balance negative and positive candidates. SystemX provides users with mechanisms to evaluate this balance over a small holdout development labeled set of candidates. </paragraph><paragraph>Takeaways. SystemX’s data model is necessary to perform can- didate generation with richly formatted data. Pruning negative can- didates via throttlers to balance negative and positive candidates not only ensures the scalability of SystemX but also improves the precision of SystemX’s output. 4.2 Multimodal LSTM for Richly Formatted Data We now describe SystemX’s deep learning model in detail. SystemX’s model extends a bidirectional LSTM (Bi-LSTM), the de-facto deep learning standard for natural language processing [23], with addi- tional features (the extended feature library) to capture not only textual features but also features from other data modalities such as structural, tabular, and visual semantics. In Section 5.3.2 we per- form an ablation study demonstrating that features from non-textual modes are key to obtaining high-quality KBs. We find that the quality of the output KB can deteriorate up to 33 F1 points when non-textual features are removed. Figure 5 illustrates SystemX’s LSTM. We now review each component of SystemX’s LSTM. </paragraph><paragraph>Bidirectional LSTM with Attention Traditionally, the primary source of signal for relation extraction comes from unstructured text. In order to understand textual signals, SystemX uses a long short-term memory network (LSTM) to extract textual features. For mention candidates, SystemX builds a Bi-LSTM to get the textual features of the mention candidate from the both directions of sentences containing the candidate. For sentence si containing the ith mention candidate in the document, the textual features hik of each word wik is encoded by both forward (defined as superscript F in equations) and backward (defined as superscript B) LSTM, which summarizes information about the whole sentence with a focus on wik. This takes the structure: </paragraph><paragraph>hF ik = LST M(hF i(k−1), Φ(si, k)) hB ik = LST M(hB i(k+1), Φ(si, k)) hik = [hF ik, hB ik] </paragraph><paragraph>where Φ(si, k) is the word embedding [38] which is the representa- tion of the semantics of the kth word in sentence si. </paragraph><section_header>SIGMOD’18, June 2018, Houston, Texas USA </section_header><paragraph>Then, the textual feature representation for mention candidate ti is calculated by an attention mechanism [44] to model the im- portance of different words from the sentence si and aggregate the feature representation of those informative words to form a final feature representation, </paragraph><section_header>uik = tanh(Wwhik + bw) </section_header><paragraph>where uik is the hidden representation of hik, and αik is to model the importance of each word in the sentence si. Special candidate markers (shown in red in Figure 5) are added to the sentences to draw emphasized attention to the candidates themselves. Finally, the textual features of candidate relations is the concatenation of its mentions’ textual features [t1, ..., tn]. </paragraph><paragraph>Extended Feature Library Features for structural, tabular, visual modalities are generated by leveraging the data model, which pre- serves each modality’s semantics. For each candidate, such as the candidate (SMBT3904, 200) shown in Figure 5, SystemX locates each mention in the data model and traverses the directed acyclic graph to compute features from the modality information stored in the nodes of the graph. For example, SystemX can traverse sibling nodes to add tabular features such as featurizing a node based on the other mentions in the same row or column. Similarly, SystemX can traverse the data model to extract structural features from tags stored while parsing the document along with the hierarchy of the document elements themselves. We review each modality: Structural features. These provide signals intrinsic to a docu- ment’s structure. These features allow SystemX to learn from struc- tural attributes such as parent and sibling relationships and a candi- date’s associated XML/HTML tags. For example, structural features capture tag attributes such as font sizes and styles, shown in yellow in Figure 5, which are often used to highlight important keywords or information. SystemX extracts many structural features from the par- ents and siblings of mentions inside the data model of the document. SystemX also tracks the structural distance of relations, which helps when candidate mentions are visually distant, but structurally close together. As an example, featurizing a candidate with the lowest common ancestor in the data model is a positive signal for linking table captions to table contents. Tabular features. These are a special subset of structural features since tables are very common structures inside documents and have high information density. Table features are drawn from the grid- like representation of rows and columns stored in the data model, shown in green in Figure 5. In addition to the tabular location of mentions, SystemX also featurizes relations with special signals such as being in the same row or column. Consider, for example, a table that contains cells with multiple lines of text; recording that two entity mentions are in the same row captures a signal that a visual alignment feature could easily miss. Visual features. These provide signals observed from a visual rendering of a document. In cases where tabular or structural features are noisy—including nearly all documents converted from PDF to </paragraph></div><div id=8><figure bbox=-28.3459720652,-169.33944385,338.427427935,808.72295615></figure><paragraph>SystemX’s data model allows users to directly express correctness using textual, structural, tabular, or visual characteristics, in addition to traditional supervision sources like existing KBs. In the ELEC- TRONICS domain, over 70% of labeling functions written by our users are based on non-textual signals. It is acceptable for these labeling functions to be noisy and conflict with one another. Data programming theory (see Appendix A.2) shows that with a sufficient number of labeling functions, data programming can still achieve quality comparable to using labeled data. </paragraph><paragraph>In Section 5.3.4, we find that using metadata, such as structural, tabular, and visual cues, results in an increase of 66 F1 points over using textual supervision sources alone in the ELECTRONICS domain. Using both sources gives a further increase of 2 F1 points over the 66 F1 improvement of metadata alone. We also show that supervision using information from all modalities, rather than textual information alone, results in an increase of 43 F1 points on average over a </paragraph><paragraph>the relation candidate (see Appendix B). Since each entity mention can be associated with many different relation candidates, we cache the featurization of each entity mention. Caching during featurization results in a 100× speed up on average in the ELECTRONICS domain, and only accounts for 10% of featurization’s memory usage. </paragraph><paragraph>Recall from Section 3.3 that SystemX’s programming model in- troduces two modes of operation: (1) development, where users iteratively improve the quality of labeling functions without execut- ing the entire pipeline, and (2) production, where the full pipeline is executed once to produce the knowledge base. We use different data representations to implement the abstract data structures of Features and Labels (a relation that stores the output of labeling functions after applying them over the generated candidates). Implementing Features as a list-of-lists structure minimizes runtime in both modes of operation since it accounts for sparsity. We also find that Labels implemented as a coordinate list during the development mode are </paragraph></div><div id=9><paragraph>5 EXPERIMENTS We evaluate SystemX over four applications: ELECTRONICS, AD- VERTISEMENTS, PALEONTOLOGY, and GENOMICS–each contain- ing several relation extraction tasks. We seek to answer: (1) how does SystemX compare against both state-of-the-art KBC techniques and manually curated knowledge bases? (2) how does modeling context scopes and multimodality impact quality? and (3) how does scaling training set size affect SystemX’s performance? </paragraph><section_header>5.1 </section_header><paragraph>SystemX: Knowledge Base Construction from Richly Formatted Data </paragraph><paragraph>optimal for fast updates. A list-of-lists implementation is used for Labels in production mode. </paragraph><section_header>Experimental Settings </section_header><paragraph>Datasets. The datasets used for evaluation vary in size and for- mat Table 1 shows the statistics of these datasets. </paragraph><paragraph>Electronics The ELECTRONICS dataset is a collection of single bipolar transistor specification datasheets from over 20 manufac- turers, downloaded from Digi-Key4, a prominent website in the electronics distribution industry. These documents consist primarily of tables and often express relations via domain-specific symbols. In this application, we extract relations between transistor part numbers and several of their electrical characteristics. We use this dataset to evaluate the effectiveness of SystemX on datasets that consist primarily of tables and numerical data. </paragraph><paragraph>Advertisements The ADVERTISEMENTS dataset contains web- pages that may contain evidence of human trafficking activity. The information in the corresponding data includes prices of services, locations, contact information, physical characteristics of the vic- tims, etc. Here, we extract all attributes associated with a trafficking advertisement. The output is deployed in production and is used by law enforcement agencies. This is a very heterogeneous dataset with millions of webpages over 100s of web domains and 1000s of unique layouts. We use this dataset to examine how robust SystemX is in the presence of extreme data variety. </paragraph><paragraph>Paleontology The PALEONTOLOGY dataset is a collection of well- curated paleontology journal articles on fossils and ancient organ- isms. Here, we extract relations between paleontological formations and their corresponding physical measurements. These papers often have tables that span multiple pages. Thus, achieving high quality in this application requires linking content in tables to the text that references it, which can be separated by 20 pages or more in the document. We use this dataset to test SystemX’s ability to draw candidates from document-level contexts. </paragraph><paragraph>Genomics The GENOMICS dataset is a collection of open-access biomedical papers on gene-wide association studies (GWAS) from the manually curated GWAS Catalog [40]. Here, we extract relations between single-nucleotide polymorphisms and human phenotypes found to be statistically significant. This dataset is published in XML format, thus, we do not have visual representations. We use this dataset to evaluate how well the SystemX framework extracts relations from data that is published natively in a tree-based format. </paragraph><section_header>4http://www.digikey.com </section_header><section_header>SIGMOD’18, June 2018, Houston, Texas USA </section_header><table_caption>Table 1: Statistics of the datasets used in our experiments. </table_caption><table><tr><td>Dataset</td><td>Size</td><td>#Docs</td><td>#Rels</td><td>Format</td></tr><tr><td>ELEC.</td><td>3GB</td><td>7K</td><td>4</td><td>PDF</td></tr><tr><td>ADS.</td><td>52GB</td><td>9.3M</td><td>4</td><td>HTML</td></tr><tr><td>PALEO.</td><td>95GB</td><td>0.3M</td><td>10</td><td>PDF</td></tr><tr><td>GEN.</td><td>1.8GB</td><td>589</td><td>4</td><td>XML</td></tr></table><table><tr><td>Dataset</td><td>Size</td><td>#Docs</td><td>#Rels</td><td>Format</td></tr><tr><td>ELEC.</td><td>3GB</td><td>7K</td><td>4</td><td>PDF</td></tr><tr><td>ADS.</td><td>52GB</td><td>9.3M</td><td>4</td><td>HTML</td></tr><tr><td>PALEO.</td><td>95GB</td><td>0.3M</td><td>10</td><td>PDF</td></tr><tr><td>GEN.</td><td>1.8GB</td><td>589</td><td>4</td><td>XML</td></tr></table><paragraph>Comparison Methods. We use two different methods to evalu- ate the quality of SystemX’s output: the upper bound of state-of-the- art KBC systems (Oracle) and manually curated knowledge bases (Existing Knowledge Bases). </paragraph><paragraph>Oracle While SystemX uses a data model to extract information from all modalities together, other state-of-the-art systems do not. For comparison, we approximate the upper bound of quality of three state-of-the-art information extraction techniques by assuming perfect precision in their extracted relations and experimentally measure their recall. </paragraph><section_header>• Text: For extraction from text, we follow [22, 34]. Candi- </section_header><section_header>dates are extracted from individual sentences, which are pre- </section_header><section_header>processed with standard NLP tools to add part-of-speech tags, </section_header><paragraph>linguistic parsing information, etc. • Table: For tables, we follow [3]. Candidates are drawn from </paragraph><paragraph>individual tables, utilizing its contents and structure. • Ensemble: We also implement an ensemble, as proposed </paragraph><section_header>in [9]. Candidates resulting form both the Text and Table </section_header><section_header>approaches are merged into a combined candidate set. </section_header><paragraph>Existing Knowledge Base We use existing knowledge bases as another comparison method. The ELECTRONICS application is com- pared against the transistor specifications published by Digi-Key, while GENOMICS is compared to the both GWAS Central [4] and GWAS Catalog [40], which are the most comprehensive collection of GWAS data and widely-used public datasets. Knowledge bases such as these are constructed using a combination of manual entry, web aggregation, paid third-party services, and automation tools. </paragraph><paragraph>SystemX Details. SystemX is implemented in Python, with database operations being handled by PostgreSQL. All experiments are executed in Jupyter Notebooks on a machine with four CPUs (each CPU is a 14-core 2.40 GHz Xeon E5–4657L), 1 TB RAM, and 12×3TB hard drives, with the Ubuntu 14.04 operating system. 5.2 Experimental Results </paragraph><paragraph>5.2.1 Oracle Comparison We compare the end-to-end qual- ity of SystemX to the upper bound of state-of-the-art systems. In Table 2, we see that SystemX outperforms the these upper bounds on each dataset. In ELECTRONICS, we see that SystemX improves a significant 71 F1 points over a text-only approach. In contrast, ADVERTISEMENTS has a higher upper bound with text than tables, which reflects how advertisements rely more on text than the largely numerical tables found in ELECTRONICS. In the PALEONTOLOGY dataset, which depends on linking references from text to tables, the unified approach of SystemX results in an increase of 43 F1 points over the ensemble baseline. In GENOMICS, all candidates are cross-context, preventing both the text-only and the table-only approaches from finding any valid candidates. </paragraph></div><div id=10><header>SIGMOD’18, June 2018, Houston, Texas USA </header><table_caption>Table 2: End-to-end quality in terms of precision, recall, and F1 score for each applications compared to the upper bound state- of-the-art systems. </table_caption><section_header>5.3 </section_header><table><tr><td>Sys.</td><td>Metric</td><td>Text</td><td>Table</td><td>Ensemble</td><td>SystemX</td></tr><tr><td></td><td>Prec.</td><td>1.00</td><td>1.00</td><td>1.00</td><td>0.73</td></tr><tr><td>ELEC.</td><td>Rec.</td><td>0.03</td><td>0.20</td><td>0.21</td><td>0.81</td></tr><tr><td></td><td>F1</td><td>0.06</td><td>0.40</td><td>0.42</td><td>0.77</td></tr><tr><td></td><td>Prec.</td><td>1.00</td><td>1.00</td><td>1.00</td><td>0.87</td></tr><tr><td>ADS.</td><td>Rec.</td><td>0.44</td><td>0.37</td><td>0.76</td><td>0.89</td></tr><tr><td></td><td>F1</td><td>0.61</td><td>0.54</td><td>0.86</td><td>0.88</td></tr><tr><td></td><td>Prec.</td><td>0.00</td><td>1.00</td><td>1.00</td><td>0.72</td></tr><tr><td>ALEO.</td><td>Rec.</td><td>0.00</td><td>0.04</td><td>0.04</td><td>0.38</td></tr><tr><td></td><td>F1</td><td>0.00*</td><td>0.08</td><td>0.08</td><td>0.51</td></tr><tr><td></td><td>Prec.</td><td>0.00</td><td>0.00</td><td>0.00</td><td>0.89</td></tr><tr><td>GEN.</td><td>Rec.</td><td>0.00</td><td>0.00</td><td>0.00</td><td>0.81</td></tr><tr><td></td><td></td><td>#</td><td>#</td><td>#</td><td></td></tr><tr><td></td><td>F1</td><td>0.00</td><td>0.00</td><td>0.00</td><td>0.85</td></tr></table><table><tr><td>Sys.</td><td>Metric</td></tr><tr><td></td><td>Prec.</td></tr><tr><td>ELEC.</td><td>Rec.</td></tr><tr><td></td><td>F1</td></tr><tr><td></td><td>Prec.</td></tr><tr><td>ADS.</td><td>Rec.</td></tr><tr><td></td><td>F1</td></tr><tr><td></td><td>Prec.</td></tr><tr><td>PALEO.</td><td>Rec.</td></tr><tr><td></td><td>F1</td></tr><tr><td></td><td>Prec.</td></tr><tr><td>GEN.</td><td>Rec.</td></tr><tr><td></td><td>F1</td></tr></table><table><tr><td>System</td><td>ELEC.</td><td>GEN.</td></tr><tr><td></td><td></td><td>GWAS GWAS</td></tr><tr><td>Knowledge Base</td><td>Digi-Key</td><td>Central Catalog</td></tr><tr><td># Entries in KB</td><td>376</td><td>3,008 4,023</td></tr><tr><td># Entries in SystemX</td><td>447</td><td>6,420 6,420</td></tr><tr><td>Coverage</td><td>0.99</td><td>0.82 0.80</td></tr><tr><td>Accuracy</td><td>0.87</td><td>0.87 0.89</td></tr><tr><td># New Correct Entries</td><td>17</td><td>3,154 2,486</td></tr><tr><td>Increase in Correct Entries</td><td>1.05×</td><td>1.87× 1.42×</td></tr></table><paragraph>5.2.2 Existing Knowledge Base Comparison We now com- pare SystemX against existing knowledge bases for ELECTRONICS and GENOMICS. No manually curated KBs are available for the other two datasets. In Table 3, we find that SystemX achieves high cover- age of the manual knowledge bases, while also correctly extracting novel relation entries with over 85% accuracy in both applications. In ELECTRONICS, SystemX achieved 99% coverage and extracted an additional 17 correct entries not found in Digi-Key’s catalog. In the GENOMICS application, we see that SystemX provides over 80% coverage of both existing KBs and finds 1.87× and 1.42× more correct entries than GWAS Central and GWAS Catalog, respectively. </paragraph><paragraph>Takeaways. SystemX achieves over 41 F1 points higher quality on average when compared against the upper bound of state-of-the- art approaches. Furthermore, SystemX attains over 80% of existing public knowledge bases while providing up to 1.87× the number of correct entries with high accuracy. </paragraph><section_header>Inclusion Studies </section_header><paragraph>We conducted inclusion studies to assess the effect of context scope, multimodal features, and multimodal supervisions on the quality of SystemX. In each study, we change one component of SystemX and hold the others constant and report the F1 score. </paragraph><paragraph>5.3.1 Context Scope Study To evaluate the importance of addressing the non-local nature of candidates in richly formatted data, we analyze how the different context scopes contribute to end- to-end quality. We limit the extracted candidates to four levels of context scope in ELECTRONICS and report the average F1 score for each. Figure 6 shows that increasing context scope can significantly improve the F1 score. Considering document context gives an addi- tional 71 F1 points (12.8×) over sentence contexts and 47 F1 points </paragraph><figure bbox=89.757551366,345.916596664,180.574143432,517.391236></figure><figure_caption>Figure 6: Average F1 score over four relations when broadening the extraction context scope in ELECTRONICS. </figure_caption><figure bbox=228.433936,316.014677616,392.59458265,548.22504></figure><figure_caption>Figure 7: The impact of each modality in the feature library. </figure_caption><paragraph>(2.6×) over table contexts. The positive correlation between quality and context scope matches our expectations since larger context scope is required to form candidates jointly from both table content and surrounding text. We see a smaller increase of 11 F1 points (1.2×) in quality between page and document contexts since many of the ELECTRONICS relation mentions are presented on the first page of the document. </paragraph><paragraph>Takeaways. Semantics can be distributed in a document or im- plied in its structure which requires larger context scope than the traditional sentence-level contexts used in previous KBC systems. </paragraph><paragraph>5.3.2 Feature Ablation Study We evaluate our multimodal feature library, which contains textual, structural, tabular, and vi- sual features. We analyze how different features benefit information extraction from richly formatted data by comparing the effects of disabling one feature type, while leaving all other types enabled and report the average F1 scores of each configuration in Figure 7. </paragraph><paragraph>We find that removing a single feature set resulted in drops from 2 F1 points (no textual features in PALEONTOLOGY) to 33 F1 points (no textual features in ADVERTISEMENTS). While it is clear in Figure 7 that each application depends on different feature types, we find that it is necessary to incorporate all feature types to achieve the highest extraction quality. </paragraph><paragraph>The characteristics of each dataset affect how valuable each fea- ture type is to relation classification. The ADVERTISEMENTS dataset consists of webpages which often use tables to format and organize information–many relations can be found within the same cell or </paragraph></div><div id=11><paragraph>SystemX: Knowledge Base Construction from Richly Formatted Data </paragraph><table_caption>Table 4: Comparing approaches to featurization based on SystemX’s data model. </table_caption><table><tr><td>Sys.</td><td>Metric</td><td>Human-tuned</td><td>Bi-LSTM w/ Attn.</td><td>SystemX</td></tr><tr><td></td><td>Prec.</td><td>0.71</td><td>0.42</td><td>0.73</td></tr><tr><td>ELEC.</td><td>Rec.</td><td>0.82</td><td>0.50</td><td>0.81</td></tr><tr><td></td><td>F1</td><td>0.76</td><td>0.45</td><td>0.77</td></tr><tr><td></td><td>Prec.</td><td>0.88</td><td>0.51</td><td>0.87</td></tr><tr><td>ADS.</td><td>Rec.</td><td>0.88</td><td>0.43</td><td>0.89</td></tr><tr><td></td><td>F1</td><td>0.88</td><td>0.47</td><td>0.88</td></tr><tr><td></td><td>Prec.</td><td>0.92</td><td>0.52</td><td>0.76</td></tr><tr><td>PALEO.</td><td>Rec.</td><td>0.37</td><td>0.15</td><td>0.38</td></tr><tr><td></td><td>F1</td><td>0.53</td><td>0.23</td><td>0.51</td></tr><tr><td></td><td>Prec.</td><td>0.92</td><td>0.66</td><td>0.89</td></tr><tr><td>GEN.</td><td>Rec.</td><td>0.82</td><td>0.41</td><td>0.81</td></tr><tr><td></td><td>F1</td><td>0.87</td><td>0.47</td><td>0.85</td></tr></table><table><tr><td>Sys.</td><td>Metric</td></tr><tr><td></td><td>Prec.</td></tr><tr><td>ELEC.</td><td>Rec.</td></tr><tr><td></td><td>F1</td></tr><tr><td></td><td>Prec.</td></tr><tr><td>ADS.</td><td>Rec.</td></tr><tr><td></td><td>F1</td></tr><tr><td></td><td>Prec.</td></tr><tr><td>PALEO.</td><td>Rec.</td></tr><tr><td></td><td>F1</td></tr><tr><td></td><td>Prec.</td></tr><tr><td>GEN.</td><td>Rec.</td></tr><tr><td></td><td>F1</td></tr></table><paragraph>phrase. This heavier reliance on textual features is reflected by the drop of 33 F1 points when textual features are disabled. In ELEC- TRONICS, both components of the (part, attribute) tuples we extract are often isolated from other text (e.g. a lone number within an oth- erwise empty cell). With little to no textual information to rely on, we only see a small drop of 5 F1 points when textual features are disabled. We see a drop of 21 F1 points when structural features are disabled in the PALEONTOLOGY application due to its reliance on structural features to link between formation names (found in text sections or table captions) to the table itself. Finally, we see similar decreases when disabling structural and tabular features in the GE- NOMICS application (24 and 29 F1 points, respectively). Because this dataset is published natively in XML, structural and tabular features are almost perfectly parsed which results in similar impacts of these features. </paragraph><paragraph>Takeaways. It is necessary to utilize multimodal features to provide a robust, domain-agnostic description of real-world data. </paragraph><paragraph>5.3.3 Study of Featurization Approaches We study three different approaches for multimodal featurization: (1) a human-tuned multimodal feature library that leverages SystemX’s data model— this requires feature engineering; (2) a Bi-LSTM with attention for textual features alone—this corresponds to a state-of-the-art LSTM for information extraction; and (3) SystemX’s approach using a Bi- LSTM with attention which is extended with features from structural, tabular, and visual modalities. </paragraph><paragraph>In Table 4, we find that the extended Bi-LSTM approach used by SystemX is able to extract relations with quality comparable to the human-tuned approach in all datasets differing by 2 F1 points at most. Furthermore, SystemX’s approach outperforms a Bi-LSTM alone by 1.7× to 2.2× by extending representation with structural, tabular, and visual features. We see that the automated representation learned by SystemX’s LSTM produces results that are comparable to a manually-tuned feature representation requiring feature engineering. </paragraph><paragraph>Takeaways. Utilizing deep learning as a basis to obtain the fea- ture representation needed to extract relations from richly formatted data obviates the need for direct feature engineering. </paragraph><paragraph>5.3.4 Supervision Ablation Study We study how using only textual LFs, only metadata LFs, and the combination of the two sets affects quality. Textual LFs only operate on textual modality characteristics (such as traditional distant supervision rules), while metadata LFs operate on structural, tabular, and visual modality characteristics. Figure 8 shows that applying metadata-based LFs </paragraph><section_header>SIGMOD’18, June 2018, Houston, Texas USA </section_header><figure bbox=92.56001595,315.701527075,210.46969953,546.9613></figure><figure_caption>Figure 8: Study of different supervision resources on quality. Metadata includes structural, tabular, and visual information. </figure_caption><figure bbox=250.903880384,358.048577216,363.14062601,512.442376384></figure><figure bbox=299.63948,387.250422103,303.15308,391.307577741></figure><figure bbox=305.041632192,386.9366,309.726432192,391.6214></figure><figure bbox=322.570592192,386.9366,327.255392192,391.6214></figure><figure bbox=293.066127808,393.414635095,296.579727808,397.471790733></figure><figure bbox=298.46828,393.100812992,303.15308,397.785612992></figure><figure bbox=302.85052,393.100812992,307.53532,397.785612992></figure><figure bbox=287.588316096,405.743061079,291.101916096,409.800216717></figure><figure bbox=296.277152192,405.429238976,300.961952192,410.114038976></figure><figure bbox=296.277152192,405.429238976,300.961952192,410.114038976></figure><figure bbox=295.181596096,442.414501312,299.866396096,447.099301312></figure><figure bbox=285.32156,442.414501312,290.00636,447.099301312></figure><figure bbox=286.49276,442.728323415,290.00636,446.785479053></figure><figure bbox=292.990483904,504.0566,297.675283904,508.7414></figure><figure bbox=280.93932,504.0566,285.62412,508.7414></figure><figure bbox=285.397203904,504.370422103,288.910803904,508.427577741></figure><figure_caption>Figure 9: Quality with increasing training set size. The x-axis is the scale of the base number of input documents for each application: ELECTRONICS is 50; ADVERTISEMENTS is 25,000; PALEONTOLOGY is 44; GENOMICS is 30. </figure_caption><paragraph>can achieve higher quality than traditional textual-level LFs alone and that the highest quality is achieved when both types of LFs are used. In the ELECTRONICS application, we see an increase of 66 F1 points (9.2×) when using metadata LFs compared to textual LFs and a 3 F1 point (1.04×) improvement over metadata LFs when both types are used. Because this dataset relies more heavily on distant signals, LFs that can label correctness based on column or row header content significantly improve extraction quality. In sharp contrast, the ADVERTISEMENTS application benefits equally from metadata and textual LFs. Yet, we increase by 20 F1 points (1.2×) when both types of LFs are applied. The PALEONTOLOGY and GENOMICS applications show more moderate increases of 40 (4.6×) and 40 (1.8×) F1 points by using both types over only textual LFs, respectively, which reflects how each dataset’s characteristics cater to particular supervision sources. Our experience with SystemX has shown that users are able to effectively express their intuitions using SystemX’s weak supervision resources. </paragraph><paragraph>Takeaways. To accommodate effective labeling functions, an in- formation extraction framework for richly formatted data should pre- serve information from all data modalities and provide easy access to this information in order to leverage higher quality supervision. 5.4 Scaling with Quantity of Input Data We substantiate the effectiveness of user-defined labeling functions in generating this training data by investigating how increasing the number of training documents affects average F1 score. Figure 9 </paragraph></div><div id=12><header>SIGMOD’18, June 2018, Houston, Texas USA </header><paragraph>shows how quality changes with increasing training set size. We scale a base number of dataset in increments of {1×, 2×, 4×, 10×, 20×} for all four applications while holding other variables constant. </paragraph><paragraph>Each of our four applications show quality improvements as the number of input documents is increased. When scaling from 1× to 20×, ADVERTISEMENTS quality improves by 38 F1 points (1.7×), while the other applications improve by 1.2× on average. This larger relative improvement may be due to the data variety of the AD- VERTISEMENTS dataset which comes from 1000s of web domains, whereas the other datasets reflect the lower format variety from academic journals and a small number of manufacturers. Despite this, these applications also benefit from increased input data. In ELECTRONICS, increased data provides SystemX a larger sample of each manufacturer’s style, which improves learning and inference. Labeling functions help SystemX avoid cascading inference errors due to lack of supervision that can plague traditional approaches. </paragraph><paragraph>Takeaways. By using user-defined labeling functions to generate training data, users are able to increase KBC quality by increasing the training set size. 6 USER STUDY Traditionally, ground truth data is created through manual annota- tion, crowdsourcing, or other time-consuming methods and then used as training data for training a machine learning model. In SystemX, we use the data programming model for users to pro- grammatically generate training data, rather than needing to perform manual annotation–a human-in-the-loop approach. In this section we qualitatively evaluate the effectiveness of our approach compared to traditional human labeling and discuss the cognitive load required by our users. </paragraph><paragraph>We conducted a user study with 10 users, where each user was asked to complete the relation extraction task of extracting maximum collector-emitter voltages from the ELECTRONICS dataset. Using the same experimental settings, we compare the effectiveness of these two approaches for obtaining training data: (1) manual annotations (Manual) and (2) using labeling functions (LF). To minimize the effect of cognitive fatigue and familiarity with the task, half of the users performed the task of manually annotating training data first, then of writing labeling functions, while the other half performed the tasks in the reverse order. We allotted 30 minutes for each task and evaluate the quality that was achieved using each approach at several checkpoints. For manual annotations, we set 5 minutes as one evalua- tion epoch. Similarly, we plot the quality achieved by user’s labeling functions each time the user performed an iteration of supervision and classification as part of SystemX’s iterative approach. Note, we filtered out two outliers and report results of 8 users. </paragraph><paragraph>In Figure 10 (left), we report the quality (F1 score) achieved by the two different approaches. The average F1 achieved using manual annotation was 0.26 while the average F1 score using labeling func- tions was 0.49, an improvement of 1.9×. We found with statistical significance that all users were able to achieve higher F1 scores using labeling functions than manually annotating candidates in 30 min- utes, regardless of the order in which they performed the approaches. There are two primary reasons for this trend. First, labeling functions provide a larger set of training data than manual annotations by en- abling users to apply patterns they find in the data programmatically </paragraph><figure bbox=90.108747314,328.293657186,177.743758738,537.883627649></figure><figure bbox=148.467152352,362.260457,149.332032352,363.125337></figure><figure bbox=142.407738206,362.260457,143.272618206,363.125337></figure><figure bbox=140.023696486,362.260457,140.888576486,363.125337></figure><figure bbox=148.169157948,362.260457,149.034037948,363.125337></figure><figure bbox=140.023696486,369.741669,140.888576486,370.606549></figure><figure bbox=157.576155,373.482275,158.441035,374.347155></figure><figure bbox=150.553188857,373.482275,151.418068857,374.347155></figure><figure bbox=141.215717346,373.482275,142.080597346,374.347155></figure><figure bbox=157.576155,373.482275,158.441035,374.347155></figure><figure bbox=142.209064459,373.482275,143.073944459,374.347155></figure><figure bbox=140.719049195,373.482275,141.583929195,374.347155></figure><figure bbox=137.540334109,373.482275,138.405214109,374.347155></figure><figure bbox=131.381577684,373.482275,132.246457684,374.347155></figure><figure bbox=122.441442856,373.482275,123.306322856,374.347155></figure><figure bbox=140.023696486,380.963487,140.888576486,381.828367></figure><figure bbox=146.381126658,392.185305,147.246006658,393.050185></figure><figure bbox=135.950976566,392.185305,136.815856566,393.050185></figure><figure bbox=137.738997045,392.185305,138.603877045,393.050185></figure><figure bbox=146.381126658,392.185305,147.246006658,393.050185></figure><figure bbox=138.136333728,392.185305,139.001213728,393.050185></figure><figure bbox=137.838339324,392.185305,138.703219324,393.050185></figure><figure bbox=135.950976566,392.185305,136.815856566,393.050185></figure><figure bbox=129.593546394,392.185305,130.458426394,393.050185></figure><figure bbox=118.865391087,395.925911,119.730271087,396.790791></figure><figure bbox=114.892002635,395.925911,115.756882635,396.790791></figure><figure bbox=144.195758685,395.925911,145.060638685,396.790791></figure><figure bbox=142.407738206,403.407123,143.272618206,404.272003></figure><figure bbox=146.182452911,410.888335,147.047332911,411.753215></figure><figure bbox=143.599759066,410.888335,144.464639066,411.753215></figure><figure bbox=151.844541185,410.888335,152.709421185,411.753215></figure><figure bbox=146.182452911,410.888335,147.047332911,411.753215></figure><figure bbox=141.414391093,410.888335,142.279271093,411.753215></figure><figure bbox=142.010401523,410.888335,142.875281523,411.753215></figure><figure bbox=144.791769115,410.888335,145.656649115,411.753215></figure><figure bbox=139.328354588,410.888335,140.193234588,411.753215></figure><figure bbox=111.415282334,410.888335,112.280162334,411.753215></figure><figure bbox=126.613505055,410.888335,127.478385055,411.753215></figure><figure bbox=131.58024062,410.888335,132.44512062,411.753215></figure><figure bbox=117.872043974,422.110153,118.736923974,422.975033></figure><figure bbox=146.381126658,429.591365,147.246006658,430.456245></figure><figure bbox=143.103080104,429.591365,143.967960104,430.456245></figure><figure bbox=129.394883458,429.591365,130.259763458,430.456245></figure><figure bbox=146.381126658,429.591365,147.246006658,430.456245></figure><figure bbox=150.95052554,429.591365,151.81540554,430.456245></figure><figure bbox=130.884898722,429.591365,131.749778722,430.456245></figure><figure bbox=137.937670792,429.591365,138.802550792,430.456245></figure><figure bbox=140.42104398,429.591365,141.28592398,430.456245></figure><figure bbox=116.382017899,429.591365,117.246897899,430.456245></figure><figure bbox=144.692437647,433.331971,145.557317647,434.196851></figure><figure bbox=110.819282715,437.072577,111.684162715,437.937457></figure><figure bbox=107.938572844,437.072577,108.803452844,437.937457></figure><figure bbox=112.209966511,440.813183,113.074846511,441.678063></figure><figure bbox=128.600199281,440.813183,129.465079281,441.678063></figure><figure bbox=118.964722555,444.553789,119.829602555,445.418669></figure><figure bbox=139.725691271,448.294395,140.590571271,449.159275></figure><figure bbox=136.348313249,448.294395,137.213193249,449.159275></figure><figure bbox=133.169598163,448.294395,134.034478163,449.159275></figure><figure bbox=139.725691271,448.294395,140.590571271,449.159275></figure><figure bbox=146.083121443,448.294395,146.948001443,449.159275></figure><figure bbox=135.454297604,448.294395,136.319177604,449.159275></figure><figure bbox=138.434338943,448.294395,139.299218943,449.159275></figure><figure bbox=129.792220141,448.294395,130.657100141,449.159275></figure><figure bbox=113.203313624,459.516213,114.068193624,460.381093></figure><figure bbox=146.679131873,466.997425,147.544011873,467.862305></figure><figure bbox=134.460950491,466.997425,135.325830491,467.862305></figure><figure bbox=127.606852168,466.997425,128.471732168,467.862305></figure><figure bbox=146.679131873,466.997425,147.544011873,467.862305></figure><figure bbox=144.791769115,466.997425,145.656649115,467.862305></figure><figure bbox=126.812178802,466.997425,127.677058802,467.862305></figure><figure bbox=131.282246216,466.997425,132.147126216,467.862305></figure><figure bbox=129.196209711,466.997425,130.061089711,467.862305></figure><figure bbox=108.633914742,466.997425,109.498794742,467.862305></figure><figure bbox=116.680033925,466.997425,117.544913925,467.862305></figure><figure bbox=112.408640258,466.997425,113.273520258,467.862305></figure><figure bbox=122.143437641,466.997425,123.008317641,467.862305></figure><figure bbox=123.732795184,466.997425,124.597675184,467.862305></figure><figure bbox=111.117277119,466.997425,111.982157119,467.862305></figure><figure bbox=122.044106173,466.997425,122.908986173,467.862305></figure><figure_caption>Figure 10: F1 quality over time with 95% confidence intervals (left). Modality Distribution of user labeling functions (right). </figure_caption><paragraph>to all candidates–a natural desire they often vocalized while per- forming manual annotation. On average, our users manually labeled 285 candidates in the allotted time, while the labeling functions they provided labeled 19,075 candidates. Users provided 7 labeling func- tions on average. Second, labeling functions tend to allow SystemX to learn more general features, where manual annotations may not adequately cover the characteristics of the dataset as a whole. For example, labeling functions are easily applied to new data. </paragraph><paragraph>In addition, we found that for richly formatted data, users relied less on textual information–a primary signal in traditional KBC tasks–and more on information from other modalities, as shown in Figure 10 (right). Users utilized the semantics from multiple modalities of the richly formatted data, with 58.5% of their labeling functions using tabular information. This reflects the characteristics of the ELECTRONICS dataset, which contains information that is primarily found in tables. In our study, the most common labeling functions in each modality were: </paragraph><section_header>• Tabular: labeling a candidate based on the words found in </section_header><paragraph>the same row or column. • Visual: labeling a candidate based on its placement in a doc- </paragraph><paragraph>ument (e.g., which page it was found on). • Structural: labeling a candidate based on its tag names. • Textual: labeling a candidate based on the textual character- </paragraph><section_header>istics of the voltage mention (e.g. magnitude). </section_header><paragraph>Takeaways. We found that leveraging labeling functions and data programming allowed users to create high quality knowledge bases more effectively than a traditional manual annotation based approach. In addition, when working with richly formatted data, users relied heavily on non-textual signals when labeling candidates. 7 CONCLUSION In this paper, we study how to extract information from richly for- matted data. We show that key challenges of this problem are (1) prevalent document-level relations, (2) multimodality, and (3) data variety. To address these, we propose SystemX, the first KBC system for richly formatted information extraction. We describe SystemX’s data model, which enables users to perform candidate extraction, multimodal featurization, and multimodal supervision through a sim- ple programming model. We evaluate SystemX on four real-world domains and achieve an average improvement of 41 F1 points over the upper bound of state-of-the-art approaches. In some domains, SystemX extracts up to 1.87× the number of correct relations com- pared to expert-curated public knowledge bases. </paragraph></div><div id=13><section_header>References </section_header><paragraph>SystemX: Knowledge Base Construction from Richly Formatted Data </paragraph><list>[1] G. Angeli, S. Gupta, M. Jose, C. D. Manning, C. Ré, J. Tibshirani, J. Y. Wu, S. Wu, and C. Zhang. Stanford’s 2014 slot filling systems. TAC KBP, 695, 2014. [2] D. Bahdanau, K. Cho, and Y. Bengio. Neural machine translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473, 2014. [3] D. W. Barowy, S. Gulwani, T. Hart, and B. Zorn. Flashrelate: extracting rela- tional data from semi-structured spreadsheets using examples. In ACM SIGPLAN Notices, volume 50, pages 218–228. ACM, 2015. [4] T. Beck, R. K. Hastings, S. Gollapudi, R. C. Free, and A. J. Brookes. Gwas central: a comprehensive resource for the comparison and interrogation of genome-wide association studies. EJHG, 22(7):949–952, 2014. [5] K. Bollacker, C. Evans, P. Paritosh, T. Sturge, and J. Taylor. Freebase: a collabo- ratively created graph database for structuring human knowledge. In SIGMOD, pages 1247–1250. ACM, 2008. [6] E. Brown, E. Epstein, J. W. Murdock, and T.-H. Fin. Tools and methods for building watson. IBM Research. Abgerufen am, 14:2013, 2013. [7] A. Carlson, J. Betteridge, B. Kisiel, B. Settles, E. R. Hruschka Jr, and T. M. Mitchell. Toward an architecture for never-ending language learning. In AAAI, volume 5, page 3, 2010. [8] M. Cosulschi, N. Constantinescu, and M. Gabroveanu. Classifcation and com- parison of information structures from a web page. Annals of the University of Craiova-Mathematics and Computer Science Series, 31, 2004. [9] X. Dong, E. Gabrilovich, G. Heitz, W. Horn, N. Lao, K. Murphy, T. Strohmann, S. Sun, and W. Zhang. Knowledge vault: A web-scale approach to probabilistic knowledge fusion. In SIGKDD, pages 601–610. ACM, 2014. [10] D. Ferrucci, E. Brown, J. Chu-Carroll, J. Fan, D. Gondek, A. A. Kalyanpur, A. Lally, J. W. Murdock, E. Nyberg, J. Prager, et al. Building watson: An overview of the deepqa project. AI magazine, 31(3):59–79, 2010. [11] H. Gao, G. Barbier, and R. Goolsby. Harnessing the crowdsourcing power of social media for disaster relief. IEEE Intelligent Systems, 26(3):10–14, 2011. [12] W. Gatterbauer, P. Bohunsky, M. Herzog, B. Krüpl, and B. Pollak. Towards domain-independent information extraction from web tables. In WWW, pages 71–80. ACM, 2007. [13] V. Govindaraju, C. Zhang, and C. Ré. Understanding tables in context using standard nlp toolkits. In ACL, pages 658–664, 2013. [14] A. Graves, M. Liwicki, S. Fernández, R. Bertolami, H. Bunke, and J. Schmidhuber. A novel connectionist system for unconstrained handwriting recognition. IEEE transactions on pattern analysis and machine intelligence, 31(5):855–868, 2009. [15] A. Graves, A.-r. Mohamed, and G. Hinton. Speech recognition with deep recurrent neural networks. In Acoustics, speech and signal processing (icassp), 2013 ieee international conference on, pages 6645–6649. IEEE, 2013. [16] M. Hewett, D. E. Oliver, D. L. Rubin, K. L. Easton, J. M. Stuart, R. B. Altman, and T. E. Klein. Pharmgkb: the pharmacogenetics knowledge base. Nucleic acids research, 30(1):163–165, 2002. [17] S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural computation, 9(8):1735–1780, 1997. [18] N. Japkowicz and S. Stephen. The class imbalance problem: A systematic study. Intelligent data analysis, 6(5):429–449, 2002. [19] M. Kovacevic, M. Diligenti, M. Gori, and V. Milutinovic. Recognition of common areas in a web page using visual information: a possible application in a page classification. In ICDM, pages 250–257. IEEE, 2002. [20] Y. LeCun, Y. Bengio, and G. Hinton. Deep learning. Nature, 521(7553):436–444, 2015. [21] J. Li, M.-T. Luong, and D. Jurafsky. A hierarchical neural autoencoder for paragraphs and documents. arXiv preprint arXiv:1506.01057, 2015. [22] A. Madaan, A. Mittal, G. R. Mausam, G. Ramakrishnan, and S. Sarawagi. Nu- merical relation extraction with minimal supervision. In AAAI, pages 2764–2771, 2016. [23] C. Manning. Representations for language: From word embeddings to sentence meanings. https://simons.berkeley.edu/talks/christopher-manning-2017-3-27, 2017. [24] B. Min, R. Grishman, L. Wan, C. Wang, and D. Gondek. Distant supervision for relation extraction with an incomplete knowledge base. In HLT-NAACL, pages 777–782, 2013. [25] M. Mintz, S. Bills, R. Snow, and D. Jurafsky. Distant supervision for relation extraction without labeled data. In ACL, pages 1003–1011, 2009. [26] N. Nakashole, M. Theobald, and G. Weikum. Scalable knowledge harvesting with high precision and high recall. In WSDM, pages 227–236. ACM, 2011. [27] T.-V. T. Nguyen and A. Moschitti. End-to-end relation extraction using distant supervision from external semantic repositories. In HLT, pages 277–282, 2011. [28] P. Pasupat and P. Liang. Zero-shot entity extraction from web pages. In ACL (1), pages 391–401, 2014. [29] G. Penn, J. Hu, H. Luo, and R. T. McDonald. Flexible web document analysis for delivery to narrow-bandwidth devices. In ICDAR, volume 1, page 1074, 2001. [30] D. Pinto, M. Branstein, R. Coleman, W. B. Croft, M. King, W. Li, and X. Wei. Quasm: a system for question answering using semi-structured data. In JCDL, pages 46–55, 2002. </list><list>SIGMOD’18, June 2018, Houston, Texas USA [31] A. Ratner, C. De Sa, S. Wu, D. Selsam, and C. Ré. Data programming: Creating large training sets, quickly. arXiv preprint arXiv:1605.07723, 2016. [32] A. J. Ratner, S. H. Bach, H. R. Ehrenberg, and C. Ré. Snorkel: Fast training set generation for information extraction. In Proceedings of the 2017 ACM International Conference on Management of Data, pages 1683–1686. ACM, 2017. [33] C. Ré, A. A. Sadeghian, Z. Shan, J. Shin, F. Wang, S. Wu, and C. Zhang. Feature engineering for knowledge base construction. arXiv preprint arXiv:1407.6439, 2014. [34] J. Shin, S. Wu, F. Wang, C. De Sa, C. Zhang, and C. Ré. Incremental knowledge base construction using deepdive. VLDB, 8(11):1310–1321, 2015. [35] A. Singhal. Introducing the knowledge graph: things, not strings. Official google blog, 2012. [36] F. M. Suchanek, G. Kasneci, and G. Weikum. Yago: A large ontology from wikipedia and wordnet. Web Semantics: Science, Services and Agents on the World Wide Web, 6(3):203–217, 2008. [37] A. Tengli, Y. Yang, and N. L. Ma. Learning table extraction from examples. In COLING, page 987, 2004. [38] J. Turian, L. Ratinov, and Y. Bengio. Word representations: a simple and general method for semi-supervised learning. In Proceedings of the 48th annual meeting of the association for computational linguistics, pages 384–394. Association for Computational Linguistics, 2010. [39] P. Verga, D. Belanger, E. Strubell, B. Roth, and A. McCallum. Multilin- gual relation extraction using compositional universal schema. arXiv preprint arXiv:1511.06396, 2015. [40] D. Welter, J. MacArthur, J. Morales, T. Burdett, P. Hall, H. Junkins, A. Klemm, P. Flicek, T. Manolio, L. Hindorff, et al. The nhgri gwas catalog, a curated resource of snp-trait associations. Nucleic acids research, 42:D1001–D1006, 2014. [41] Y. Wu, M. Schuster, Z. Chen, Q. V. Le, M. Norouzi, W. Macherey, M. Krikun, Y. Cao, Q. Gao, K. Macherey, et al. Google’s neural machine translation sys- tem: Bridging the gap between human and machine translation. arXiv preprint arXiv:1609.08144, 2016. [42] M. Yahya, S. E. Whang, R. Gupta, and A. Halevy. ReNoun : Fact Extraction for Nominal Attributes. EMNLP, pages 325–335, 2014. [43] Y. Yang and H. Zhang. Html page analysis based on visual cues. In ICDAR, pages 859–864. IEEE, 2001. [44] Z. Yang, D. Yang, C. Dyer, X. He, A. J. Smola, and E. H. Hovy. Hierarchical attention networks for document classification. In HLT-NAACL, pages 1480–1489, 2016. [45] Y. Zhang, A. Chaganty, A. Paranjape, D. Chen, J. Bolton, P. Qi, and C. D. Man- ning. Stanford at tac kbp 2016: Sealing pipeline leaks and understanding chinese. Proceedings of TAC, 2016. A DATA PROGRAMMING Machine-learning based KBC systems rely heavily on ground truth data (called training data) to achieve high quality. Traditionally, manual annotations or incomplete KBs are used to construct training data for machine-learning based KBC systems. However, these re- sources are either costly to obtain or may have limited coverage over the candidates considered during the KBC process. To address this challenge, SystemX builds upon the newly introduced paradigm of data programming [31], which enables domain experts to program- matically generate large training data sets by leveraging multiple weak supervision sources and domain knowledge. Data programming provides a simple, unifying framework for weak supervision, in which training labels are noisy and may come from multiple, potentially overlapping sources. In data programming, users encode this weak supervision in the form of labeling functions, which are user-defined functions that each provide a label for some subset of the data, and collectively generate a large and potentially overlapping set of training labels. Many different weak supervision approaches can be expressed as labeling functions. This includes strategies which utilize existing knowledge bases, individual annota- tor’s labels (as in crowdsourcing), or user-defined functions that rely on domain-specific patterns and dictionaries to assign labels to the input data. </list></div><div id=14><list>SIGMOD’18, June 2018, Houston, Texas USA The aforementioned sources of supervision can have varying degrees of accuracy, and may conflict with each other. Data pro- gramming relies on a generative probabilistic model to estimate the accuracy of each labeling function by reasoning about the conflicts and overlap across labeling functions. The estimated labeling func- tion accuracies are in turn used to assign a probabilistic label to each candidate, which are used in conjunction with a noise-aware discriminative model to train a machine learning model for KBC. A.1 Components of Data Programming The main components in data programming are as follows: Candidates A set of candidates C are probabilistically classified. Labeling Functions Labeling functions are used to programmati- cally provide labels for training data. A labeling function is a user- defined procedure that takes a candidate as input and outputs a label. Labels can be as simple as true or false for binary tasks, or one of many classes for more complex multiclass tasks. Since each labeling function is applied to all candidates and labeling functions are rarely perfectly accurate, there may be disagreements between them. The labeling functions provided by the user for binary classification can be more formally defined as follows: For each labeling function λi and r ∈ C, we have λi : r (cid:55)→ {−1, 0, 1} where +1 or −1 denotes a candidate as true or false, and 0 abstains. The output of apply- ing a set of l labeling functions to k candidates is the label matrix Λ ∈ {−1, 0, 1}k×l. Output Data programming frameworks output a confidence value p for the classification for each candidate as a vector Y ∈ {p}k. To perform data programming in SystemX, we rely on a data pro- gramming engine, Snorkel5. Snorkel accepts candidates, and labels as input, and produces marginal probabilities for each candidate as output. These input and output components are stored as relational tables, whose schemas are detailed in Section 3. A.2 Theoretical Guarantees While data programming uses labeling functions to generate noisy training data, it theoretically achieves a learning rate similar to meth- ods that use manually labeled data [31]. In the typical supervised learning setup, users are required to manually label ˜O(ϵ−2) ex- amples for the target model to achieve an expected loss of ϵ. To achieve this rate, data programming only requires the user to specify a constant number of labeling functions that does not depend on ϵ. Let β be the minimum coverage across labeling functions (i.e., the probability that a labeling function provides a label for an input point), and γ be the minimum reliability of labeling functions, where γ = 2 · a − 1 with a denoting the accuracy of a labeling function. Then under the assumptions that: (1) labeling functions are condition- ally independent given the true labels of input data, (2) the number of user-provided labeling functions is at least ˜O(γ−3β−1), and (3) there are k = ˜O(ϵ−2) candidates, data programming achieves an expected loss ϵ. Despite the strict assumptions with respect to la- beling functions, we find that using data programming to develop KBC systems for richly formatted data leads to high-quality KBs (across diverse real-world applications) even when some of the data programming assumptions are not met (see Section 5.4). 5snorkel.stanford.edu </list><list>B EXTENDED FEATURE LIBRARY SystemX augments a bidirectional LSTM with features from an extended feature library in order to better model the multiple modal- ities of richly formatted data. In addition, these extended features can provide signals drawn from the large contexts since they can be calculated using SystemX’s data model of the document, rather than being limited to a single sentence or table. In Section 5, we find that including multimodal features is critical to achieving high quality relation extraction. The provided extended feature library serves as a baseline example of these types of features, which can be easily enhanced in the future. However, even with these baseline features, our users have been able to build high quality knowledge bases for their applications. The extended feature library consists of a baseline set of features from the structural, tabular, and visual modalities. Table 5 lists the details of extended feature library. When input documents are in PDF format, we use tools such as Poppler to convert them into HTML format in order to extract tabular and structural information such as parent-child relationships, or row and column numbers. Similarly, when HTML/XML documents are provided, we render them as PDF in order to extract visual information such as character positions and bounding boxes. As shown in Table 5, the features are represented as strings. This feature space is then mapped into one-dimensional bit vector for each candidate, where each bit represents whether the candidate has the corresponding feature. C SystemX AT SCALE We discuss design decisions for KBC from richly formatted data by analyzing the benefits of caching, as well as discussing data representation choices with respect to performance for the abstract data structures used in SystemX. C.1 Data Caching With richly formatted data, which frequently requires document- level context, thousands of candidates need to be featurized for each document. Candidate features from the extended feature library are computed at both the mention level and relation level, by traversing the data model accessing modality attributes. Because each men- tion of a relation is part of many candidates, naïve featurization of candidates can result in the redundant computation of thousands of mention features. This pattern highlights the value of data caching when performing multimodal featurization on richly formatted data. Traditional KBC systems that operate on single sentences of unstructured text pragmatically assume that only a small number of candidates will need to be featurized for each sentence, and do not cache mention features as a result. Example C.1 (Inefficient Featurization). In Figure 1, the transistor part mention MMBT3904 could be matched with up to 15 different numerical values in the datasheet. Without caching, the features of the MMBT3904 would be unnecessarily recalculated 14 times, once for each candidate. In real documents 100s of feature calculations would be wasted. In Example C.1, eliminating unnecessary feature computations can improve performance by an order of magnitude. To optimize the feature generation process, SystemX implements a document-level caching scheme for mention features. The first </list></div><div id=15><list>SystemX: Knowledge Base Construction from Richly Formatted Data prefixes represent the feature templates and the remainder of the string represents a feature’s value. Feature Type Arity Description Structural Unary HTML tag of the mention TAG_<h1> Structural Unary HTML attributes of the mention Structural Unary HTML tag of the mention’s parent Structural Unary HTML tag of the mention’s previous sibling Structural Unary HTML tag of the mention’s next sibling Structural Unary Position of a node among its siblings NODE_POS_1 Structural Unary HTML class sequence of the mention’s ancestors Structural Unary HTML tag sequence of the mention’s ancestors Structural Unary HTML id’s of the mention’s ancestors Structural Binary HTML tags shared between mentions on the path to the root of the document Structural Binary Minimum distance between two mentions to their lowest common ancestor Tabular Unary N-grams in the same cell as the mentiona CELL_cevb Tabular Unary Row number of the mention ROW_NUM_5 Tabular Unary Column number of the mention COL_NUM_3 Tabular Unary Number of rows the mention spans ROW_SPAN_1 Tabular Unary Number of columns the mention spans COL_SPAN_1 Tabular Unary Row header n-grams in the table of the mention Tabular Unary Column header n-grams in the table of the mention Tabular Unary N-grams from all Cells that are in the same row as the given mentiona Tabular Unary N-grams from all Cells that are in the same column as the given mentiona Tabular Binary Whether two mentions are in the same table SAME_TABLEb Tabular Binary Row number difference if two mentions are in the same table Tabular Binary Column number difference if two mentions are in the same table Tabular Binary Manhattan distance between two mentions in the same table Tabular Binary Whether two mentions are in the same cell SAME_CELLb Tabular Binary Mention distance in words of mentions in the same cell Tabular Binary Mention distance in chars of mentions in the same cell Tabular Binary Whether two mentions in a cell are in the same sentence Tabular Binary Whether two mention are in the different tables DIFF_TABLEb Tabular Binary Row number difference of two mentions in different tables Tabular Binary Column number difference if two mentions are in the different table Tabular Binary Manhattan distance between two mentions in different tables Visual Unary N-grams of all lemmas visually aligned with the mentiona Visual Unary Page number of the mention PAGE_1 Visual Binary Whether two mentions are on the same page SAME_PAGE Visual Binary Whether two mentions are horizontally aligned Visual Binary Whether two mentions are vertically aligned Visual Binary Whether two mentions’ left bounding box borders are vertically aligned Visual Binary Whether two mentions’ right bounding box borders are vertically aligned Visual Binary Whether the center of two mentions’ bounding boxes are vertically aligned a All N-grams are 1-grams by default. b This feature was not present in the example candidate. The values shown are example values from other documents. c In this example, the mention is 200, which forms part of the feature prefix. The value is shown in square brackets. computation of a mention feature requires traversing the data model. Then, the result is cached for fast access if the feature is needed again. All features are cached until all candidates in a document are memory footprint of featurization. fully featurized, after which the cache is flushed. Because SystemX operates on documents atomically, caching a single document at a time improves performance without adding significant memory overhead. In the ELECTRONICS application, we find that caching </list><list>SIGMOD’18, June 2018, Houston, Texas USA Example Value TAG_<h1> HTML_ATTR_font-family:Arial PARENT_TAG_<p> PREV_SIB_TAG_<td> NEXT_SIB_TAG_<h1> NODE_POS_1 ANCESTOR_CLASS_<s1> ANCESTOR_TAG_<body>_<p> ANCESTOR_ID_l1b COMMON_ANCESTOR_<body> LOWEST_ANCESTOR_DEPTH_1 CELL_cevb ROW_NUM_5 COL_NUM_3 ROW_SPAN_1 COL_SPAN_1 ROW_HEAD_collector COL_HEAD_value ROW_200_[ma]c COL_200_[6]c SAME_TABLEb SAME_TABLE_ROW_DIFF_1b SAME_TABLE_COL_DIFF_3b SAME_TABLE_MANHATTAN_DIST_10b SAME_CELLb WORD_DIFF_1b CHAR_DIFF_1b SAME_PHRASEb DIFF_TABLEb DIFF_TABLE_ROW_DIFF_4b DIFF_TABLE_COL_DIFF_2b DIFF_TABLE_MANHATTAN_DIST_7b ALIGNED_current PAGE_1 SAME_PAGE HORZ_ALIGNEDb VERT_ALIGNED VERT_ALIGNED_LEFTb VERT_ALIGNED_RIGHTb VERT_ALIGNED_CENTERb achieves over 100× speed up on average and in some cases even over 1000×, while only accounting for approximately 10% of the memory footprint of featurization. Takeaways. When performing feature generation from richly for- matted data, caching the intermediate results can yield over 1000× </list><table_caption>Table 5: Features from SystemX’s feature library. Example values are drawn from the example candidate in Figure 1. Capitalized prefixes represent the feature templates and the remainder of the string represents a feature’s value. </table_caption><table><tr><td>Feature Type</td><td>Arity</td><td>Description</td><td>Example Value</td></tr><tr><td>Structural</td><td>Unary</td><td>HTML tag of the mention</td><td>TAG_<h1></td></tr><tr><td>Structural</td><td>Unary</td><td>HTML attributes of the mention</td><td>HTML_ATTR_font-family:Arial</td></tr><tr><td>Structural</td><td>Unary</td><td>HTML tag of the mention’s parent</td><td>PARENT_TAG_<p></td></tr><tr><td>Structural</td><td>Unary</td><td>HTML tag of the mention’s previous sibling</td><td>PREV_SIB_TAG_<td></td></tr><tr><td>Structural</td><td>Unary</td><td>HTML tag of the mention’s next sibling</td><td>NEXT_SIB_TAG_<h1></td></tr><tr><td>Structural</td><td>Unary</td><td>Position of a node among its siblings</td><td>NODE_POS_1</td></tr><tr><td>Structural</td><td>Unary</td><td>HTML class sequence of the mention’s ancestors</td><td>ANCESTOR_CLASS_<s1></td></tr><tr><td>Structural</td><td>Unary</td><td>HTML tag sequence of the mention’s ancestors</td><td>ANCESTOR_TAG_<body>_<p></td></tr><tr><td>Structural</td><td>Unary</td><td>HTML id’s of the mention’s ancestors</td><td>bANCESTOR_ID_l1</td></tr><tr><td>Structural</td><td>Binary</td><td>HTML tags shared between mentions on the path to the root of the document</td><td>COMMON_ANCESTOR_<body></td></tr><tr><td>Structural</td><td>Binary</td><td>Minimum distance between two mentions to their lowest common ancestor</td><td>LOWEST_ANCESTOR_DEPTH_1</td></tr><tr><td>Tabular</td><td>Unary</td><td>N-grams in the same cell as the mentiona</td><td>bCELL_cev</td></tr><tr><td>Tabular</td><td>Unary</td><td>Row number of the mention</td><td>ROW_NUM_5</td></tr><tr><td>Tabular</td><td>Unary</td><td>Column number of the mention</td><td>COL_NUM_3</td></tr><tr><td>Tabular</td><td>Unary</td><td>Number of rows the mention spans</td><td>ROW_SPAN_1</td></tr><tr><td>Tabular</td><td>Unary</td><td>Number of columns the mention spans</td><td>COL_SPAN_1</td></tr><tr><td>Tabular</td><td>Unary</td><td>Row header n-grams in the table of the mention</td><td>ROW_HEAD_collector</td></tr><tr><td>Tabular</td><td>Unary</td><td>Column header n-grams in the table of the mention</td><td>COL_HEAD_value</td></tr><tr><td>Tabular</td><td>Unary</td><td>N-grams from all Cells that are in the same row as the given mentiona</td><td>cROW_200_[ma]</td></tr><tr><td>Tabular</td><td>Unary</td><td>N-grams from all Cells that are in the same column as the given mentiona</td><td>COL_200_[6]c</td></tr><tr><td>Tabular</td><td>Binary</td><td>Whether two mentions are in the same table</td><td>bSAME_TABLE</td></tr><tr><td>Tabular</td><td>Binary</td><td>Row number difference if two mentions are in the same table</td><td>bSAME_TABLE_ROW_DIFF_1</td></tr><tr><td>Tabular</td><td>Binary</td><td>Column number difference if two mentions are in the same table</td><td>bSAME_TABLE_COL_DIFF_3</td></tr><tr><td>Tabular</td><td>Binary</td><td>Manhattan distance between two mentions in the same table</td><td>SAME_TABLE_MANHATTAN_DIST_10</td></tr><tr><td>Tabular</td><td>Binary</td><td>Whether two mentions are in the same cell</td><td>bSAME_CELL</td></tr><tr><td>Tabular</td><td>Binary</td><td>Mention distance in words of mentions in the same cell</td><td>WORD_DIFF_1b</td></tr><tr><td>Tabular</td><td>Binary</td><td>Mention distance in chars of mentions in the same cell</td><td>CHAR_DIFF_1b</td></tr><tr><td>Tabular</td><td>Binary</td><td>Whether two mentions in a cell are in the same sentence</td><td>SAME_PHRASEb</td></tr><tr><td>Tabular</td><td>Binary</td><td>Whether two mention are in the different tables</td><td>bDIFF_TABLE</td></tr><tr><td>Tabular</td><td>Binary</td><td>Row number difference of two mentions in different tables</td><td>bDIFF_TABLE_ROW_DIFF_4</td></tr><tr><td>Tabular</td><td>Binary</td><td>Column number difference if two mentions are in the different table</td><td>bDIFF_TABLE_COL_DIFF_2</td></tr><tr><td>Tabular</td><td>Binary</td><td>Manhattan distance between two mentions in different tables</td><td>DIFF_TABLE_MANHATTAN_DIST_7b</td></tr><tr><td>Visual</td><td>Unary</td><td>N-grams of all lemmas visually aligned with the mentiona</td><td>ALIGNED_current</td></tr><tr><td>Visual</td><td>Unary</td><td>Page number of the mention</td><td>PAGE_1</td></tr><tr><td>Visual</td><td>Binary</td><td>Whether two mentions are on the same page</td><td>SAME_PAGE</td></tr><tr><td>Visual</td><td>Binary</td><td>Whether two mentions are horizontally aligned</td><td>bHORZ_ALIGNED</td></tr><tr><td>Visual</td><td>Binary</td><td>Whether two mentions are vertically aligned</td><td>VERT_ALIGNED</td></tr><tr><td>Visual</td><td>Binary</td><td>Whether two mentions’ left bounding box borders are vertically aligned</td><td>VERT_ALIGNED_LEFTb</td></tr><tr><td>Visual</td><td>Binary</td><td>Whether two mentions’ right bounding box borders are vertically aligned</td><td>bVERT_ALIGNED_RIGHT</td></tr><tr><td>Visual</td><td>Binary</td><td>Whether the center of two mentions’ bounding boxes are vertically aligned</td><td>bVERT_ALIGNED_CENTER</td></tr></table><table><tr><td>Feature Type</td><td>Arity</td><td>Description</td></tr><tr><td>Structural</td><td>Unary</td><td>HTML tag of the mention</td></tr><tr><td>Structural</td><td>Unary</td><td>HTML attributes of the mention</td></tr><tr><td>Structural</td><td>Unary</td><td>HTML tag of the mention’s parent</td></tr><tr><td>Structural</td><td>Unary</td><td>HTML tag of the mention’s previous sibling</td></tr><tr><td>Structural</td><td>Unary</td><td>HTML tag of the mention’s next sibling</td></tr><tr><td>Structural</td><td>Unary</td><td>Position of a node among its siblings</td></tr><tr><td>Structural</td><td>Unary</td><td>HTML class sequence of the mention’s ancestors</td></tr><tr><td>Structural</td><td>Unary</td><td>HTML tag sequence of the mention’s ancestors</td></tr><tr><td>Structural</td><td>Unary</td><td>HTML id’s of the mention’s ancestors</td></tr><tr><td>Structural</td><td>Binary</td><td>HTML tags shared between mentions on the path to the root of the document</td></tr><tr><td>Structural</td><td>Binary</td><td>Minimum distance between two mentions to their lowest common ancestor</td></tr><tr><td>Tabular</td><td>Unary</td><td>N-grams in the same cell as the mentiona</td></tr><tr><td>Tabular</td><td>Unary</td><td>Row number of the mention</td></tr><tr><td>Tabular</td><td>Unary</td><td>Column number of the mention</td></tr><tr><td>Tabular</td><td>Unary</td><td>Number of rows the mention spans</td></tr><tr><td>Tabular</td><td>Unary</td><td>Number of columns the mention spans</td></tr><tr><td>Tabular</td><td>Unary</td><td>Row header n-grams in the table of the mention</td></tr><tr><td>Tabular</td><td>Unary</td><td>Column header n-grams in the table of the mention</td></tr><tr><td>Tabular</td><td>Unary</td><td>N-grams from all Cells that are in the same row as the given mentiona</td></tr><tr><td>Tabular</td><td>Unary</td><td>N-grams from all Cells that are in the same column as the given mentiona</td></tr><tr><td>Tabular</td><td>Binary</td><td>Whether two mentions are in the same table</td></tr><tr><td>Tabular</td><td>Binary</td><td>Row number difference if two mentions are in the same table</td></tr><tr><td>Tabular</td><td>Binary</td><td>Column number difference if two mentions are in the same table</td></tr><tr><td>Tabular</td><td>Binary</td><td>Manhattan distance between two mentions in the same table</td></tr><tr><td>Tabular</td><td>Binary</td><td>Whether two mentions are in the same cell</td></tr><tr><td>Tabular</td><td>Binary</td><td>Mention distance in words of mentions in the same cell</td></tr><tr><td>Tabular</td><td>Binary</td><td>Mention distance in chars of mentions in the same cell</td></tr><tr><td>Tabular</td><td>Binary</td><td>Whether two mentions in a cell are in the same sentence</td></tr><tr><td>Tabular</td><td>Binary</td><td>Whether two mention are in the different tables</td></tr><tr><td>Tabular</td><td>Binary</td><td>Row number difference of two mentions in different tables</td></tr><tr><td>Tabular</td><td>Binary</td><td>Column number difference if two mentions are in the different table</td></tr><tr><td>Tabular</td><td>Binary</td><td>Manhattan distance between two mentions in different tables</td></tr><tr><td>Visual</td><td>Unary</td><td>N-grams of all lemmas visually aligned with the mentiona</td></tr><tr><td>Visual</td><td>Unary</td><td>Page number of the mention</td></tr><tr><td>Visual</td><td>Binary</td><td>Whether two mentions are on the same page</td></tr><tr><td>Visual</td><td>Binary</td><td>Whether two mentions are horizontally aligned</td></tr><tr><td>Visual</td><td>Binary</td><td>Whether two mentions are vertically aligned</td></tr><tr><td>Visual</td><td>Binary</td><td>Whether two mentions’ left bounding box borders are vertically aligned</td></tr><tr><td>Visual</td><td>Binary</td><td>Whether two mentions’ right bounding box borders are vertically aligned</td></tr><tr><td>Visual</td><td>Binary</td><td>Whether the center of two mentions’ bounding boxes are vertically aligned</td></tr></table></div><div id=16><header>SIGMOD’18, June 2018, Houston, Texas USA </header><list>improvements in featurization runtime without adding significant memory overheads. C.2 Data Representations The SystemX programming model involves two modes of operation: (1) development and (2) production. In development, users iteratively improve the quality of their labeling functions through error analysis, without executing the full pipeline as in previous techniques such as incremental KBC [34]. Once labeling functions are finalized, the SystemX pipeline is only run once in production. In both modes of operation, SystemX produces two abstract data structures (Features and Labels as described in Section 3). These data structures have three access patterns: (1) materialization, where the data structure is created; (2) updates, which include inserts, dele- tions, and value changes; and (3) queries, where users can inspect the features and labels to make informed updates to labeling functions. Both Features and Labels can be viewed as matrices, where each row represents annotations for a candidate (see Section 3.2). Features are dynamically named during multimodal featurization, but are static for the lifetime of a candidate; Labels are statically named in in classification, but are updated during development. Typically Features are sparse: in the ELECTRONICS application, each candidate has about 100 features while the number of unique features can be more than 10M. Labels are also sparse, where the number of unique labels corresponds to the number of labeling functions. The data representation that is implemented to store these abstract data structures can significantly affect overall system runtime. In the ELECTRONICS application, multimodal featurization accounts for 50% of end-to-end runtime while classification accounts for 15%. We discuss two common sparse matrix representations that can be materialized in a SQL database. • List of lists (LIL): each row stores a list of (column _key, value) pairs. Zero-valued pairs are omitted. An entire row can be retrieved in a single query. However, updating values requires iterating over sublists. • Coordinate list (COO): rows store (row_key, column_key, value) triples. Zero-valued triples are omitted. With COO, multiple queries must be performed to fetch a row’s attributes. However, updating values takes constant time. The choice of data representation for Features and Labels reflects their different access patterns, as well as the mode of operation. During development, Features are materialized once, but frequently queried during the iterative KBC process. Labels are updated each time a user modifies labeling functions. In production, Features access pattern remains the same. However, Labels are not updated once the user has finalized their set of labeling functions. From the access patterns in the SystemX pipeline, and the char- acteristics of each sparse matrix representation, we find that imple- menting Features as a LIL minimizes runtime both in production and development. Labels, however, should be implemented as COO to support fast insertions during iterative KBC and reduce runtimes foe each iteration. In production, Labels can also be implemented as LIL to avoid the computation overhead of COO. In the ELECTRON- ICS application, we find that LIL provides 1.4× speedup over COO </list><list>in production, and COO provides over 5.8× speedup over LIL when adding a new labeling function. Takeaways. We find that Labels should be implemented as a coordinate list during development, which supports fast updates for supervision, while Features should use a list of lists, which provides faster query times. In production, both features and labels should use a list of list representation. D RELATED WORK We briefly review prior work in a few categories. Context Scope Existing KBC systems often restrict relation men- tions to specific context scopes within documents such as single sentences [22, 42] or tables [7]. Others perform KBC from richly formatted data by ensembling candidates discovered using separate extraction tasks [9, 13], which overlooks candidates composed of mentions that must be found jointly from document-level context scopes. Multimodality In unstructured data information extraction systems, only textual features [25] are utilized. Recognizing the need to repre- sent layout information as well when working with richly formatted data, various additional feature libraries have been proposed. Some have relied predominantly on structural features, usually in the con- text of web tables [29, 30, 37]. Others have built systems that rely only on visual information [12, 43]. There have been instances of visual information being used to supplement a tree-based represen- tation of a document [8, 19], but these systems were designed for other tasks, such as document classification and page segmentation. By utilizing our deep learning based featurization approach, which supports all of these representations, SystemX obviates the need to focus on feature engineering and frees the user to iterate over the supervision and learning stages of the framework. Supervision Sources Distant supervision is one effective way to programmatically create training data in machine learning area. In this paradigm, facts from existing knowledge bases are paired with unlabeled documents to create noisy or “weakly” labeled training examples [1, 24, 25, 27]. Besides existing knowledge bases, crowd- sourcing [11] and heuristics from domain experts [28] have also proven to be effective weak supervision sources. In our work, we show that by incorporating all kinds of supervision in one frame- work in a noise-aware way, we are able to achieve high quality in knowledge base construction. Furthermore, we empower users to add supervision based on intuition from any modality of data through our programming model. </list></div></html>